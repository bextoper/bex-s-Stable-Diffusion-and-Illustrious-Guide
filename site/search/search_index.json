{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Hi there, it's Bex. You may know me from Statuo's Aegis City Discord or Chubcord. While both are botmaking/chat-botting oriented, image gen is a huge thing in the hobby and is fun in general. This Rentry is a place for me to organise my experience, some tricks, and technical stuff that will be useful to simply look up. I also intend this Rentry to be a useful resource for people inexperienced with Stable Diffusion and/or Illustrious-based models, as well as people who simply want to learn more. You may be surprised by how much stuff is unintuitive, almost not documented and hard to find and understand. While initially it started as a comprehensive guide for everything related to Stable Diffusion and Illustrious, I had a realization that if I actually stick to it and do all the research and investigation, all the time in the world won't be enough, and I want to eventually finish this thing, so let's get it straight: while I will be touching on a lot of stuff, this is not a comprehensive guide. I will not be explaining every single little thing. This is for people who are interested in Stable Diffusion and want to get more from their gens, so I expect you, dear reader, to have agency of your own. I'll be saying it many times throughout the guide, but: try doing things yourself, experiment, do the opposite of what I say if you want to, fail again and again to get the gen of your dreams; be creative . Anyway, let's get to it. If you have any questions, advices or corrections, feel free to contact me on Discord, @bextoper , either in the DMs or by pinging me in Aegis City or Chub Discord. I'm in no way an expert in Stable Diffusion or image gen, but there's a lot of stuff I've personally struggled with that took me months to learn and overcome, and I think it would be valuable to share some of it. UPD: Hell yeah, the migration. Rentry was lagging extremely hard, taking about a minute to process me adding 15 words, which is extremely miserable. Kinda understandable, honestly; My guide has ~20k words and ~120k characters, but still. Here, using mkdocs and Github Pages, I can just edit documents on my PC and push it to the site, without worrying about Rentry being a bitch, so yeah: welcome to the new (no) and improved (not really) SD/Illustrious guide.","title":"Introduction"},{"location":"#introduction","text":"Hi there, it's Bex. You may know me from Statuo's Aegis City Discord or Chubcord. While both are botmaking/chat-botting oriented, image gen is a huge thing in the hobby and is fun in general. This Rentry is a place for me to organise my experience, some tricks, and technical stuff that will be useful to simply look up. I also intend this Rentry to be a useful resource for people inexperienced with Stable Diffusion and/or Illustrious-based models, as well as people who simply want to learn more. You may be surprised by how much stuff is unintuitive, almost not documented and hard to find and understand. While initially it started as a comprehensive guide for everything related to Stable Diffusion and Illustrious, I had a realization that if I actually stick to it and do all the research and investigation, all the time in the world won't be enough, and I want to eventually finish this thing, so let's get it straight: while I will be touching on a lot of stuff, this is not a comprehensive guide. I will not be explaining every single little thing. This is for people who are interested in Stable Diffusion and want to get more from their gens, so I expect you, dear reader, to have agency of your own. I'll be saying it many times throughout the guide, but: try doing things yourself, experiment, do the opposite of what I say if you want to, fail again and again to get the gen of your dreams; be creative . Anyway, let's get to it. If you have any questions, advices or corrections, feel free to contact me on Discord, @bextoper , either in the DMs or by pinging me in Aegis City or Chub Discord. I'm in no way an expert in Stable Diffusion or image gen, but there's a lot of stuff I've personally struggled with that took me months to learn and overcome, and I think it would be valuable to share some of it. UPD: Hell yeah, the migration. Rentry was lagging extremely hard, taking about a minute to process me adding 15 words, which is extremely miserable. Kinda understandable, honestly; My guide has ~20k words and ~120k characters, but still. Here, using mkdocs and Github Pages, I can just edit documents on my PC and push it to the site, without worrying about Rentry being a bitch, so yeah: welcome to the new (no) and improved (not really) SD/Illustrious guide.","title":"Introduction"},{"location":"advanced/","text":"Advanced Section: LoRAs, Samplers, Comprehensive Tagging and Prompting, Advanced Syntax, and More This is the part I'm dreading. I have so much to write about and even more stuff I feel like I'm forgetting. This part can be messy and not extremely detailed as I'm explaining my experience and \"feels\" on how certain things work or impact your generation. I can be very wrong on some aspects and completely misunderstand others, so if you see anything that you disagree with or it doesn't make sense, tell me. This part will be loosely sorted, refer to the Table of Contents to find stuff you want. Here we go. Preamble So, let's talk about the extent of stuff I'm going to talk about here. While some things are universal to SD and Illustrious, I don't think I'll be able to completely avoid stuff specific to, say, Forge, or some extensions/functions I rely on personally. Some things may not apply to your specific frontend or, especially, model. Illustrious models are generally similar to each other, but they still can have their specifics. To give you some ground to, maybe, follow me along, here are my exact parameters that I'll use to talk about everything in this section. I will touch on some of them in more detail in this section. Prompt Order Let's start with basics. again. Ideally, your prompt order should be: <Quality Tags?>, <People Tags>, <Character Tags?>, <Action Tags>, <Appearance Tags>, <Background Tags>, <Composition Tags>, <Style/Artist Tags?>, <Quality Tags?> . It's not a strict requirement, but it has it's own huge advantages. Besides keeping stuff organised, we're positioning more important stuff closer to the beginning, and the closer a prompt is to the beginning, the harder the model will adhere to it. \"Adhere\" here is not really similar to stuff like LLMs, for example. Here, it's more like \"what will the model consider more important\". Very roughly speaking, we want the model to focus on a detailed and correct character first, then assign specifics to them, and only then it will start figuring out the background and style. Of course, models do all of that simultaneously, like how it positions the character correctly according to Composition Tags, but there's still an order of More Important to Less Important. Another huge reason is preventing concepts from separating into different CLIPs when you're not intending to do it (we'll talk about them when it comes to Syntax). Since we're here, let's touch on Quality tags. Quality tags; Do we need them? Quality tags have been a staple of image genning since the earliest days. Adding masterpiece to prompts is almost a reflex at this point, but it's, first, a bit different with Illustrious, and, second, has it's own drawbacks. Let's start with a bit of philosophy. SD 1.0, then SD 1.5, and now SDXL always had a collection of images in the Training Data that represent either good quality , best quality , masterpiece , bad quality , worst quality , etc. Even the newest checkpoints inherit some of them; that's why these tags work, because Danbooru (from where Illustrious gets almost all training data) does not have it's own quality tags. A lot of Training Data associated with these quality tags is outdated and, well, does not fit the anime/drawing style we all are trying to get here; that's the first reason I argue for abandoning Positive Quality tags completely. The second reason is, when we use these Positive Quality tags, we're needlessly increasing the complexity of prompt and generation with, well, Training Data of unknown quality/stylistic match with what we're trying to get, plus these tags take space that could've been used for stuff that's actually improving our image stylistically or with details/actions/etc. Okay, you get it, I don't like Positive Quality tags. What about Negative Quality tags? That's a completely separate case. When we negative worst quality, bad quality , we're not missing out on anything; it's a separate CLIP that doesn't increase the Prompt Complexity, and it doesn't make available Training Data that we want slimmer: it's just using shitty images to show the model how NOT to do. Even with that, there is stuff you should be aware of. First, a lot of recommended Quality Tags you may find on different models' pages are, in fact, unnecessary and may make your experience much worse. Take, for example, sketch that's almost unanimously suggested as a Negative tag on almost all models. At first you may think, \"well, I don't want shitty drawn images to be used as a data for my beautiful gens\", and it's a rational thought; but if we take a look at these sketch images, we see that they're not \"shitty\", they just have a specific look and style to it that can be the exact thing you're looking for. Keep in mind that this sketch training data will be just a fraction of the overall data used for making your gen; it comes from all the other tags, like 1girl , outdoors , character tags, style tags, etc. You can think of it as, the overall look and \"style\" of your generation tries to reach a median value of all the different styles used for all regular tags in your image, and style tags like sketch just move it one way or another a little and do not define the overwhelming look of the image, if it makes sense. TL;DR: be thoughtful of what tags you're using for your image, even if everybody else is using them. Think of why exactly you want this or that tag, experiment and see what gives you the best result. Quality tags; Compilation There are a few Quality Tags combinations I can suggest: Positive: None. Negative: worst quality, worst detail, bad quality, simple background, sketch, censor This one is my main Quality Prompt. It's lightweight and doesn't limit your options too much, but here are a few things you should know: One, simple background in the negatives improves the quality of indoors and outdoors but completely prevents you from using \"simple backgrounds\" like white background , grey background , etc. It's pretty safe to use for most purposes, but keep it in mind if you decide to generate simpler backgrounds. Two, sketch in the negatives gives the images more of a \"2.5D\" look. It's great for when you want a more refined and clean style, but prevents some cool styles completely. See the previous section for more in-depth explanation. Three, censor . While you never want a censor in your NSFW gens, it's completely redundant in SFW gens, and I'm not a fan of redundancy. I usually take it out when genning SFW and put it back in NSFW gens. You can just have it always there and it won't impact your SFW gens much, but just know that it's there. To fight watermarks, add signature, watermark to your negatives (but avoid it if you can. You'll be cutting off a huge chunk of training data with them in the Negatives). Positive: ,masterpiece,best quality,amazing quality . Negative: bad quality,worst quality,worst detail,sketch,censor, A solid Quality Prompt that I use for all my model comparisons, just to include all regular SD users that do use Positive Quality tags. The only issue I have with it is Positive Quality prompt, which I already explained in the previous section. Overall, a completely solid prompt. Positive: ,masterpiece, best quality, amazing quality, very aesthetic, absurdres, newest, . Negative: ,lowres, (worst quality, bad quality:1.2), bad anatomy, sketch, jpeg artifacts, signature, watermark, old, oldest, censored, l0li, bar_censor, (pregnant), chibi, simple background, conjoined, futanari, (yes I had to censor one tag, hi Rentry pls don't ban). I used to use this prompt a lot when I was still on NTR Mix, but now I see it as a bit overwhelming. It gives the image a pretty nice 2.5D look but can have a pretty big negative impact on detail and background quality. In my opinion, a lot of tags here are redundant and unnecessary in most cases. This prompt tries to be a catch-all, but I appreciate the reverse approach much more. No quality/negative tags. With each new release, their role kinda gets less and less important, and with some styles, they just hurt your gens. How I suggest doing is, just gen. If you see something you want to remove from the gen, and you can't do it with positive prompting , add it to the negatives. Besides that, I recommend no pupils in your negatives; it's extremely useful almost in every gen. Negative Prompting: How and When Since we're on topic of Negative prompting, let's dive a bit deeper. While Positive prompting is somewhat straightforward: you just prompt for what you want to see, it doesn't work exactly like this with Negatives. You can't always go \"Hm, I don't want to see this, let's Negative it\"; you shouldn't, at least. Negative-ing is extremely powerful; it's basically purging all training data based a particular tag, which can result in \"collateral damage\" to training data you would want to see. Instead, you should do your best to find a solution based on Positive prompts; to a degree, of course. If you solve it in a tag or two - great, there's no need to add Negative prompts. If it requires more effort, or, which happens often, there's no Positive prompt you can add to solve the problem, then yes, adding negatives is worth it. They're not something to be scared of; a tag or two or five generally won't result in quality loss, but it depends. As it would happen often going on, you should always evaluate how many images a particular tag has on Booru. You should avoid negative-ing a tag that has 100k+ images, to a degree. As the first example, let's imagine that we're generating a picture of a girl with green eyes . Even with ADetailer, models may want to turn the whole eye into a sea of green color, without pupils. There's no tag for black pupil , so there's nothing left to do but negative no pupils , and it indeed solves the issue completely. As an opposite example, let's imagine generating an image of 1girl . It happens pretty often that when you generate an image of a single person, a copy of theirs appears somewhere in the background, especially if the background is complex. You may try to solve this by negative-ing 2girls, 3girls, multiple girls , but it just won't work. Instead, you should positive solo ; that's it, the issue is completely gone. Same applies to nudity tags: it's easier and much more \"gentle\" to use topless , no panties , no bra , etc instead of Negative-ing shirt, panties, bra . On the same topic; Tags Tags; Evaluating if You Should Use a Specific Tag Not all tags are born equal. While there are some necessary tags that always work, like 1girl , indoors or blonde hair , others may result in messy images or not work at all. Why does it happen? The reason is, some tags are just less populated than others. For example, a tag called grabbing own ankles does exist, but it only has 23 images on Danbooru, which means that using it is practically meaningless. More populated tags that count a few hundred images may work, but they may look messy, be impossible to influence or just won't work, overwhelmed by other more populated tags. A safe area for general tags starts at 1k+ images, with them working close to perfectly at 2k-3k population of images. It's a bit more difficult with characters and styles: they must have at least 2k population to even have a chance at working. Characters with 2k-ish images will often have wrong clothes, hair/eyes color, and will be completely messed up if you try to generate two characters in the same gen, for example. I'll touch on multiple character gens and how you can circumvent some of the issues later. Anyway, the moral is \u2013 always keep in mind how populated the tag you're adding is, and try to avoid tags with <1k images entirely (besides artist tags. They start working from 100+ images getting close to perfect at 500+) Another thing is overtrained tags . Tags with 100k+ results can completely overwhelm others, even if they technically allow for another action. For example, let's take 1girl, arm up, arm behind back . Technically, these tags allow the other, but arm up is 5x more populated than arm behind back , so arm behind back may be ignored or inconsistent. You can avoid that by manipulating weight, like (arm up:0.4) ; more about it later. Tags; How Including Redundant Tags Ruins Gens Let's start with an example. We have a prompt, 1girl, 1boy, hug, sobbing, crying, streaming tears, head on chest, arms around waist, dress, medium breasts, covered nipples, green eyes, barefoot, pov . We aim for a POV image from 1boy 's perspective where he's hugging a crying girl, but no matter what we do, we get either a non-POV image somewhere from the side; the guy hugging this girl with stretched hands (so it's not head on chest ), or even a second boy appearing in the image; to put it simply \u2014 the result is completely different from what we prompted. What could be the issue? On the first glance, the prompt is completely fine. Before digging deeper, let's talk philosophy. Why are we even prompting for stuff? So that we could see it, of course. It's more true than ever for the AI model; if you prompted for something, the model will try it's hardest to include it. It pulls up all the training data corresponding to the stuff you prompted and uses it for the generation. Let's get back to our prompt. What we're actually expecting is an image of a girl in the POV's arms, burying her head in his chest, which means that it would be from above and/or height difference / size difference , and also means that we would be unable to see that she has covered nipples , green eyes , streaming tears , that she's barefoot or has medium breasts . When we prompt for something, the model will include it, going so far as to disregard other parts of the prompt. If something should not be seen on the image, do not prompt for it . Tags; Tag Bleeding, and How To Avoid It It's a fairly simple section, but it would be wrong not to include. Let's start with an example. We're prompting for 1girl, smirk, standing, against wall, green hair, ... , . Huge chances are, we're going to not only get green hair , but also green eyes, dress, some accessory, or something green in the background. It's not only with hair, really, but with any mention of any color. The solution is fairly simple; we just specify specific concepts that got affected by the bleeding. If with green hair we also got green eyes, just prompt for blue eyes . The fact of multiple colors in the prompt lowers the chance of Color Tags bleeding by itself. Sadly, Color Tags bleeding is the simplest of bleeding tags. I cannot possibly account for every case of Tag Bleeding, but, in general, simply specifying the part that got affected by bleeding should be enough. I'll touch on it more when I get to Syntax, CLIPs and BREAKs. Tags; How Tags Impact Seemingly Unrelated Things As always, an example. We have a prompt, 1girl, lying, on bed, presenting, seductive smile, underwear, large breasts , ... , . Here, we want to focus on the large breasts tag. The gen will look like an adult (in anime terms, at least) woman. If we swap this tag for small breasts or flat chest , we would notice that the character suddenly started looking much younger than before. This is an example of how tags affect unrelated things, in this case, it's the age. The reason is pretty obvious: a lot of images that have small breasts and flat chest tags are also l0l1 and child images. Same for school uniform and petite , for example. In this specific case, we can solve this by adding l0l1, child into the Negatives, or adding mature female or stuff like milf if that's what you're going for (in this case, the solution using Negatives and Positive are both viable and used for completely separate things). This is just one example of this behavior. Another example would be using Character Tags. Some media have very distinct art-styles that will be brought into the gen; characters from stuff like Steins;Gate or OMORI will bring some of their style, and there's little to nothing you can do about them. The morale is, when you're thinking on what tags you should use, also keep in mind what kind of other aspects it can bring into your gen. Tags; Rating Tags and How to Avoid (or Embrace) Horny Rating Tags are extremely powerful instruments that... you will rarely use, really. You can see their exact definitions on the Rate Wiki , but in general: There are four rating tags, general, sensitive, questionable, explicit . First is no nudity or suggestive imagery, second is no nudity but with some suggestive stuff, third is no genital nudity (breasts are OK) and no sex, and fourth is genital nudity and/or sex and/or gore and/or other extremely graphic shit. You can use them accordingly, but they should have a purpose, as with everything. If you're going for a completely SFW gen but the model keeps throwing lewd shit at you, you can try using general , for example, and so on. One thing you should never do is negative one of them: they're some of the most populated tags in the training data, and negative-ing one of them will have a negative impact on the gen. With NSFW, there's really no need to use explicit or alike ever, as you can just specifically prompt for what you want. Tags; How Models Won't Do Anything Unless You Ask Them An example. We have a prompt, 1girl, undressing, averting eyes, shy, blush, open shirt , ... , . If we try to gen it, we would quickly find out that no matter how much we regen, the model will never generate nipples and will be extremely reluctant to even show breasts, despite open shirt seemingly implying it. That's where we get into the area of introducing concepts to the model. No matter how smart image gen model may seem, they still do not understand connections between things that are obvious to us. In the example above, to actually get a look at the tiddies, we have to specify; it's either breasts or some version with a size like large breasts , and nipples to actually see them. Without using both tags, the model would have almost never actually generated partial nudity we want. It's very similar with NSFW sex tags; with sex, you should always add stuff like nude if nude, penis and/or pussy if it's sex or other sexual activities. It's pretty similar to what I explained in Redundant Tags : you prompt what you see, exactly like you want to see it; but, just like in Redundant Tags , you should prompt only for necessary stuff. I'll explain Iterating gens a bit later. Tags; Conclusion What I listed is in no way exhaustive, but I hope it got the point across: you should have a very specific way of Thinking with Tags\u2122. You should evaluate what the tag you add will bring besides it's main function; how effective it would be; will it have negative impact on the gen; is this tag necessary to get what you want; and, most importantly, that you should experiment . Try stuff out, spend time tinkering with tags, fail over and over again to actually get what you want and learn something new . Try non-Booru tags, some of them do work (or so people say), combine tags, see how they interact \u2014 create. While image gen is not a real form of art, it's similar to one in how you use your creativity; just instead of drawing, you learn to combine things to get amazing results. Prompt; CLIPs, and Why You Should Follow The Prompt Order Let's talk about how your prompts are read by the model. I won't be technical at all as I have about 0 idea how it actually works, but there is practical stuff to know about. If you pay some attention, you may notice an indicator at the top right of both Positive Prompt and Negative prompt that says something like 0/75 . This 75 is the token count that make a single CLIP . Roughly speaking, as you press \"Generate\", Stable Diffusion processes your prompt in chunks of 75 tokens. Once you exceed these 75 tokens, 75/75 transitions into 78/150 (for example), and another CLIP is created. CLIPs are processed separately and then combined. Usually, it's not something you have to worry too hard about. Even if you exceed the first CLIP, prompt from the CLIP still gets correctly applied to the gen. Stuff suddenly gets more complicated once your prompt becomes complicated. For example, we have a prompt 1girl, ... , holding sword BREAK katana . Here, BREAK symbolizes a transition between CLIPs. If holding sword and katana were in a single CLIP, the model won't have a second thought that the sword that's being held is indeed katana . In a case where two tags are separated in different CLIPs, the model might get an idea that holding sword and katana are unrelated to each other, thus making a separate sword somewhere in the gen. While you won't (usually) get a catastrophic problem from CLIPs, it's a nice habit to follow the Prompt Order. Nothing bad will ever happen if artist or style tags are separated, same for backgrounds and composition tags. By following the Prompt Order, you make yourself safe from unnecessary issues you may get. Addendum: On a related to the Prompt Order topic: how the tags are positioned in relation to each other is also important. While I can't find an exact rule on how this works yet; for example, in 2girls, girlA, girlB, holding sword, holding gun , the model will understand that it's one girl holding a gun, and another holding a sword. As you get to genning it, you will notice that, for example, 4/5 times it's girlA holding the sword and girl B holding the gun. By either moving girlA and girlB around, or the same with holding sword and holding gun , this behaviour will change. It doesn't always look like this, especially with more complicated prompts, but sometimes you might want to move the tags around to hopefully get a better/more appropriate result, even with simple 1girl gens. While having an idea about CLIPs is not that important by itself, it's necessary to understand the BREAK separator; an extremely important and sometimes life-saving piece of SD syntax. We'll talk about BREAK s in detail later. Syntax Syntax; Introduction We're all used to separating tags by , commas, but it's not the only piece of Stable Diffusion syntax there is. In fact, Stable Diffusion is extremely versatile in how you can manipulate your whole prompt, separate tags and even change the generation on the go. Before we actually get into it, a small warning. Most of the time, you don't need advanced syntax. It will only make stuff worse and needlessly complicated. If you don't want to spend time tuning precise settings and genning dozens or even hundreds of faulty generations, you shouldn't approach this. Most things can be done with good enough prompting anyway. However, there are some use cases where syntax is extremely useful, and a few situations where it's absolutely necessary. IMPORTANT NOTE: When it will come to practical use and examples, most of what I'm talking about from here on out is empirical and can be extremely different from model to model. Some models are simply better at following your prompts and doing complex scenes than others. I can be wrong and make mistakes; some methods I describe are inconsistent, others got little to no practical use by me or literally have a single use-case. I'll try to be detailed, but there won't be a single solution for all of your problems. Keep that in mind. Syntax; Separators So, we all know and love the , comma. It is used to put distinguish parts of the prompt for the tokenizer. I honestly searched a lot for the use-cases of different separators, but, well, had 0 luck. I tried them, and, well, didn't see much of an impact. There is a situation where I do use them, but I'll get to it later. For the sake of completionism; Stable Diffusion supports the following separators (besides comma): - . Period. Some sites describe it as a \"Hard Separator\" - ; Semicolon. The same sites describe it as a \"Hard Separator\" as well??? - ! Exclamation Mark. Some sites claim that it's there to \"convey a sense of emphasis\"?????? - Newline (just pressing enter). The only thing (besides a comma) that had some impact on the gen for me. You can try to use it to fight tag bleeding. Honestly, the only real advice I can give is; just stick to commas and try putting a newline if you need to fight the tag bleeding and nothing else helps. Syntax; Prompt's Weight Manipulation Just typing a tag is not the only thing you can do. You can also manipulate how powerful a tag (or LoRA) is in relation to other tags. There are a few ways to do that: 1. Using brackets ( ) . A pair of round brackets around a part of the prompt increases it's weight by 1.1. For example, no humans, city, cityscape, scenery, (lamppost) . Here, (lamppost) has a strength of 1.1. You can combine these brackets, for example (((lamppost))) is a weight of 1.1*1.1*1.1 or 1.1^3 , which is 1.33 . 2. Using square brackets [ ] . A pair of square brackets around a part of the prompt decreases it's weight by 1.1. [lamppost] , for example, has a weight of 0.9. [[[lamppost]]] has weight of 1 / 1.1 / 1.1 /1.1 or 1.1^-3 , which is 0.75 . 3. Using a colon : . With a colon, you can accurately define the weight of a tag. For example, (lamppost:0.5) has a strength of 0.5. This is a way I recommend sticking to. LoRAs follow a similar pattern; it will look something like this: <lora:lora_Name:0.5> , where :0.5 is the strength. Be mindful of brackets that are a part of the prompt, horror_(style) for example. With a modified weight, it will look like (horror \\(style\\):0.5) . When should you resolve to this and how? In my opinion, it should be used to, A: Fight Tag Bleeding, and B: Fight Tags that are overwhelming other tags, like in the Evaluating if You Should Use a Specific Tag section. It's quite straightforward: you just lower the weight of the tag in question and see if it helps. It's mostly trial and error, and there are no fixed solutions for everything, so I leave it you you. Syntax; BREAK is love, BREAK is life It's a little bit difficult to approach this section, but let's start from understanding what it does. Let's have an example, no humans, building, house, dusk BREAK door, window, lamppost, bench,, . Here, I use BREAK to separate two parts of the prompt into different CLIPs, so that no humans, building, house, dusk (First CLIP) and door, window, lamppost, bench, (Second CLIP) are processed (somewhat) separately. In general, first CLIP is much more important than the second: image's composition is about 70% defined by the very first CLIP. What does it achieve? Multiple (Defined) Character Gens without Extensions First, it's the most powerful way to fight Tag Bleeding, and second, it allows you to define different concepts/characters completely separately. For example, in a prompt like 2girls, looking at viewer, smile, side-by-side, hand on another's shoulder, red shirt, jeans, fox ears, orange hair, cowboy shot, white background BREAK black hair, sundress, pointy ears, it's the only way to get a satisfactory result. What to note: Concepts applicable to the whole image must be in the first CLIP. In this case, it's 2girls, looking at viewer, side-by-side, hand on another's shoulder, cowboy shot, white background . You must use an action that involves multiple characters. Here, it's side-by-side, hand on another's shoulder . It'll be much harder to achieve anything meaningful without something to involve all characters. (note: it is possible to define the active participant by having the action tag in the corresponding character's part of the prompt. Here, it's 9/10 times the girl with fox ears doing the hug, and if we move hand on another's shoulder to the second CLIP, it will be an elf.) Keep overprompting to absolute minimum . Here, an extra animal ears to the first part of the prompt or elf to the second will ruin the gen. This is where manipulating weights will come very useful. If you define anything appearance-wise with the first character, do the same to the other. Here, we defined that the girl with fox ears has red shirt, jeans, orange hair, . It means that we also should define same things for our elf, so we add black hair, sundress to the second CLIP. Try to keep the image complexity low. If you can, avoid extremely defined backgrounds, difficult scenarios and such. Multiple character gens are already pushing image gen models to their absolute limits, so you have to be reasonable in what you want to achieve. Don't waste hours trying to get the impossible like I do sometimes. Do not expect it to work perfectly. Sometimes you get a 90% consistency, this prompt works about 50% of the time. That's basically it for simple 2 character gens. With 3 characters, especially if the prompt gets more complex, we get into an almost esoteric territory, so I'll talk about separately later. -> <- -> Metadata: 2girls, looking at viewer, smile, side-by-side, hand on another's shoulder, red shirt, jeans, fox ears, orange hair, cowboy shot, white background BREAK black hair, sundress, pointy ears, Negative prompt: extra ears, worst quality, bad quality Steps: 29, Sampler: DPM++ 2M, Schedule type: Karras, CFG scale: 5, Seed: 687164227, Size: 832x1216, Model hash: 0842c2a1d8, Model: Amanatsu_v11, Clip skip: 2, ADetailer model: face_yolov8n.pt <- And another; -> <- -> Metadata: 1boy, looking at viewer, on couch, (spread arms:0.7), (spread legs:0.7), black hair, smirk, furry, wolf boy, loafers, indoors, suit jacket, open jacket, dress pants, straight-on, general BREAK 3girls, (sitting on lap:1.2), sitting on person, elf, shy, maid, long hair, short hair, pink hair, white hair, Negative prompt: worst quality, bad quality, simple background, Steps: 28, Sampler: DPM++ 2M, Schedule type: Karras, CFG scale: 5, Seed: 535719555, Size: 1216x832,, Model: Amanatsu_v11, Denoising strength: 0.39, Clip skip: 2, ADetailer model: face_yolov8n.pt, Hires CFG Scale: 5, Hires upscale: 1.6, Hires steps: 23, Hires upscaler: R-ESRGAN 4x+ Anime6B, <- Yes, 3girls , this is deliberate. I will expand this section one day. Multiple (Non-Original) Character Gens Stuff is significantly easier if you just use Character tags. For example, let's get a gen with these characters: stelle \\(honkai: star rail\\), belle \\(zenless zone zero\\), lumine \\(genshin impact\\) . It follows the principles above with some slight alterations, so a few notes: - With 2-character gens, you can pass without using BREAK at all. It would simply be 2girls, girlA, girlB, <Mutual Action Tags>, <etc...> , and it's enough. For separate actions, just use BREAKs, and keep the action inside the corresponding character's section. - If the model doesn't have enough training data about characters, it might start getting their features wrong or mess them up. In this case, it's BREAKs all over again; just specify messed up features for each character separately, inside their corresponding sections. - You shouldn't mix up Non-Original characters with defined characters; it won't work 90% of the time. All the characters in your gen should be either Non-Original or Defined. - In the gen below, I used a separate CLIP for actions. The reasoning is that I don't care which character performs what action, so anyone of them can do it. Using a separate CLIP for composition/style/misc tags is also very viable, I do it often. -> <- -> Metadata: 3girls, stelle (honkai: star rail), grey hair, yellow eyes, BREAK belle (zenless zone zero), blue hair, green eyes, BREAK lumine (genshin impact), blonde hair, yellow eyes, BREAK looking at viewer, arm around shoulder, w, looking at viewer, smile, wide shot, simple background, Steps: 1, Sampler: Euler, Schedule type: Karras, CFG scale: 5, Seed: 43880133, Size: 128x128, Model hash: 0842c2a1d8, Model: amanatsuIllustrious_v11, Denoising strength: 0.75, Clip skip: 2, ADetailer model: face_yolov8n.pt <- Syntax; Prompt Merging, Delaying Prompts and Keeping the Quality with Styles Let's discuss some in-built Stable Diffusion scripts. One of the more useful is a Prompt Switch script that looks like this: [A:B:x] , where A is the first prompt, B is a second prompt, and X is a fraction of total steps at which the switch occurs. For example, we have [pixel art:2000's \\(style\\):0.2] . The translation of it would be: First 20% of the gen, the active prompt is pixel art . At 20%, it's replaced with 2000's \\(style\\) and stays like this until the end. What it means in practice? The overall composition of the gen is almost completely defined by about 2-3 first steps (With Euler or DPM++ 2M. It can vary based on the Sampler and Scheduler). By using [A:B:x] , we not only merge two styles into one, but also choose which style does the overall image composition; we're talking little details, the background, the look of the character, the quality of the image . This is where we can do another thing. In [A:B:x] , both A and B can be completely empty. Let's just not enter any A. We'll get: [:pixel art:0.2] . First, some theory. When you're using a Style Tag, an Artist Tar or a LoRA, it absolutely always has a negative impact on the gen's quality. With just a single Style or two styles at reduced weights, this impact is negligible. However, if an Artist or Style in question doesn't have enough images (<100 with Artists, <1k for Styles); or we start adding 3+ Styles/Artists/LoRAs, the gen's quality will get noticeably worse. One of the reasons is because the model will just have no idea how to make the image's composition with given Style Prompt. To spare the model from it, we can just have the Style Prompt disabled completely during the first few steps of the gen. With [:pixel art:0.2] , it only kicks in once the composition is already completely finished, and all it does is what it's supposed to do: apply style to the gen. Note that 0.2 is an arbitrary number; you may want to test in range from 0.15 to 0.5 . Syntax; Prompt Addition When it comes to solo gens, you can also sometimes add one tag to the other. It's mostly unpredictable and there are often better ways to achieve stuff, but it's an option. For example red hair, blue hair has a chance of generating a hair color that's between red and blue. It can also generate a multi-colored hair, but it's easier to just specify for multicolored hair (+ you can pick a specific type of it from here ) and have a prompt like red hair, blue hair, multicolored hair . There is another way to specifically merge tags using | pipe. For example, (red hair|blue hair:1.0) . What pipe does is, it switches from one prompt to the other each step. Whille it is a way to specific get a blend between A and B, it is unreliable. This is where we can return to [A:B:x] . For example, [red hair:blue hair:0.5] . This is a much more reliable and better way to achieve good merging. There are also other ways to use it, for example red hair, [:blue hair:0.2] and such, but I leave it to you. LoRAs LoRAs, or Low-Rank Adaptations, are a cheap and reliable way to impact the generation right on the model's technical level, basically finetuning it on the fly. I'm not the biggest LoRA enthusiast out there, so I'll be somewhat short. First, with Illustrious, I suggest using LoRAs only for styles/detail . I tried a few, like, pose and \"character appearance\" LoRA, and, well, just using Booru tags seems like a better option both quality and versatility-wise. It's just my opinion though. Second, do not use more than two style LoRAs at the same time , at least without making them skip first few steps. Without trickery, I recommend keeping the overall weight for two LoRAs below 1.1-1.3, for example, lora_A would have a weight of 0.7, and lora_B will have a weight of 0.4, making the total weight 1.1. These settings seem like they give the most style while keeping the quality basically the same as without LoRAs. Honestly, the very same goes for Style Tags. With Style Tags + LoRAs (that are applied like I suggested previously), a total of three seems stable. Just be mindful of weights. That's mostly all I have to say. LoRAs can be of various quality and made for different purposes/models, so it's just trial and error. You just apply them, tinker with weights and see if the results are satisfactory. Generation Parameters Generation Parameters; Introduction Congrats on making it this far (yes, I'm running out of ideas on how to begin sections). For most intents and purposes, you can simply setup your parameters once, never change them again and be happy. The following is parameters I consider to be stable and \"good\". Please keep in mind that I'll explore other parameters later, you may want to see them: - Sampler: DPM++ 2M Karras - Steps: 28 - CLIP Skip: 2 - CFG: 4 - Resolution: 832x1216 (or reverse), or 1024x1536 (if you're using a model based on Illustrious 1.0/1.1/2.0). And you kinda never have to change them, they are just good. But there are cases when you'd want to try something new and models that just won't take this. For a much more detailed technical information on how samplers/schedulers works and how they're different, please check 11yu's Rentry on Tech for Diffusion Image Gens . Generation Parameters; Align Your Steps, and Full-Quality Gens in Just 12 Steps Available on most local frontends, there is a Scheduler called Align Your Steps, or AYS for short. AYS uses some ancient wizardry to make the gens in 10-12 steps that look the same as 28 steps Karras or Euler a. With parameters, I suggest using DPM++ 2M Sampler, regular Align Your Steps scheduler and from 10 to 12 steps. I don't think that AYS is negatively impacted by CFG, so I just keep it at 5 and go lower as needed. -> <- -> Metadata: 1girl, makima (chainsaw man), looking at viewer, sitting, crossed legs, on chair, head tilt, indoors, office, cowboy shot, masterpiece,best quality,amazing quality Negative prompt: bad quality,worst quality,worst detail,sketch,censor Steps: 12, Sampler: DPM++ 2M, Schedule type: Align Your Steps, CFG scale: 5, Seed: 3757563295, Size: 832x1216, Model hash: 0842c2a1d8, Model: Amanatsu_v11, Clip skip: 2, ADetailer model: face_yolov8n.pt <- Keep in mind that while AYS is great for simple gens, it can start messing up as you increase the complexity of your gen. The most noticeable issue I had is AYS merging hands of multiple characters into a single blob. Besides that, AYS doesn't have any artifacts and looks incredible. It works with LoRAs, Styles and Artists with no issue. On my hardware (RTX 3070 8 GB), it lowers the generation time from 15-20 seconds with Karras and 28 steps to 9-10 seconds with AYS. Generation Parameters; Extra-Quality Gens in 60 Steps There are two notable Samplers that give you some beautiful gens without Hires and sometimes even replacing ADetailer. These are, DPM++ 3M SDE Exponential and IPNDM Automatic . Both require at least 40 steps, and I consider 60 steps to be a sweet spot, sometimes you may need up to 80. In my experience, IPNDM is better than DPM++ 3M SDE ; IPNDM shows much less artifacting than DPM++ 3M SDE, which is understandable, given the nature of SDE (SDE adds some noise each step. More variety, less stability), but I still use it occasionally. In some cases, these two Samplers can even replace Hires, especially given that Hires is much slower. With some gens, faces without ADetailer look even better than with it, and it's especially mindblowing. -> <- -> Metadata: 1girl, makima (chainsaw man), looking at viewer, sitting, crossed legs, on chair, head tilt, indoors, office, cowboy shot, masterpiece,best quality,amazing quality, Negative prompt: bad quality,worst quality,worst detail,sketch,censor Steps: 70, Sampler: IPNDM, Schedule type: Normal, CFG scale: 5, Seed: 3757563295, Size: 832x1216, Model hash: 0842c2a1d8, Model: Amanatsu_v11, Clip skip: 2, ADetailer model: face_yolov8n.pt <- Generation Parameters; LCM, and Draft Gens in Just 5 Steps Note that to use LCM, you must download it's LoRA and include it in the gen at weight 1. I suggest using LCM sampler with SGM Uniform scheduler. Your CFG must be in range from 1 to 1.5, I suggest the latter. LCM gives you perfectly fine gens extremely quickly, but they have significantly less detail than usual + there can be occasional artifacting. -> <- -> Metadata: 1girl, makima (chainsaw man), looking at viewer, sitting, crossed legs, on chair, head tilt, indoors, office, cowboy shot, masterpiece,best quality,amazing quality , Negative prompt: bad quality,worst quality,worst detail,sketch,censor Steps: 5, Sampler: LCM, Schedule type: SGM Uniform, CFG scale: 1.5, Seed: 3757563297, Size: 832x1216, Model hash: 0842c2a1d8, Model: Amanatsu_v11, Clip skip: 2, ADetailer model: face_yolov8n.pt <- Generation Parameters; IPNDM_V and Quick(er) High-Quality Gens. Found out about it recently, but it's just incredible. This sampler gives better quality at 40 steps than DPM++ 3M or regular IPNDM do at 80+ steps. First, make sure to use either Karras or Exponential schedulers. I recommend sticking to 40 steps, 30 is also viable, and technically you can go all the way down to 15 steps. Do not go above 40 steps; the image will get worse or break down. -> <- -> Metadata: 1girl, makima (chainsaw man), looking at viewer, sitting, crossed legs, on chair, head tilt, yellow eyes, indoors, office, cowboy shot, masterpiece,best quality,amazing quality Negative prompt: no pupils,, worst quality, bad quality, simple background, Steps: 40, Sampler: IPNDM_V, Schedule type: Karras, CFG scale: 5, Seed: 3757563295, Size: 832x1216, Model: Amanatsu_v11 <- And another; Note that it has no ADetailer; -> <- -> Metadata: 2girls, looking at viewer, smile, side-by-side, hand on another's shoulder, red shirt, jeans, fox ears, orange hair, cowboy shot, white background BREAK black hair, sundress, pointy ears Negative prompt: no pupils,, worst quality, bad quality, simple background, Steps: 40, Sampler: IPNDM_V, Schedule type: Karras, CFG scale: 5, Seed: 54443983, Size: 832x1216, Model: Amanatsu_v11 <- Generation Parameters; DEIS, Another High-ish Quality Sampler. DEIS is the High-Quality and relatively low-steps of IPNDM_V, but without it's downsides. It's incredibly stable and gives amazing results. Make sure to use it with SGM Uniform scheduler. I suggest 40 steps, but you can go up to 60 if needed. Comparing to IPNDM_V, they're about similar in quality; one is better in some aspects and worse in another, and vice-versa. It still gets you much more detail compared to DPM++ 2M/Euler a in just slightly more time, and it's more stable and reliable than IPNDM_V, so I can confidently recommend DEIS. -> <- -> Metadata: 1girl, standing, casting spell, magical weaving chaotic threads of white light coming out of hands, magic filling the area, outstretched arms, magic, fighting stance, white cloak, long hair, blonde hair, solo BREAK lens flare, chromatic aberration, diffraction spikes, fisheye, outdoors, night, embers, full body, dutch angle, (from side:0.4), (three quarter view:0.5), battle, war, army, soldier, fantasy Negative prompt: worst quality, bad quality, simple background, Steps: 40, Sampler: DEIS, Schedule type: SGM Uniform, CFG scale: 4.5, Seed: 1082317166, Size: 1536x1024, Model hash: 351447d6cd, Model: annnslmixillustrious_v31, ADetailer model: face_yolov8n.pt <- And another one; -> <- -> Metadata: 2girls, standing, outstretched arm, hand on another's hand, black shirt, brown skirt, long hair, baseball cap, white background, two-tone background, dual persona, human, from side, (full body:0.5), portal (object), BREAK 2girls, floating, midair, reaching towards another, looking at another, hand on another's hand, purple dress, darkness, dark background BREAK (chromatic aberration:1.2), (glitch:1.2), vhs artifacts, (digital dissolve:0.9), Negative prompt: worst quality, bad quality, 3girls, 4girls, mirror, Steps: 60, Sampler: DEIS, Schedule type: SGM Uniform, CFG scale: 4.5, Seed: 869983613, Size: 1536x1024, Model hash: 351447d6cd, Model: annnslmixillustrious_v31, ADetailer model: face_yolov8n.pt, ADetailer confidence: 0.3, ADetailer dilate erode: 4, ADetailer mask blur: 4, ADetailer denoising strength: 0.4, ADetailer inpaint only masked: True, ADetailer inpaint padding: 32, ADetailer model 2nd: hand_yolov8n.pt <- Generation Parameters; CFG++, Pain and Incredibe Gens (ComfyUI) This section is only for ComfyUI, Forge doesn't have CFG++ samplers and reForge is dead + has a different implementation of it. So, CFG++. The main principle behind it is that the sampler itself chooses an appropriate CFG scale. When it comes to practical usage, in general , you choose CFG two times lower than without CFG++; if you usually run CFG 4, use CFG 2 with CFG++; this is in theory. In practice, I suggest first trying CFG 1; if it's shit (it most likely will be), try going to CFG 1.5, and then increasing until you get a good image. CFG 1.5 - CFG 1.9 seem to be good values for me, but it can be different from model to model, and even gen to gen sometimes. As a CFG++ sampler, I suggest res_multistep_cfg_pp , and either SGM Uniform or Karras as a scheduler. In my experience, using step count higher than 30 makes gens worse, so I recommend sticking to it and doing a Hires pass later if you want. In my eyes, it may be the best sampler we have; it straight up improved the quality of lighting, colors, and detail compared to the same CFG 3 gen of DPM++ 3M SDE at 60 steps. More after the image: -> <- -> Generation Parameters: Positive: 1girl, kneeling, praying, magic, golden threads of magic, magic circle, magic symbols, blue hair, long hair, white coat, red eyes, elf, dark, indoors, church, light particles, three quarter view, full body, solo; Negative: no pupils, Sampler: res_multistep_cfg_pp, scheduler: SGM Uniform, Step Count: 30, CFG Scale: 1.8, Face Detailer, 1.5x Hires Pass, seed: 43, 1024 x 1536, checkpoint: Nova Orange v9.0. ComfyUI. <- So, let's talk about practical usage in more detail. Main advantages of CFG++ are light contrast, colors and color contrast. With conventional samplers, when you try to go for a darker gen, you often get an image that's grey and still bright. Using some magic, CFG++ just solves the issue entirely. You (almost) get true black and incredible color contrast. The only issue with CFG++ is that it's very picky about your CFG scale, sometimes requiring you to set different values for different gens on the same model. You get something a little bit wrong, and you get an overexposed / oversaturated gen. Right now, I use this sampler for almost every model, it's just that good. ( I lost prompts for individual images, so no prompts. Sorry. ) Generation Parameters: Style Prompt: 2d, sketch, oekaki, limited palette, black background , Negatives: no pupils, loli, child , Sampler: res_multistep_ancestral_cfgpp , Scheduler: Karras , CFG Scale: 1.8, Steps: 30, Checkpoint: Nova Anime v7.0, Resolution: either 1536 x 1024 or 1024 x 1536 . No Fixes, Hires or additional passes. Generation Parameters; Hires Hires is an incredible feature to get higher resolution gens with increased detail. I suggest the following parameters: - Upscaler: R-ESRGAN 4x+ Anime6b OR Lanczos - Hires Steps: 20-22 - Denoising Strength: 0.39-0.43 - Upscale by: 1.5 - 1.65 -> <- -> Metadata: 1girl, makima (chainsaw man), looking at viewer, sitting, crossed legs, on chair, head tilt, indoors, office, cowboy shot, masterpiece,best quality,amazing quality, Negative prompt: bad quality,worst quality,worst detail,sketch,censor Steps: 28, Sampler: DPM++ 2M, Schedule type: Karras, CFG scale: 5, Seed: 3757563295, Size: 832x1216, Model hash: 0842c2a1d8, Model: Amanatsu_v11, Denoising strength: 0.39, Clip skip: 2, ADetailer model: face_yolov8n.pt, Hires CFG Scale: 5, Hires upscale: 1.5, Hires steps: 20, Hires upscaler: R-ESRGAN 4x+ Anime6B <- Generation Parameters; Refiner Refiner means that the first x fraction of steps is done by one model, and then it's replaced with another model. It's trial and error, so there's not that much to talk about. One thing I want to note is, remember that the overall composition of the image is done by the first model, not the refiner model. You can have the first model to 2-4 steps and then switch to another model; as a result, the image's composition is done by model A and the whole style is done by model B. Prompting; Iterating on Your Gens This part is more of a philosophical view on how you should do prompting. I like to call it Iterative Prompting . The main point is, you start from drafting the overall image, and then focus on each part of the image, following the Prompt Order. You refine each part of the prompt, generate the image, reflect on the result and keep refining the prompt until you get the image you want consistently, after that you enable all the resource-intensive stuff like Hires or High-Steps Samplers and play with styles. Let's have a big example. I come up with an overall idea of the gen. Let's start with an idea: a creepy/dangerous punk girl in a darker style. After some thinking, I come up with this prompt: 1girl, looking at viewer, head tilt, raised eyebrow, smirk, holding cane, red jacket, open jacket, long hair, multicolored hair, dark, darkness, city, high contrast, and gen. I look at the gens. While high contrast definitely gives an interesting look, the overall image is too dark. smirk is also not optimal, her expression is really weird in a bad way. Besides this, her overall look is pretty close to what I wanted, so I also need to add Composition tags. Reflecting on this, I edit the prompt, 1girl, looking at viewer, head tilt, raised eyebrow, (smirk:0.5), (grin:0.5), holding cane, red jacket, open jacket, long hair, multicolored hair, dark, darkness, city, (high contrast:0.7), cowboy shot, close-up, and generate it. I'm already quite happy with the image, but I have some other ideas I want to try. I'd like to have more character focus, so I remove background tags. I also want even less dark image, so I lower the weight of dark . New prompt is: 1girl, looking at viewer, head tilt, raised eyebrow, (smirk:0.5), (grin:0.5), holding cane, cane, red jacket, open jacket, long hair, multicolored hair, (dark:0.5), darkness, (high contrast:0.7), cowboy shot, close-up, . Time to gen First, she's tilting her head too much. Second, to make her more visually interesting, I add hererochromia , then specify two hair colors and rely on Tag Bleeding to also do eye colors. To add more to the composition, some slight dutch angle . Prompt turned into: 1girl, looking at viewer, (head tilt:0.7), (smirk:0.5), (grin:0.5), holding cane, cane, red jacket, open jacket, heterochromia, long hair, white hair, red hair, multicolored hair, (dark:0.5), darkness, (high contrast:0.7), cowboy shot, close-up, (dutch angle:0.3), and I gen it. At this point, I feel pretty happy about the result, so I start adding style and LoRAs, as well as doing some minor edits. New prompt: 1girl, looking at viewer, (head tilt:0.7), (smirk:0.5), (grin:0.5), holding cane, cane, red jacket, open jacket, heterochromia, ringed eyes, long hair, white hair, red hair, multicolored hair, (cowboy shot:0.7), close-up, (dutch angle:0.3), traditional media, <lora:illustrious_quality_modifiers_masterpieces_v1:0.7> <lora:illustriousXL_stabilizer_v1.72:0.35> Not specifying backgrounds and having simple background in the negatives really ruined the gen, so I fix it: 1girl, looking at viewer, (head tilt:0.7), (smirk:0.5), (grin:0.5), holding cane, cane, red jacket, open jacket, heterochromia, ringed eyes, long hair, white hair, red hair, multicolored hair, dark background, (cowboy shot:0.7), close-up, (dutch angle:0.3), traditional media, <lora:illustrious_quality_modifiers_masterpieces_v1:0.7> <lora:illustriousXL_stabilizer_v1.72:0.35> Now I feel completely happy. I take the seed of my favourite gen and reuse it, this time with Hires. Woila, we're done. -> <- -> Metadata: 1girl, looking at viewer, (head tilt:0.7), (smirk:0.5), (grin:0.5), holding cane, cane, red jacket, open jacket, heterochromia, ringed eyes, long hair, white hair, red hair, multicolored hair, dark background, (cowboy shot:0.7), close-up, (dutch angle:0.3), traditional media, , Negative prompt: worst quality, bad quality, Steps: 28, Sampler: DPM++ 2M, Schedule type: Karras, CFG scale: 5, Seed: 1003245552, Size: 832x1216, Model hash: 0842c2a1d8, Model: Amanatsu_v11, Denoising strength: 0.39, Clip skip: 2, ADetailer model: face_yolov8n.pt, Hires CFG Scale: 5, Hires upscale: 1.6, Hires steps: 22, Hires upscaler: R-ESRGAN 4x+ Anime6B <- From start to finish, this generation took 50 images to refine the first prompt into the last. This is the way I do all my gens, and I highly recommend following the same principle. It's extremely easy to mess up at the very beginning and spend hours trying to fix it; I've been there. Prompting; Prompt Complexity This is a quite ephemeral thing that I think is extremely important. I consider Prompt Complexity to be this: it's the amount of separate concepts you require the model to understand and generate correctly. Things like Styles are not separate specific concepts, so they're not increasing the complexity. Appearance tags on solo gens have no other interpretation than, well, that they're worn by this character, so it's also not an issue. Things start getting fun once you start doing complicated stuff. Asking for two separate actions by a single character relies on the model understanding how it's going to look, so it increases the complexity a lot. Defining two separate characters makes the complexity skyrocket: the model has to associate different characteristic (that are technically applicable to both characters) to separate characters. It's fine if you assign actions that require multiple characters, but stuff like black hair , pointy ears , animal ears can be assigned to either, and the model will most often just make both characters like this. Resulting from this, Prompt Complexity is a sum of every Tag Bleeding, Redundant Tags, Overtrained and Undertrained Tags, as well as ambiguous tags. The less complex your prompt is, the more consistency and quality you get. You should always look for opportunities to make your prompt simpler, to use a more specific (but still populated) tag, replace multiple tags with just one, avoid redundancy and bleeding. It's a nice habit that may save hours of work. New Illustrious 1.0/1.1/2.0-Based Models So, there are finally models based on new Illustrious 1.0/1.1 that don't suck ass, and I finally got to try them. You can find a comparison of a few of them in my thread on Aegiscord. They're pretty good, and because of their support of higher resolutions and Natural Language, I use them quite often. Let's talk about it in more detail. You can see examples of them in the DEIS section. Higher Resolutions ; While Illustrious 1.0/1.1-based models supports a variety of 1536 x 1536 resolutions, I suggest sticking to two: 1024 x 1536 or 1536 x 1024 ; they show the most prompt adherence and overall quality. Higher resolution helps immensely in face quality, small/subtle details, and detail in general. Keep in mind that while these higher resolutions are more than usable, prompt adherence of them is worse than with regular SDXL sizes. You can always use good old 832 x 1216 , 1216 x 832 , 1024 x 1024 , etc. Natural Language Prompting ; While tags remain the main way of prompting, you can finally add in some Natural Language for more complicated prompts. For example, in the magical girl image I used as an example for DEIS above, I used magical weaving chaotic threads of white light coming out of hands , and it worked almost perfectly. As anything related to Natural Language, there can't be no guidelines or specific rules for usage, so all I can suggest is experimenting; but I'd advise that if you can do a specific thing you want with Tags, you should do just that. Natural Language is unpredictable, and I think it should be used only if there's no other way. Impressions: I'm not ready to speak definitively on them right now, but I've made my impressions on them. Compared to Illustrious 0.1, Illustrious 1.0/1.1 based models show much more creativity if you prompt for it, and they can easily do extremely complex gens that 0.1 would've struggled with. 1.0/1.1 is not fool proofed in any way, and complex gens still take a lot of knowledge and understanding of how tag, prompt and gen, but Natural Language and improved creativity makes the job noticeably easier. I'm not ready to abandon Amanatsu completely, but AnnnslMix specifically is really good. Stuff like this makes me excited for what's to come. UPD: Illustrious 2.0 :So, we finally got good Illustrious 2.0 models, like Nova Orange and Nova Anime. First, they seem to really hate regular samplers, like DPM++ 2M and regular Euler. From my testing, these models require you to use a sampler that does noise injection, so it's samplers that have Ancestral or SDE in them. I got the best results using Euler ancestral and res_multistep_ancestral . Besides that, they really like CFG++ samplers, so I recommend using res_multistep_ancestral_cfgpp with CFG at about 1.5 - 1.7 in Comfy. Second, they're now much more coherent at higher resolutions, sometimes showing just better results there compared to regular stuff like 832 x 1216 . Note that it results in higher VRAM usage; with my RTX 3070 8 GB, I usually reach about 7.0 - 7.5 GB usage. Third, I don't see any improvements in Natural Language compared to Illustrious 1.0/1.1; you can try your luck using them, but it's preffered to stick to tags if you can. Fourth, from my testing, Illustrious 2.0 models show much better prompt adherence and understanding; you can combine more difficult tags, sometimes ditch the \"Prompt for what you want to see\" principle and other creative stuff; you are not as likely to ruin your gen as before. I suspect recent Illustrious 2.0-based models to be heavily overtrained, but it's something to be researched. V-Pred Models; How to Use While I won't be going into deail about how V-Pred models are different from eps-pred, there are some differences in how you should approach them. In general, if you see that a model you're using is a v-pred (or there are unexpected artifacts/noise in the image), you should know: - DPM++ 2M Karras and some other samplers/schedulers do not work . - DPM++ 2M SGM Uniform, Euler a Normal, DEIS SGM Uniform and IPNDM_V SGM Uniform are my recommended samplers. - I highly recommend using a CFG++ sampler, like res_multistep_cfgpp . - Avoid using dark , day and other tags that change the lighting. While v-pred is famous for it's great color range and lighting, it's extremely prone to over-exposing gens or making them too dark. - Be ready for instabilities and jank. I personally do not like v-pred at all, and all models using it (that I tested) sucked. You can get these advantages of v-pred on regular eps-pred models with some prompting (to some extent), like contrast, hdr, vibrant, shadow, dark, reflection, and lighting tags.","title":"Advanced Section"},{"location":"advanced/#advanced-section-loras-samplers-comprehensive-tagging-and-prompting-advanced-syntax-and-more","text":"This is the part I'm dreading. I have so much to write about and even more stuff I feel like I'm forgetting. This part can be messy and not extremely detailed as I'm explaining my experience and \"feels\" on how certain things work or impact your generation. I can be very wrong on some aspects and completely misunderstand others, so if you see anything that you disagree with or it doesn't make sense, tell me. This part will be loosely sorted, refer to the Table of Contents to find stuff you want. Here we go.","title":"Advanced Section: LoRAs, Samplers, Comprehensive Tagging and Prompting, Advanced Syntax, and More"},{"location":"advanced/#preamble","text":"So, let's talk about the extent of stuff I'm going to talk about here. While some things are universal to SD and Illustrious, I don't think I'll be able to completely avoid stuff specific to, say, Forge, or some extensions/functions I rely on personally. Some things may not apply to your specific frontend or, especially, model. Illustrious models are generally similar to each other, but they still can have their specifics. To give you some ground to, maybe, follow me along, here are my exact parameters that I'll use to talk about everything in this section. I will touch on some of them in more detail in this section.","title":"Preamble"},{"location":"advanced/#prompt-order","text":"Let's start with basics. again. Ideally, your prompt order should be: <Quality Tags?>, <People Tags>, <Character Tags?>, <Action Tags>, <Appearance Tags>, <Background Tags>, <Composition Tags>, <Style/Artist Tags?>, <Quality Tags?> . It's not a strict requirement, but it has it's own huge advantages. Besides keeping stuff organised, we're positioning more important stuff closer to the beginning, and the closer a prompt is to the beginning, the harder the model will adhere to it. \"Adhere\" here is not really similar to stuff like LLMs, for example. Here, it's more like \"what will the model consider more important\". Very roughly speaking, we want the model to focus on a detailed and correct character first, then assign specifics to them, and only then it will start figuring out the background and style. Of course, models do all of that simultaneously, like how it positions the character correctly according to Composition Tags, but there's still an order of More Important to Less Important. Another huge reason is preventing concepts from separating into different CLIPs when you're not intending to do it (we'll talk about them when it comes to Syntax). Since we're here, let's touch on Quality tags.","title":"Prompt Order"},{"location":"advanced/#quality-tags-do-we-need-them","text":"Quality tags have been a staple of image genning since the earliest days. Adding masterpiece to prompts is almost a reflex at this point, but it's, first, a bit different with Illustrious, and, second, has it's own drawbacks. Let's start with a bit of philosophy. SD 1.0, then SD 1.5, and now SDXL always had a collection of images in the Training Data that represent either good quality , best quality , masterpiece , bad quality , worst quality , etc. Even the newest checkpoints inherit some of them; that's why these tags work, because Danbooru (from where Illustrious gets almost all training data) does not have it's own quality tags. A lot of Training Data associated with these quality tags is outdated and, well, does not fit the anime/drawing style we all are trying to get here; that's the first reason I argue for abandoning Positive Quality tags completely. The second reason is, when we use these Positive Quality tags, we're needlessly increasing the complexity of prompt and generation with, well, Training Data of unknown quality/stylistic match with what we're trying to get, plus these tags take space that could've been used for stuff that's actually improving our image stylistically or with details/actions/etc. Okay, you get it, I don't like Positive Quality tags. What about Negative Quality tags? That's a completely separate case. When we negative worst quality, bad quality , we're not missing out on anything; it's a separate CLIP that doesn't increase the Prompt Complexity, and it doesn't make available Training Data that we want slimmer: it's just using shitty images to show the model how NOT to do. Even with that, there is stuff you should be aware of. First, a lot of recommended Quality Tags you may find on different models' pages are, in fact, unnecessary and may make your experience much worse. Take, for example, sketch that's almost unanimously suggested as a Negative tag on almost all models. At first you may think, \"well, I don't want shitty drawn images to be used as a data for my beautiful gens\", and it's a rational thought; but if we take a look at these sketch images, we see that they're not \"shitty\", they just have a specific look and style to it that can be the exact thing you're looking for. Keep in mind that this sketch training data will be just a fraction of the overall data used for making your gen; it comes from all the other tags, like 1girl , outdoors , character tags, style tags, etc. You can think of it as, the overall look and \"style\" of your generation tries to reach a median value of all the different styles used for all regular tags in your image, and style tags like sketch just move it one way or another a little and do not define the overwhelming look of the image, if it makes sense. TL;DR: be thoughtful of what tags you're using for your image, even if everybody else is using them. Think of why exactly you want this or that tag, experiment and see what gives you the best result.","title":"Quality tags; Do we need them?"},{"location":"advanced/#quality-tags-compilation","text":"There are a few Quality Tags combinations I can suggest: Positive: None. Negative: worst quality, worst detail, bad quality, simple background, sketch, censor This one is my main Quality Prompt. It's lightweight and doesn't limit your options too much, but here are a few things you should know: One, simple background in the negatives improves the quality of indoors and outdoors but completely prevents you from using \"simple backgrounds\" like white background , grey background , etc. It's pretty safe to use for most purposes, but keep it in mind if you decide to generate simpler backgrounds. Two, sketch in the negatives gives the images more of a \"2.5D\" look. It's great for when you want a more refined and clean style, but prevents some cool styles completely. See the previous section for more in-depth explanation. Three, censor . While you never want a censor in your NSFW gens, it's completely redundant in SFW gens, and I'm not a fan of redundancy. I usually take it out when genning SFW and put it back in NSFW gens. You can just have it always there and it won't impact your SFW gens much, but just know that it's there. To fight watermarks, add signature, watermark to your negatives (but avoid it if you can. You'll be cutting off a huge chunk of training data with them in the Negatives). Positive: ,masterpiece,best quality,amazing quality . Negative: bad quality,worst quality,worst detail,sketch,censor, A solid Quality Prompt that I use for all my model comparisons, just to include all regular SD users that do use Positive Quality tags. The only issue I have with it is Positive Quality prompt, which I already explained in the previous section. Overall, a completely solid prompt. Positive: ,masterpiece, best quality, amazing quality, very aesthetic, absurdres, newest, . Negative: ,lowres, (worst quality, bad quality:1.2), bad anatomy, sketch, jpeg artifacts, signature, watermark, old, oldest, censored, l0li, bar_censor, (pregnant), chibi, simple background, conjoined, futanari, (yes I had to censor one tag, hi Rentry pls don't ban). I used to use this prompt a lot when I was still on NTR Mix, but now I see it as a bit overwhelming. It gives the image a pretty nice 2.5D look but can have a pretty big negative impact on detail and background quality. In my opinion, a lot of tags here are redundant and unnecessary in most cases. This prompt tries to be a catch-all, but I appreciate the reverse approach much more. No quality/negative tags. With each new release, their role kinda gets less and less important, and with some styles, they just hurt your gens. How I suggest doing is, just gen. If you see something you want to remove from the gen, and you can't do it with positive prompting , add it to the negatives. Besides that, I recommend no pupils in your negatives; it's extremely useful almost in every gen.","title":"Quality tags; Compilation"},{"location":"advanced/#negative-prompting-how-and-when","text":"Since we're on topic of Negative prompting, let's dive a bit deeper. While Positive prompting is somewhat straightforward: you just prompt for what you want to see, it doesn't work exactly like this with Negatives. You can't always go \"Hm, I don't want to see this, let's Negative it\"; you shouldn't, at least. Negative-ing is extremely powerful; it's basically purging all training data based a particular tag, which can result in \"collateral damage\" to training data you would want to see. Instead, you should do your best to find a solution based on Positive prompts; to a degree, of course. If you solve it in a tag or two - great, there's no need to add Negative prompts. If it requires more effort, or, which happens often, there's no Positive prompt you can add to solve the problem, then yes, adding negatives is worth it. They're not something to be scared of; a tag or two or five generally won't result in quality loss, but it depends. As it would happen often going on, you should always evaluate how many images a particular tag has on Booru. You should avoid negative-ing a tag that has 100k+ images, to a degree. As the first example, let's imagine that we're generating a picture of a girl with green eyes . Even with ADetailer, models may want to turn the whole eye into a sea of green color, without pupils. There's no tag for black pupil , so there's nothing left to do but negative no pupils , and it indeed solves the issue completely. As an opposite example, let's imagine generating an image of 1girl . It happens pretty often that when you generate an image of a single person, a copy of theirs appears somewhere in the background, especially if the background is complex. You may try to solve this by negative-ing 2girls, 3girls, multiple girls , but it just won't work. Instead, you should positive solo ; that's it, the issue is completely gone. Same applies to nudity tags: it's easier and much more \"gentle\" to use topless , no panties , no bra , etc instead of Negative-ing shirt, panties, bra . On the same topic;","title":"Negative Prompting: How and When"},{"location":"advanced/#tags","text":"","title":"Tags"},{"location":"advanced/#tags-evaluating-if-you-should-use-a-specific-tag","text":"Not all tags are born equal. While there are some necessary tags that always work, like 1girl , indoors or blonde hair , others may result in messy images or not work at all. Why does it happen? The reason is, some tags are just less populated than others. For example, a tag called grabbing own ankles does exist, but it only has 23 images on Danbooru, which means that using it is practically meaningless. More populated tags that count a few hundred images may work, but they may look messy, be impossible to influence or just won't work, overwhelmed by other more populated tags. A safe area for general tags starts at 1k+ images, with them working close to perfectly at 2k-3k population of images. It's a bit more difficult with characters and styles: they must have at least 2k population to even have a chance at working. Characters with 2k-ish images will often have wrong clothes, hair/eyes color, and will be completely messed up if you try to generate two characters in the same gen, for example. I'll touch on multiple character gens and how you can circumvent some of the issues later. Anyway, the moral is \u2013 always keep in mind how populated the tag you're adding is, and try to avoid tags with <1k images entirely (besides artist tags. They start working from 100+ images getting close to perfect at 500+) Another thing is overtrained tags . Tags with 100k+ results can completely overwhelm others, even if they technically allow for another action. For example, let's take 1girl, arm up, arm behind back . Technically, these tags allow the other, but arm up is 5x more populated than arm behind back , so arm behind back may be ignored or inconsistent. You can avoid that by manipulating weight, like (arm up:0.4) ; more about it later.","title":"Tags; Evaluating if You Should Use a Specific Tag"},{"location":"advanced/#tags-how-including-redundant-tags-ruins-gens","text":"Let's start with an example. We have a prompt, 1girl, 1boy, hug, sobbing, crying, streaming tears, head on chest, arms around waist, dress, medium breasts, covered nipples, green eyes, barefoot, pov . We aim for a POV image from 1boy 's perspective where he's hugging a crying girl, but no matter what we do, we get either a non-POV image somewhere from the side; the guy hugging this girl with stretched hands (so it's not head on chest ), or even a second boy appearing in the image; to put it simply \u2014 the result is completely different from what we prompted. What could be the issue? On the first glance, the prompt is completely fine. Before digging deeper, let's talk philosophy. Why are we even prompting for stuff? So that we could see it, of course. It's more true than ever for the AI model; if you prompted for something, the model will try it's hardest to include it. It pulls up all the training data corresponding to the stuff you prompted and uses it for the generation. Let's get back to our prompt. What we're actually expecting is an image of a girl in the POV's arms, burying her head in his chest, which means that it would be from above and/or height difference / size difference , and also means that we would be unable to see that she has covered nipples , green eyes , streaming tears , that she's barefoot or has medium breasts . When we prompt for something, the model will include it, going so far as to disregard other parts of the prompt. If something should not be seen on the image, do not prompt for it .","title":"Tags; How Including Redundant Tags Ruins Gens"},{"location":"advanced/#tags-tag-bleeding-and-how-to-avoid-it","text":"It's a fairly simple section, but it would be wrong not to include. Let's start with an example. We're prompting for 1girl, smirk, standing, against wall, green hair, ... , . Huge chances are, we're going to not only get green hair , but also green eyes, dress, some accessory, or something green in the background. It's not only with hair, really, but with any mention of any color. The solution is fairly simple; we just specify specific concepts that got affected by the bleeding. If with green hair we also got green eyes, just prompt for blue eyes . The fact of multiple colors in the prompt lowers the chance of Color Tags bleeding by itself. Sadly, Color Tags bleeding is the simplest of bleeding tags. I cannot possibly account for every case of Tag Bleeding, but, in general, simply specifying the part that got affected by bleeding should be enough. I'll touch on it more when I get to Syntax, CLIPs and BREAKs.","title":"Tags; Tag Bleeding, and How To Avoid It"},{"location":"advanced/#tags-how-tags-impact-seemingly-unrelated-things","text":"As always, an example. We have a prompt, 1girl, lying, on bed, presenting, seductive smile, underwear, large breasts , ... , . Here, we want to focus on the large breasts tag. The gen will look like an adult (in anime terms, at least) woman. If we swap this tag for small breasts or flat chest , we would notice that the character suddenly started looking much younger than before. This is an example of how tags affect unrelated things, in this case, it's the age. The reason is pretty obvious: a lot of images that have small breasts and flat chest tags are also l0l1 and child images. Same for school uniform and petite , for example. In this specific case, we can solve this by adding l0l1, child into the Negatives, or adding mature female or stuff like milf if that's what you're going for (in this case, the solution using Negatives and Positive are both viable and used for completely separate things). This is just one example of this behavior. Another example would be using Character Tags. Some media have very distinct art-styles that will be brought into the gen; characters from stuff like Steins;Gate or OMORI will bring some of their style, and there's little to nothing you can do about them. The morale is, when you're thinking on what tags you should use, also keep in mind what kind of other aspects it can bring into your gen.","title":"Tags; How Tags Impact Seemingly Unrelated Things"},{"location":"advanced/#tags-rating-tags-and-how-to-avoid-or-embrace-horny","text":"Rating Tags are extremely powerful instruments that... you will rarely use, really. You can see their exact definitions on the Rate Wiki , but in general: There are four rating tags, general, sensitive, questionable, explicit . First is no nudity or suggestive imagery, second is no nudity but with some suggestive stuff, third is no genital nudity (breasts are OK) and no sex, and fourth is genital nudity and/or sex and/or gore and/or other extremely graphic shit. You can use them accordingly, but they should have a purpose, as with everything. If you're going for a completely SFW gen but the model keeps throwing lewd shit at you, you can try using general , for example, and so on. One thing you should never do is negative one of them: they're some of the most populated tags in the training data, and negative-ing one of them will have a negative impact on the gen. With NSFW, there's really no need to use explicit or alike ever, as you can just specifically prompt for what you want.","title":"Tags; Rating Tags and How to Avoid (or Embrace) Horny"},{"location":"advanced/#tags-how-models-wont-do-anything-unless-you-ask-them","text":"An example. We have a prompt, 1girl, undressing, averting eyes, shy, blush, open shirt , ... , . If we try to gen it, we would quickly find out that no matter how much we regen, the model will never generate nipples and will be extremely reluctant to even show breasts, despite open shirt seemingly implying it. That's where we get into the area of introducing concepts to the model. No matter how smart image gen model may seem, they still do not understand connections between things that are obvious to us. In the example above, to actually get a look at the tiddies, we have to specify; it's either breasts or some version with a size like large breasts , and nipples to actually see them. Without using both tags, the model would have almost never actually generated partial nudity we want. It's very similar with NSFW sex tags; with sex, you should always add stuff like nude if nude, penis and/or pussy if it's sex or other sexual activities. It's pretty similar to what I explained in Redundant Tags : you prompt what you see, exactly like you want to see it; but, just like in Redundant Tags , you should prompt only for necessary stuff. I'll explain Iterating gens a bit later.","title":"Tags; How Models Won't Do Anything Unless You Ask Them"},{"location":"advanced/#tags-conclusion","text":"What I listed is in no way exhaustive, but I hope it got the point across: you should have a very specific way of Thinking with Tags\u2122. You should evaluate what the tag you add will bring besides it's main function; how effective it would be; will it have negative impact on the gen; is this tag necessary to get what you want; and, most importantly, that you should experiment . Try stuff out, spend time tinkering with tags, fail over and over again to actually get what you want and learn something new . Try non-Booru tags, some of them do work (or so people say), combine tags, see how they interact \u2014 create. While image gen is not a real form of art, it's similar to one in how you use your creativity; just instead of drawing, you learn to combine things to get amazing results.","title":"Tags; Conclusion"},{"location":"advanced/#prompt-clips-and-why-you-should-follow-the-prompt-order","text":"Let's talk about how your prompts are read by the model. I won't be technical at all as I have about 0 idea how it actually works, but there is practical stuff to know about. If you pay some attention, you may notice an indicator at the top right of both Positive Prompt and Negative prompt that says something like 0/75 . This 75 is the token count that make a single CLIP . Roughly speaking, as you press \"Generate\", Stable Diffusion processes your prompt in chunks of 75 tokens. Once you exceed these 75 tokens, 75/75 transitions into 78/150 (for example), and another CLIP is created. CLIPs are processed separately and then combined. Usually, it's not something you have to worry too hard about. Even if you exceed the first CLIP, prompt from the CLIP still gets correctly applied to the gen. Stuff suddenly gets more complicated once your prompt becomes complicated. For example, we have a prompt 1girl, ... , holding sword BREAK katana . Here, BREAK symbolizes a transition between CLIPs. If holding sword and katana were in a single CLIP, the model won't have a second thought that the sword that's being held is indeed katana . In a case where two tags are separated in different CLIPs, the model might get an idea that holding sword and katana are unrelated to each other, thus making a separate sword somewhere in the gen. While you won't (usually) get a catastrophic problem from CLIPs, it's a nice habit to follow the Prompt Order. Nothing bad will ever happen if artist or style tags are separated, same for backgrounds and composition tags. By following the Prompt Order, you make yourself safe from unnecessary issues you may get. Addendum: On a related to the Prompt Order topic: how the tags are positioned in relation to each other is also important. While I can't find an exact rule on how this works yet; for example, in 2girls, girlA, girlB, holding sword, holding gun , the model will understand that it's one girl holding a gun, and another holding a sword. As you get to genning it, you will notice that, for example, 4/5 times it's girlA holding the sword and girl B holding the gun. By either moving girlA and girlB around, or the same with holding sword and holding gun , this behaviour will change. It doesn't always look like this, especially with more complicated prompts, but sometimes you might want to move the tags around to hopefully get a better/more appropriate result, even with simple 1girl gens. While having an idea about CLIPs is not that important by itself, it's necessary to understand the BREAK separator; an extremely important and sometimes life-saving piece of SD syntax. We'll talk about BREAK s in detail later.","title":"Prompt; CLIPs, and Why You Should Follow The Prompt Order"},{"location":"advanced/#syntax","text":"","title":"Syntax"},{"location":"advanced/#syntax-introduction","text":"We're all used to separating tags by , commas, but it's not the only piece of Stable Diffusion syntax there is. In fact, Stable Diffusion is extremely versatile in how you can manipulate your whole prompt, separate tags and even change the generation on the go. Before we actually get into it, a small warning. Most of the time, you don't need advanced syntax. It will only make stuff worse and needlessly complicated. If you don't want to spend time tuning precise settings and genning dozens or even hundreds of faulty generations, you shouldn't approach this. Most things can be done with good enough prompting anyway. However, there are some use cases where syntax is extremely useful, and a few situations where it's absolutely necessary. IMPORTANT NOTE: When it will come to practical use and examples, most of what I'm talking about from here on out is empirical and can be extremely different from model to model. Some models are simply better at following your prompts and doing complex scenes than others. I can be wrong and make mistakes; some methods I describe are inconsistent, others got little to no practical use by me or literally have a single use-case. I'll try to be detailed, but there won't be a single solution for all of your problems. Keep that in mind.","title":"Syntax; Introduction"},{"location":"advanced/#syntax-separators","text":"So, we all know and love the , comma. It is used to put distinguish parts of the prompt for the tokenizer. I honestly searched a lot for the use-cases of different separators, but, well, had 0 luck. I tried them, and, well, didn't see much of an impact. There is a situation where I do use them, but I'll get to it later. For the sake of completionism; Stable Diffusion supports the following separators (besides comma): - . Period. Some sites describe it as a \"Hard Separator\" - ; Semicolon. The same sites describe it as a \"Hard Separator\" as well??? - ! Exclamation Mark. Some sites claim that it's there to \"convey a sense of emphasis\"?????? - Newline (just pressing enter). The only thing (besides a comma) that had some impact on the gen for me. You can try to use it to fight tag bleeding. Honestly, the only real advice I can give is; just stick to commas and try putting a newline if you need to fight the tag bleeding and nothing else helps.","title":"Syntax; Separators"},{"location":"advanced/#syntax-prompts-weight-manipulation","text":"Just typing a tag is not the only thing you can do. You can also manipulate how powerful a tag (or LoRA) is in relation to other tags. There are a few ways to do that: 1. Using brackets ( ) . A pair of round brackets around a part of the prompt increases it's weight by 1.1. For example, no humans, city, cityscape, scenery, (lamppost) . Here, (lamppost) has a strength of 1.1. You can combine these brackets, for example (((lamppost))) is a weight of 1.1*1.1*1.1 or 1.1^3 , which is 1.33 . 2. Using square brackets [ ] . A pair of square brackets around a part of the prompt decreases it's weight by 1.1. [lamppost] , for example, has a weight of 0.9. [[[lamppost]]] has weight of 1 / 1.1 / 1.1 /1.1 or 1.1^-3 , which is 0.75 . 3. Using a colon : . With a colon, you can accurately define the weight of a tag. For example, (lamppost:0.5) has a strength of 0.5. This is a way I recommend sticking to. LoRAs follow a similar pattern; it will look something like this: <lora:lora_Name:0.5> , where :0.5 is the strength. Be mindful of brackets that are a part of the prompt, horror_(style) for example. With a modified weight, it will look like (horror \\(style\\):0.5) . When should you resolve to this and how? In my opinion, it should be used to, A: Fight Tag Bleeding, and B: Fight Tags that are overwhelming other tags, like in the Evaluating if You Should Use a Specific Tag section. It's quite straightforward: you just lower the weight of the tag in question and see if it helps. It's mostly trial and error, and there are no fixed solutions for everything, so I leave it you you.","title":"Syntax; Prompt's Weight Manipulation"},{"location":"advanced/#syntax-break-is-love-break-is-life","text":"It's a little bit difficult to approach this section, but let's start from understanding what it does. Let's have an example, no humans, building, house, dusk BREAK door, window, lamppost, bench,, . Here, I use BREAK to separate two parts of the prompt into different CLIPs, so that no humans, building, house, dusk (First CLIP) and door, window, lamppost, bench, (Second CLIP) are processed (somewhat) separately. In general, first CLIP is much more important than the second: image's composition is about 70% defined by the very first CLIP. What does it achieve?","title":"Syntax; BREAK is love, BREAK is life"},{"location":"advanced/#multiple-defined-character-gens-without-extensions","text":"First, it's the most powerful way to fight Tag Bleeding, and second, it allows you to define different concepts/characters completely separately. For example, in a prompt like 2girls, looking at viewer, smile, side-by-side, hand on another's shoulder, red shirt, jeans, fox ears, orange hair, cowboy shot, white background BREAK black hair, sundress, pointy ears, it's the only way to get a satisfactory result. What to note: Concepts applicable to the whole image must be in the first CLIP. In this case, it's 2girls, looking at viewer, side-by-side, hand on another's shoulder, cowboy shot, white background . You must use an action that involves multiple characters. Here, it's side-by-side, hand on another's shoulder . It'll be much harder to achieve anything meaningful without something to involve all characters. (note: it is possible to define the active participant by having the action tag in the corresponding character's part of the prompt. Here, it's 9/10 times the girl with fox ears doing the hug, and if we move hand on another's shoulder to the second CLIP, it will be an elf.) Keep overprompting to absolute minimum . Here, an extra animal ears to the first part of the prompt or elf to the second will ruin the gen. This is where manipulating weights will come very useful. If you define anything appearance-wise with the first character, do the same to the other. Here, we defined that the girl with fox ears has red shirt, jeans, orange hair, . It means that we also should define same things for our elf, so we add black hair, sundress to the second CLIP. Try to keep the image complexity low. If you can, avoid extremely defined backgrounds, difficult scenarios and such. Multiple character gens are already pushing image gen models to their absolute limits, so you have to be reasonable in what you want to achieve. Don't waste hours trying to get the impossible like I do sometimes. Do not expect it to work perfectly. Sometimes you get a 90% consistency, this prompt works about 50% of the time. That's basically it for simple 2 character gens. With 3 characters, especially if the prompt gets more complex, we get into an almost esoteric territory, so I'll talk about separately later. -> <- -> Metadata: 2girls, looking at viewer, smile, side-by-side, hand on another's shoulder, red shirt, jeans, fox ears, orange hair, cowboy shot, white background BREAK black hair, sundress, pointy ears, Negative prompt: extra ears, worst quality, bad quality Steps: 29, Sampler: DPM++ 2M, Schedule type: Karras, CFG scale: 5, Seed: 687164227, Size: 832x1216, Model hash: 0842c2a1d8, Model: Amanatsu_v11, Clip skip: 2, ADetailer model: face_yolov8n.pt <- And another; -> <- -> Metadata: 1boy, looking at viewer, on couch, (spread arms:0.7), (spread legs:0.7), black hair, smirk, furry, wolf boy, loafers, indoors, suit jacket, open jacket, dress pants, straight-on, general BREAK 3girls, (sitting on lap:1.2), sitting on person, elf, shy, maid, long hair, short hair, pink hair, white hair, Negative prompt: worst quality, bad quality, simple background, Steps: 28, Sampler: DPM++ 2M, Schedule type: Karras, CFG scale: 5, Seed: 535719555, Size: 1216x832,, Model: Amanatsu_v11, Denoising strength: 0.39, Clip skip: 2, ADetailer model: face_yolov8n.pt, Hires CFG Scale: 5, Hires upscale: 1.6, Hires steps: 23, Hires upscaler: R-ESRGAN 4x+ Anime6B, <- Yes, 3girls , this is deliberate. I will expand this section one day.","title":"Multiple (Defined) Character Gens without Extensions"},{"location":"advanced/#multiple-non-original-character-gens","text":"Stuff is significantly easier if you just use Character tags. For example, let's get a gen with these characters: stelle \\(honkai: star rail\\), belle \\(zenless zone zero\\), lumine \\(genshin impact\\) . It follows the principles above with some slight alterations, so a few notes: - With 2-character gens, you can pass without using BREAK at all. It would simply be 2girls, girlA, girlB, <Mutual Action Tags>, <etc...> , and it's enough. For separate actions, just use BREAKs, and keep the action inside the corresponding character's section. - If the model doesn't have enough training data about characters, it might start getting their features wrong or mess them up. In this case, it's BREAKs all over again; just specify messed up features for each character separately, inside their corresponding sections. - You shouldn't mix up Non-Original characters with defined characters; it won't work 90% of the time. All the characters in your gen should be either Non-Original or Defined. - In the gen below, I used a separate CLIP for actions. The reasoning is that I don't care which character performs what action, so anyone of them can do it. Using a separate CLIP for composition/style/misc tags is also very viable, I do it often. -> <- -> Metadata: 3girls, stelle (honkai: star rail), grey hair, yellow eyes, BREAK belle (zenless zone zero), blue hair, green eyes, BREAK lumine (genshin impact), blonde hair, yellow eyes, BREAK looking at viewer, arm around shoulder, w, looking at viewer, smile, wide shot, simple background, Steps: 1, Sampler: Euler, Schedule type: Karras, CFG scale: 5, Seed: 43880133, Size: 128x128, Model hash: 0842c2a1d8, Model: amanatsuIllustrious_v11, Denoising strength: 0.75, Clip skip: 2, ADetailer model: face_yolov8n.pt <-","title":"Multiple (Non-Original) Character Gens"},{"location":"advanced/#syntax-prompt-merging-delaying-prompts-and-keeping-the-quality-with-styles","text":"Let's discuss some in-built Stable Diffusion scripts. One of the more useful is a Prompt Switch script that looks like this: [A:B:x] , where A is the first prompt, B is a second prompt, and X is a fraction of total steps at which the switch occurs. For example, we have [pixel art:2000's \\(style\\):0.2] . The translation of it would be: First 20% of the gen, the active prompt is pixel art . At 20%, it's replaced with 2000's \\(style\\) and stays like this until the end. What it means in practice? The overall composition of the gen is almost completely defined by about 2-3 first steps (With Euler or DPM++ 2M. It can vary based on the Sampler and Scheduler). By using [A:B:x] , we not only merge two styles into one, but also choose which style does the overall image composition; we're talking little details, the background, the look of the character, the quality of the image . This is where we can do another thing. In [A:B:x] , both A and B can be completely empty. Let's just not enter any A. We'll get: [:pixel art:0.2] . First, some theory. When you're using a Style Tag, an Artist Tar or a LoRA, it absolutely always has a negative impact on the gen's quality. With just a single Style or two styles at reduced weights, this impact is negligible. However, if an Artist or Style in question doesn't have enough images (<100 with Artists, <1k for Styles); or we start adding 3+ Styles/Artists/LoRAs, the gen's quality will get noticeably worse. One of the reasons is because the model will just have no idea how to make the image's composition with given Style Prompt. To spare the model from it, we can just have the Style Prompt disabled completely during the first few steps of the gen. With [:pixel art:0.2] , it only kicks in once the composition is already completely finished, and all it does is what it's supposed to do: apply style to the gen. Note that 0.2 is an arbitrary number; you may want to test in range from 0.15 to 0.5 .","title":"Syntax; Prompt Merging, Delaying Prompts and Keeping the Quality with Styles"},{"location":"advanced/#syntax-prompt-addition","text":"When it comes to solo gens, you can also sometimes add one tag to the other. It's mostly unpredictable and there are often better ways to achieve stuff, but it's an option. For example red hair, blue hair has a chance of generating a hair color that's between red and blue. It can also generate a multi-colored hair, but it's easier to just specify for multicolored hair (+ you can pick a specific type of it from here ) and have a prompt like red hair, blue hair, multicolored hair . There is another way to specifically merge tags using | pipe. For example, (red hair|blue hair:1.0) . What pipe does is, it switches from one prompt to the other each step. Whille it is a way to specific get a blend between A and B, it is unreliable. This is where we can return to [A:B:x] . For example, [red hair:blue hair:0.5] . This is a much more reliable and better way to achieve good merging. There are also other ways to use it, for example red hair, [:blue hair:0.2] and such, but I leave it to you.","title":"Syntax; Prompt Addition"},{"location":"advanced/#loras","text":"LoRAs, or Low-Rank Adaptations, are a cheap and reliable way to impact the generation right on the model's technical level, basically finetuning it on the fly. I'm not the biggest LoRA enthusiast out there, so I'll be somewhat short. First, with Illustrious, I suggest using LoRAs only for styles/detail . I tried a few, like, pose and \"character appearance\" LoRA, and, well, just using Booru tags seems like a better option both quality and versatility-wise. It's just my opinion though. Second, do not use more than two style LoRAs at the same time , at least without making them skip first few steps. Without trickery, I recommend keeping the overall weight for two LoRAs below 1.1-1.3, for example, lora_A would have a weight of 0.7, and lora_B will have a weight of 0.4, making the total weight 1.1. These settings seem like they give the most style while keeping the quality basically the same as without LoRAs. Honestly, the very same goes for Style Tags. With Style Tags + LoRAs (that are applied like I suggested previously), a total of three seems stable. Just be mindful of weights. That's mostly all I have to say. LoRAs can be of various quality and made for different purposes/models, so it's just trial and error. You just apply them, tinker with weights and see if the results are satisfactory.","title":"LoRAs"},{"location":"advanced/#generation-parameters","text":"","title":"Generation Parameters"},{"location":"advanced/#generation-parameters-introduction","text":"Congrats on making it this far (yes, I'm running out of ideas on how to begin sections). For most intents and purposes, you can simply setup your parameters once, never change them again and be happy. The following is parameters I consider to be stable and \"good\". Please keep in mind that I'll explore other parameters later, you may want to see them: - Sampler: DPM++ 2M Karras - Steps: 28 - CLIP Skip: 2 - CFG: 4 - Resolution: 832x1216 (or reverse), or 1024x1536 (if you're using a model based on Illustrious 1.0/1.1/2.0). And you kinda never have to change them, they are just good. But there are cases when you'd want to try something new and models that just won't take this. For a much more detailed technical information on how samplers/schedulers works and how they're different, please check 11yu's Rentry on Tech for Diffusion Image Gens .","title":"Generation Parameters; Introduction"},{"location":"advanced/#generation-parameters-align-your-steps-and-full-quality-gens-in-just-12-steps","text":"Available on most local frontends, there is a Scheduler called Align Your Steps, or AYS for short. AYS uses some ancient wizardry to make the gens in 10-12 steps that look the same as 28 steps Karras or Euler a. With parameters, I suggest using DPM++ 2M Sampler, regular Align Your Steps scheduler and from 10 to 12 steps. I don't think that AYS is negatively impacted by CFG, so I just keep it at 5 and go lower as needed. -> <- -> Metadata: 1girl, makima (chainsaw man), looking at viewer, sitting, crossed legs, on chair, head tilt, indoors, office, cowboy shot, masterpiece,best quality,amazing quality Negative prompt: bad quality,worst quality,worst detail,sketch,censor Steps: 12, Sampler: DPM++ 2M, Schedule type: Align Your Steps, CFG scale: 5, Seed: 3757563295, Size: 832x1216, Model hash: 0842c2a1d8, Model: Amanatsu_v11, Clip skip: 2, ADetailer model: face_yolov8n.pt <- Keep in mind that while AYS is great for simple gens, it can start messing up as you increase the complexity of your gen. The most noticeable issue I had is AYS merging hands of multiple characters into a single blob. Besides that, AYS doesn't have any artifacts and looks incredible. It works with LoRAs, Styles and Artists with no issue. On my hardware (RTX 3070 8 GB), it lowers the generation time from 15-20 seconds with Karras and 28 steps to 9-10 seconds with AYS.","title":"Generation Parameters; Align Your Steps, and Full-Quality Gens in Just 12 Steps"},{"location":"advanced/#generation-parameters-extra-quality-gens-in-60-steps","text":"There are two notable Samplers that give you some beautiful gens without Hires and sometimes even replacing ADetailer. These are, DPM++ 3M SDE Exponential and IPNDM Automatic . Both require at least 40 steps, and I consider 60 steps to be a sweet spot, sometimes you may need up to 80. In my experience, IPNDM is better than DPM++ 3M SDE ; IPNDM shows much less artifacting than DPM++ 3M SDE, which is understandable, given the nature of SDE (SDE adds some noise each step. More variety, less stability), but I still use it occasionally. In some cases, these two Samplers can even replace Hires, especially given that Hires is much slower. With some gens, faces without ADetailer look even better than with it, and it's especially mindblowing. -> <- -> Metadata: 1girl, makima (chainsaw man), looking at viewer, sitting, crossed legs, on chair, head tilt, indoors, office, cowboy shot, masterpiece,best quality,amazing quality, Negative prompt: bad quality,worst quality,worst detail,sketch,censor Steps: 70, Sampler: IPNDM, Schedule type: Normal, CFG scale: 5, Seed: 3757563295, Size: 832x1216, Model hash: 0842c2a1d8, Model: Amanatsu_v11, Clip skip: 2, ADetailer model: face_yolov8n.pt <-","title":"Generation Parameters; Extra-Quality Gens in 60 Steps"},{"location":"advanced/#generation-parameters-lcm-and-draft-gens-in-just-5-steps","text":"Note that to use LCM, you must download it's LoRA and include it in the gen at weight 1. I suggest using LCM sampler with SGM Uniform scheduler. Your CFG must be in range from 1 to 1.5, I suggest the latter. LCM gives you perfectly fine gens extremely quickly, but they have significantly less detail than usual + there can be occasional artifacting. -> <- -> Metadata: 1girl, makima (chainsaw man), looking at viewer, sitting, crossed legs, on chair, head tilt, indoors, office, cowboy shot, masterpiece,best quality,amazing quality , Negative prompt: bad quality,worst quality,worst detail,sketch,censor Steps: 5, Sampler: LCM, Schedule type: SGM Uniform, CFG scale: 1.5, Seed: 3757563297, Size: 832x1216, Model hash: 0842c2a1d8, Model: Amanatsu_v11, Clip skip: 2, ADetailer model: face_yolov8n.pt <-","title":"Generation Parameters; LCM, and Draft Gens in Just 5 Steps"},{"location":"advanced/#generation-parameters-ipndm_v-and-quicker-high-quality-gens","text":"Found out about it recently, but it's just incredible. This sampler gives better quality at 40 steps than DPM++ 3M or regular IPNDM do at 80+ steps. First, make sure to use either Karras or Exponential schedulers. I recommend sticking to 40 steps, 30 is also viable, and technically you can go all the way down to 15 steps. Do not go above 40 steps; the image will get worse or break down. -> <- -> Metadata: 1girl, makima (chainsaw man), looking at viewer, sitting, crossed legs, on chair, head tilt, yellow eyes, indoors, office, cowboy shot, masterpiece,best quality,amazing quality Negative prompt: no pupils,, worst quality, bad quality, simple background, Steps: 40, Sampler: IPNDM_V, Schedule type: Karras, CFG scale: 5, Seed: 3757563295, Size: 832x1216, Model: Amanatsu_v11 <- And another; Note that it has no ADetailer; -> <- -> Metadata: 2girls, looking at viewer, smile, side-by-side, hand on another's shoulder, red shirt, jeans, fox ears, orange hair, cowboy shot, white background BREAK black hair, sundress, pointy ears Negative prompt: no pupils,, worst quality, bad quality, simple background, Steps: 40, Sampler: IPNDM_V, Schedule type: Karras, CFG scale: 5, Seed: 54443983, Size: 832x1216, Model: Amanatsu_v11 <-","title":"Generation Parameters; IPNDM_V and Quick(er) High-Quality Gens."},{"location":"advanced/#generation-parameters-deis-another-high-ish-quality-sampler","text":"DEIS is the High-Quality and relatively low-steps of IPNDM_V, but without it's downsides. It's incredibly stable and gives amazing results. Make sure to use it with SGM Uniform scheduler. I suggest 40 steps, but you can go up to 60 if needed. Comparing to IPNDM_V, they're about similar in quality; one is better in some aspects and worse in another, and vice-versa. It still gets you much more detail compared to DPM++ 2M/Euler a in just slightly more time, and it's more stable and reliable than IPNDM_V, so I can confidently recommend DEIS. -> <- -> Metadata: 1girl, standing, casting spell, magical weaving chaotic threads of white light coming out of hands, magic filling the area, outstretched arms, magic, fighting stance, white cloak, long hair, blonde hair, solo BREAK lens flare, chromatic aberration, diffraction spikes, fisheye, outdoors, night, embers, full body, dutch angle, (from side:0.4), (three quarter view:0.5), battle, war, army, soldier, fantasy Negative prompt: worst quality, bad quality, simple background, Steps: 40, Sampler: DEIS, Schedule type: SGM Uniform, CFG scale: 4.5, Seed: 1082317166, Size: 1536x1024, Model hash: 351447d6cd, Model: annnslmixillustrious_v31, ADetailer model: face_yolov8n.pt <- And another one; -> <- -> Metadata: 2girls, standing, outstretched arm, hand on another's hand, black shirt, brown skirt, long hair, baseball cap, white background, two-tone background, dual persona, human, from side, (full body:0.5), portal (object), BREAK 2girls, floating, midair, reaching towards another, looking at another, hand on another's hand, purple dress, darkness, dark background BREAK (chromatic aberration:1.2), (glitch:1.2), vhs artifacts, (digital dissolve:0.9), Negative prompt: worst quality, bad quality, 3girls, 4girls, mirror, Steps: 60, Sampler: DEIS, Schedule type: SGM Uniform, CFG scale: 4.5, Seed: 869983613, Size: 1536x1024, Model hash: 351447d6cd, Model: annnslmixillustrious_v31, ADetailer model: face_yolov8n.pt, ADetailer confidence: 0.3, ADetailer dilate erode: 4, ADetailer mask blur: 4, ADetailer denoising strength: 0.4, ADetailer inpaint only masked: True, ADetailer inpaint padding: 32, ADetailer model 2nd: hand_yolov8n.pt <-","title":"Generation Parameters; DEIS, Another High-ish Quality Sampler."},{"location":"advanced/#generation-parameters-cfg-pain-and-incredibe-gens-comfyui","text":"This section is only for ComfyUI, Forge doesn't have CFG++ samplers and reForge is dead + has a different implementation of it. So, CFG++. The main principle behind it is that the sampler itself chooses an appropriate CFG scale. When it comes to practical usage, in general , you choose CFG two times lower than without CFG++; if you usually run CFG 4, use CFG 2 with CFG++; this is in theory. In practice, I suggest first trying CFG 1; if it's shit (it most likely will be), try going to CFG 1.5, and then increasing until you get a good image. CFG 1.5 - CFG 1.9 seem to be good values for me, but it can be different from model to model, and even gen to gen sometimes. As a CFG++ sampler, I suggest res_multistep_cfg_pp , and either SGM Uniform or Karras as a scheduler. In my experience, using step count higher than 30 makes gens worse, so I recommend sticking to it and doing a Hires pass later if you want. In my eyes, it may be the best sampler we have; it straight up improved the quality of lighting, colors, and detail compared to the same CFG 3 gen of DPM++ 3M SDE at 60 steps. More after the image: -> <- -> Generation Parameters: Positive: 1girl, kneeling, praying, magic, golden threads of magic, magic circle, magic symbols, blue hair, long hair, white coat, red eyes, elf, dark, indoors, church, light particles, three quarter view, full body, solo; Negative: no pupils, Sampler: res_multistep_cfg_pp, scheduler: SGM Uniform, Step Count: 30, CFG Scale: 1.8, Face Detailer, 1.5x Hires Pass, seed: 43, 1024 x 1536, checkpoint: Nova Orange v9.0. ComfyUI. <- So, let's talk about practical usage in more detail. Main advantages of CFG++ are light contrast, colors and color contrast. With conventional samplers, when you try to go for a darker gen, you often get an image that's grey and still bright. Using some magic, CFG++ just solves the issue entirely. You (almost) get true black and incredible color contrast. The only issue with CFG++ is that it's very picky about your CFG scale, sometimes requiring you to set different values for different gens on the same model. You get something a little bit wrong, and you get an overexposed / oversaturated gen. Right now, I use this sampler for almost every model, it's just that good. ( I lost prompts for individual images, so no prompts. Sorry. ) Generation Parameters: Style Prompt: 2d, sketch, oekaki, limited palette, black background , Negatives: no pupils, loli, child , Sampler: res_multistep_ancestral_cfgpp , Scheduler: Karras , CFG Scale: 1.8, Steps: 30, Checkpoint: Nova Anime v7.0, Resolution: either 1536 x 1024 or 1024 x 1536 . No Fixes, Hires or additional passes.","title":"Generation Parameters; CFG++, Pain and Incredibe Gens (ComfyUI)"},{"location":"advanced/#generation-parameters-hires","text":"Hires is an incredible feature to get higher resolution gens with increased detail. I suggest the following parameters: - Upscaler: R-ESRGAN 4x+ Anime6b OR Lanczos - Hires Steps: 20-22 - Denoising Strength: 0.39-0.43 - Upscale by: 1.5 - 1.65 -> <- -> Metadata: 1girl, makima (chainsaw man), looking at viewer, sitting, crossed legs, on chair, head tilt, indoors, office, cowboy shot, masterpiece,best quality,amazing quality, Negative prompt: bad quality,worst quality,worst detail,sketch,censor Steps: 28, Sampler: DPM++ 2M, Schedule type: Karras, CFG scale: 5, Seed: 3757563295, Size: 832x1216, Model hash: 0842c2a1d8, Model: Amanatsu_v11, Denoising strength: 0.39, Clip skip: 2, ADetailer model: face_yolov8n.pt, Hires CFG Scale: 5, Hires upscale: 1.5, Hires steps: 20, Hires upscaler: R-ESRGAN 4x+ Anime6B <-","title":"Generation Parameters; Hires"},{"location":"advanced/#generation-parameters-refiner","text":"Refiner means that the first x fraction of steps is done by one model, and then it's replaced with another model. It's trial and error, so there's not that much to talk about. One thing I want to note is, remember that the overall composition of the image is done by the first model, not the refiner model. You can have the first model to 2-4 steps and then switch to another model; as a result, the image's composition is done by model A and the whole style is done by model B.","title":"Generation Parameters; Refiner"},{"location":"advanced/#prompting-iterating-on-your-gens","text":"This part is more of a philosophical view on how you should do prompting. I like to call it Iterative Prompting . The main point is, you start from drafting the overall image, and then focus on each part of the image, following the Prompt Order. You refine each part of the prompt, generate the image, reflect on the result and keep refining the prompt until you get the image you want consistently, after that you enable all the resource-intensive stuff like Hires or High-Steps Samplers and play with styles. Let's have a big example. I come up with an overall idea of the gen. Let's start with an idea: a creepy/dangerous punk girl in a darker style. After some thinking, I come up with this prompt: 1girl, looking at viewer, head tilt, raised eyebrow, smirk, holding cane, red jacket, open jacket, long hair, multicolored hair, dark, darkness, city, high contrast, and gen. I look at the gens. While high contrast definitely gives an interesting look, the overall image is too dark. smirk is also not optimal, her expression is really weird in a bad way. Besides this, her overall look is pretty close to what I wanted, so I also need to add Composition tags. Reflecting on this, I edit the prompt, 1girl, looking at viewer, head tilt, raised eyebrow, (smirk:0.5), (grin:0.5), holding cane, red jacket, open jacket, long hair, multicolored hair, dark, darkness, city, (high contrast:0.7), cowboy shot, close-up, and generate it. I'm already quite happy with the image, but I have some other ideas I want to try. I'd like to have more character focus, so I remove background tags. I also want even less dark image, so I lower the weight of dark . New prompt is: 1girl, looking at viewer, head tilt, raised eyebrow, (smirk:0.5), (grin:0.5), holding cane, cane, red jacket, open jacket, long hair, multicolored hair, (dark:0.5), darkness, (high contrast:0.7), cowboy shot, close-up, . Time to gen First, she's tilting her head too much. Second, to make her more visually interesting, I add hererochromia , then specify two hair colors and rely on Tag Bleeding to also do eye colors. To add more to the composition, some slight dutch angle . Prompt turned into: 1girl, looking at viewer, (head tilt:0.7), (smirk:0.5), (grin:0.5), holding cane, cane, red jacket, open jacket, heterochromia, long hair, white hair, red hair, multicolored hair, (dark:0.5), darkness, (high contrast:0.7), cowboy shot, close-up, (dutch angle:0.3), and I gen it. At this point, I feel pretty happy about the result, so I start adding style and LoRAs, as well as doing some minor edits. New prompt: 1girl, looking at viewer, (head tilt:0.7), (smirk:0.5), (grin:0.5), holding cane, cane, red jacket, open jacket, heterochromia, ringed eyes, long hair, white hair, red hair, multicolored hair, (cowboy shot:0.7), close-up, (dutch angle:0.3), traditional media, <lora:illustrious_quality_modifiers_masterpieces_v1:0.7> <lora:illustriousXL_stabilizer_v1.72:0.35> Not specifying backgrounds and having simple background in the negatives really ruined the gen, so I fix it: 1girl, looking at viewer, (head tilt:0.7), (smirk:0.5), (grin:0.5), holding cane, cane, red jacket, open jacket, heterochromia, ringed eyes, long hair, white hair, red hair, multicolored hair, dark background, (cowboy shot:0.7), close-up, (dutch angle:0.3), traditional media, <lora:illustrious_quality_modifiers_masterpieces_v1:0.7> <lora:illustriousXL_stabilizer_v1.72:0.35> Now I feel completely happy. I take the seed of my favourite gen and reuse it, this time with Hires. Woila, we're done. -> <- -> Metadata: 1girl, looking at viewer, (head tilt:0.7), (smirk:0.5), (grin:0.5), holding cane, cane, red jacket, open jacket, heterochromia, ringed eyes, long hair, white hair, red hair, multicolored hair, dark background, (cowboy shot:0.7), close-up, (dutch angle:0.3), traditional media, , Negative prompt: worst quality, bad quality, Steps: 28, Sampler: DPM++ 2M, Schedule type: Karras, CFG scale: 5, Seed: 1003245552, Size: 832x1216, Model hash: 0842c2a1d8, Model: Amanatsu_v11, Denoising strength: 0.39, Clip skip: 2, ADetailer model: face_yolov8n.pt, Hires CFG Scale: 5, Hires upscale: 1.6, Hires steps: 22, Hires upscaler: R-ESRGAN 4x+ Anime6B <- From start to finish, this generation took 50 images to refine the first prompt into the last. This is the way I do all my gens, and I highly recommend following the same principle. It's extremely easy to mess up at the very beginning and spend hours trying to fix it; I've been there.","title":"Prompting; Iterating on Your Gens"},{"location":"advanced/#prompting-prompt-complexity","text":"This is a quite ephemeral thing that I think is extremely important. I consider Prompt Complexity to be this: it's the amount of separate concepts you require the model to understand and generate correctly. Things like Styles are not separate specific concepts, so they're not increasing the complexity. Appearance tags on solo gens have no other interpretation than, well, that they're worn by this character, so it's also not an issue. Things start getting fun once you start doing complicated stuff. Asking for two separate actions by a single character relies on the model understanding how it's going to look, so it increases the complexity a lot. Defining two separate characters makes the complexity skyrocket: the model has to associate different characteristic (that are technically applicable to both characters) to separate characters. It's fine if you assign actions that require multiple characters, but stuff like black hair , pointy ears , animal ears can be assigned to either, and the model will most often just make both characters like this. Resulting from this, Prompt Complexity is a sum of every Tag Bleeding, Redundant Tags, Overtrained and Undertrained Tags, as well as ambiguous tags. The less complex your prompt is, the more consistency and quality you get. You should always look for opportunities to make your prompt simpler, to use a more specific (but still populated) tag, replace multiple tags with just one, avoid redundancy and bleeding. It's a nice habit that may save hours of work.","title":"Prompting; Prompt Complexity"},{"location":"advanced/#new-illustrious-101120-based-models","text":"So, there are finally models based on new Illustrious 1.0/1.1 that don't suck ass, and I finally got to try them. You can find a comparison of a few of them in my thread on Aegiscord. They're pretty good, and because of their support of higher resolutions and Natural Language, I use them quite often. Let's talk about it in more detail. You can see examples of them in the DEIS section. Higher Resolutions ; While Illustrious 1.0/1.1-based models supports a variety of 1536 x 1536 resolutions, I suggest sticking to two: 1024 x 1536 or 1536 x 1024 ; they show the most prompt adherence and overall quality. Higher resolution helps immensely in face quality, small/subtle details, and detail in general. Keep in mind that while these higher resolutions are more than usable, prompt adherence of them is worse than with regular SDXL sizes. You can always use good old 832 x 1216 , 1216 x 832 , 1024 x 1024 , etc. Natural Language Prompting ; While tags remain the main way of prompting, you can finally add in some Natural Language for more complicated prompts. For example, in the magical girl image I used as an example for DEIS above, I used magical weaving chaotic threads of white light coming out of hands , and it worked almost perfectly. As anything related to Natural Language, there can't be no guidelines or specific rules for usage, so all I can suggest is experimenting; but I'd advise that if you can do a specific thing you want with Tags, you should do just that. Natural Language is unpredictable, and I think it should be used only if there's no other way. Impressions: I'm not ready to speak definitively on them right now, but I've made my impressions on them. Compared to Illustrious 0.1, Illustrious 1.0/1.1 based models show much more creativity if you prompt for it, and they can easily do extremely complex gens that 0.1 would've struggled with. 1.0/1.1 is not fool proofed in any way, and complex gens still take a lot of knowledge and understanding of how tag, prompt and gen, but Natural Language and improved creativity makes the job noticeably easier. I'm not ready to abandon Amanatsu completely, but AnnnslMix specifically is really good. Stuff like this makes me excited for what's to come. UPD: Illustrious 2.0 :So, we finally got good Illustrious 2.0 models, like Nova Orange and Nova Anime. First, they seem to really hate regular samplers, like DPM++ 2M and regular Euler. From my testing, these models require you to use a sampler that does noise injection, so it's samplers that have Ancestral or SDE in them. I got the best results using Euler ancestral and res_multistep_ancestral . Besides that, they really like CFG++ samplers, so I recommend using res_multistep_ancestral_cfgpp with CFG at about 1.5 - 1.7 in Comfy. Second, they're now much more coherent at higher resolutions, sometimes showing just better results there compared to regular stuff like 832 x 1216 . Note that it results in higher VRAM usage; with my RTX 3070 8 GB, I usually reach about 7.0 - 7.5 GB usage. Third, I don't see any improvements in Natural Language compared to Illustrious 1.0/1.1; you can try your luck using them, but it's preffered to stick to tags if you can. Fourth, from my testing, Illustrious 2.0 models show much better prompt adherence and understanding; you can combine more difficult tags, sometimes ditch the \"Prompt for what you want to see\" principle and other creative stuff; you are not as likely to ruin your gen as before. I suspect recent Illustrious 2.0-based models to be heavily overtrained, but it's something to be researched.","title":"New Illustrious 1.0/1.1/2.0-Based Models"},{"location":"advanced/#v-pred-models-how-to-use","text":"While I won't be going into deail about how V-Pred models are different from eps-pred, there are some differences in how you should approach them. In general, if you see that a model you're using is a v-pred (or there are unexpected artifacts/noise in the image), you should know: - DPM++ 2M Karras and some other samplers/schedulers do not work . - DPM++ 2M SGM Uniform, Euler a Normal, DEIS SGM Uniform and IPNDM_V SGM Uniform are my recommended samplers. - I highly recommend using a CFG++ sampler, like res_multistep_cfgpp . - Avoid using dark , day and other tags that change the lighting. While v-pred is famous for it's great color range and lighting, it's extremely prone to over-exposing gens or making them too dark. - Be ready for instabilities and jank. I personally do not like v-pred at all, and all models using it (that I tested) sucked. You can get these advantages of v-pred on regular eps-pred models with some prompting (to some extent), like contrast, hdr, vibrant, shadow, dark, reflection, and lighting tags.","title":"V-Pred Models; How to Use"},{"location":"comfy_guide/","text":"ComfyUI ComfyUI; Introduction So, the big boy. ComfyUI is arguably the most versatile and customisable frontend out there, but it\u2019s pretty hard to get into. It\u2019s pretty interesting to me, just because I can try various methods of doing various things, but it\u2019s really hard to just start using; you need to spend hours to get basic functionality. For new users, I highly suggest first picking a simpler frontend like Forge, just to learn basics and have all functions easily available. But for folks who\u2019re interested in what Comfy can do, here\u2019s a detailed section on it. Important. This section is intended for people who have at least some understanding of how image gen works, I won\u2019t be describing every single little detail . First, ComfyUI is a node-based frontend. All functionality is achieved by adding and connecting nodes. On the left of most nodes, there are Inputs , for example image , latent , clip , etc. In the middle, there are different parameters, like denoising , cfg , checkpoint_name , etc. On the right, there are outputs, like the same image , latent , etc. Keep in mind that most parameters can be also turned into inputs, for example you can connect a node called Primitive to cfg , steps , text , etc. Most nodes can take multiple nodes connected to a single input/output, for example, we connect both Load Checkpoint and Load LoRA to the same model input of KSampler. I'll talk about these nodes later. ComfyUI; Installation Go to Comfy\u2019s Github . Here, go to Getting Started; you can pick either a Desktop Application that will install Comfy as an app (it should be the simplest option), or Windows Portable Package if you want a portable instance. They should be effectively the same, but I personally use the Windows Portable Package. To update Comfy, go to /update/ and start update_comfyui.bat . To provide models from another frontend (Forge, in this case) to Comfy, go to /comfy/ and find a file called extra_model_paths.yaml.example . Rename it to extra_model_paths.yaml and open it in Notepad. Find base_path: line under a111: and replace the path/to/stable-diffusion-webui/ string to your A1111/Forge/reForge path, for example base path: E:/Forge/webui . ComfyUI; Custom Nodes Sooner or later, you will want to use custom nodes. To get them, install ComfyUI Manager by following the instructions on Github. After that, you\u2019ll get a Manager button in your Comfy. Press it \u2014> Custom Nodes Manager, and you\u2019ll be able to download custom nodes. I\u2019ll mention a few of them later, but you\u2019d want to get ComfyUI-custom-scripts as it gives a ton of essentials nodes and it\u2019s a prerequisite for a lot of other custom nodes. ComfyUI; Building Your Workflow While there are example workflows available, I highly recommend building your own. This is an incredible learning experience, and you\u2019d know exactly what it does and how it works. For now, I\u2019ll be using default nodes exclusively. Let\u2019s begin KSampler; The Main Node KSampler is a node that transforms latent noise into your image. To find it, you can either RMB \u2014> Add Node \u2014> Sampling \u2014> KSampler, or search for it in Node Library. Let\u2019s take a look at it and see what inputs it takes, what parameters has and what it outputs. KSampler; Loading Model The first input KSampler takes is a model. To load a model, we will use a node called Load Checkpoint. There, you can select which model you want to use and output the model, CLIP and VAE. We\u2019ll figure out CLIP and VAE later, but now, let\u2019s connect MODEL from Load Checkpoint to KSampler. KSampler; Positive, Negative and CLIP in General. Positive and Negative are your Positive and Negative prompts. To load them, we will use a node called CLIP Text Encode (Prompt). It inputs CLIP, which we connect to Load Checkpoint, and outputs Conditioning, which we connect to KSampler. You should make two CLIP Text Encodes, one for Positive prompt and second for Negative Prompt. You can recolour and rename nodes, to have them distinct for convenience. KSampler; Latent Image Latent Image is the noise that Stable Diffusion resolves into your image. To load it, we will use Empty Latent Image node. You just connect Latent output to Latent input of KSampler. There, make sure to select a correct resolution you want your image to be, 832 x 1216 for example. KSampler; Parameters Let\u2019s see what parameters you can configure in KSampler. - Seed. Selects a seed for your gen. - Control after Generate. Determines whether Seed changes or not after a single gen, and how it changes. Fixed to not have it changed, Increment for +1, Decrement for -1, Random for random. - Steps. Regular Steps counter, basically how much times it interacts with the image. - Sampler Name. The sampler you want to use. - Scheduler. What scheduler you want to use. - Denoise. The fraction of image which is turned into noise. Always keep at 1.0 for txt2img, values lower than 1 are needed for img2img. KSampler; Output and VAE Decode. When KSampler is done, it\u2019ll output your image via Latent output. To turn this Latent into an image, add the VAE Decode node. Connect Latent output of KSampler to VAE Decode, then connect VAE output of Load Checkpoint to VAE Decode, and add Save Image node to connect Image output of VAE Decode to Image input of Save Image. Now, we\u2019re finally done with basic img2img. Configure everything, enter your prompt and press Run at the bottom. KSampler; Final Result This is the bare minimum you need for txt2img gens. From now on, I\u2019ll be less detailed about each node. Workflow; Separating your Prompt and Quality Presets First, I\u2019ll specify custom nodes with Node Name (custom-node-pack-title) . To download them, just search the name in ( ) in Custom Nodes Manager. It\u2019s extremely convenient to just enter your prompt in a separate box and have your quality tags applied automatically. To do this, we\u2019ll use String Function node (comfyui-custom-scripts). Add two of them. Then, we\u2019ll use a default node called Primitive. It\u2019s empty at first, but will double the input it\u2019s connected to. Connect Primitive to text_a. Then, you can either connect another Primitive to text_b or use Preset Text (comfyui-custom-scripts), connecting it to text_b. In String Function, choose Append in Action and Yes in tidy_tags. Connect String output to text of Clip Text Encode and voila, it\u2019s done. You use Primitive connected to text_a for your regular prompt, and just change Preset Text or another Primitive connected to text_b. Workflow; Cleaning your Workflow with Pipes So, there\u2019s a very useful custom nodes pack called comfyui-impact-pack . It\u2019s mostly used for Face Fixing (that I\u2019ll mention later), but it also introduces incredible Pipe nodes. They\u2019re great for an easier way to connect multitude of nodes, especially if you also do Face Fixing and/or Hires. First, we have a ToBasicPipe. It takes Model, CLIP, VAE, Positive, and Negative, and outputs a basic_pipe. Using FromBasicPipe, you can output them. . You can also use KSampler (pipe). This is extremely useful for more complicated workflows. Workflow; Face Fixing For Face Fixing, we\u2019ll use comfyui-impact-pack and ComfyUI Impact Subpack . First, let\u2019s create a FaceDetailer (pipe) node. Then, we make a BasicPipe -> DetailerPipe node. We connect Basic_pipe. Then, we add UltralyticsDetectorProvider, select face_yolo8m.pt and connect BBOX_DETECTOR to BasicPipe -> DetailerPipe . Now we create SamLoader (Impact), leave everything at default and connect SAM_MODEL. Any parameters in BasicPipe -> DetailerPipe don\u2019t need to be changed. We connect detailer_pipe to FaceDetailer. Image input of FaceDetailer needs to be connected to VAE Decode. As for the parameters of FaceDetailer, I suggest leaving everything at default, but changing bbox_crop_factor to 2, so that the area of FaceFixing will not be that big. After that, you connect Image output to Save Image, and we\u2019re done. Workflow; Hires Let\u2019s go to Hires. In my case, this Hires will be going after ADetailer, so the current gen is Image. First, we add Upscale Image (using Model) node. Then we add Load Upscale Model and connect it. In Load Upscale Model, I highly recommend 4x-AnimeSharp. After going through Upscale Image, the gen will be upscaled by 4x. To make it smaller, we\u2019ll use Upscale Image By node. Connect it to the previous node. I suggest using Lanczos as a method. Then, we create a Math Expression node (comfyui-custom-scripts). We create a Primitive for variable a; it\u2019ll be our Upscale By value. As an expression, we enter a/4 . This way, we downscale this 4x upscaled image to the result we want, relatively to the original gen. Connect Float of Math Expression to Upscale Image By node\u2019s upscale by variable. Now, we create VAE Encode node to turn our image into Latent, and make a KSampler. Using Latent from VAE Encode to connect to KSampler, we connect basic pipe and choose the parameters: I suggest about 20-25 steps, 3.0 - 4.0 CFG, Euler a Normal, 0.35 - 0.45 denoise. With KSampler, out upscaled image will get touched by Stable Diffusion to make it better. We connect KSampler to VAE Decode, make Save Image, and we\u2019re done. This is how it looks: Workflow; Regional Prompting For regional prompting, we're going to need comfyui-impact-pack . First, we need the Sampler node that's going to generate our image; we need to use RegionalSampler node . You can connect Empty Latent Image to it's Samples input. Let's assume we're doing 2 character gens. We create two RegionalPrompt nodes. For each of them, we create Mask Area Rect and KSamplerAdvancedProvider . Connect Mask Area Rect to a corresponding RegionalPrompt . In Mask Area Rect , we choose a part of the image. With 2 character gens, I put w to 50 , h to 100 , blur to 10 for the first node, and the same for the second, but also doing x to 50 . These are percents, so they scale with your image. Create toBasicPipe for each KSamplerAdvancedProvider . Make separate LoadCheckpoint and two CLIP Text Encode for each . Connect corresponding nodes to their own toBasicPipe, then connect basic_pipe with it's corresponding KSamplerAdvancedProvider. Make sure that each KSamplerAdvancedProvider has the same settings. Do not use anything else but dpmpp_2m or euler or euler_ancestral . Now your RegionalPrompt and KSamplerAdvancedProvider should be fully populated. Create CombineRegionalPrompts node, and connect both RegionalPrompt to it. Then, REGIONAL_PROMPTS output to RegionalSampler 's regional_prompts . Now, make a single KSamplerAdvancedProvider , connect a checkpoint and two prompts to it, and connect this KSamplerAdvancedProvider to RegionalSampler 's base_sampler input. Make an output, and the workflow is ready. Now, let's talk parameters and what the hell we even did. First, make sure that each KSamplerAdvancedProvider has the exact same settings. Parameters from txt2img work here, but one important thing: Use no less than 30 steps, and put base_only_steps of RegionalSampler to 1/3 of your total steps; it'll be 10 steps with 30 total steps. Now, about prompts. CLIPs connected to base_sampeler of RegionalSampler are your base prompt . For the duration of base_only_steps , only this prompt is applied to the gen. Your base prompt should have a general outline of the image; for example, 2girls, standing, indoors, cowboy shot, pants, . It's there to make the overall composition, and stop RegionalSampler from messing up the gen. Two pairs of CLIPs connected to your two RegionalPrompt s are your prompts for these two parts of the image. Both should be prompted like it's the only character present; for example first prompt is 1girl, standing, looking at viewer, blue hair, smile, elf, maid, and the second is 1girl, standing, looking at viewer, smile, animal ears, red shirt, red hair, . We don't do composition or angles here, as it's already defined by your base prompt, but you may want to transfer styles and artists to each prompt, just in case. Here's the result: You can get the workflow here , but I highly recommend building your own workflow to understand how exactly everything here works. Workflow; LoRA To use LoRAs, we can\u2019t do it like in ForgeUI; Default nodes don\u2019t understand and such, so we\u2019ll use nodes. Add Load LoRA node. Now: Input Model and CLIP are connected to Load Checkpoint , and output Model and CLIP are connected to KSampler or ToBasicPipe. As for the parameters, Strength_LoRA is the same as editing weights in Forge/AUTO1111, and strength_clip is a different option I don\u2019t know much about, so play with it yourselves. Workflow; Groups, Bypass, Activation. Groups. These are convenient things to group your nodes. Press Create Node, select nodes with held CTRL, RMB -> Add Selected Nodes to Group. Bypass. If you press Bypass in the context menu of a node, or press CTRL + B (CMD + B) on a node, it\u2019ll turn purple. This means that inputs and outputs still work, but the node itself it disabled. It\u2019s useful to disable parts of your workflow, like skipping LoRAs. Activation on. While there are a few things with triggers and such I don\u2019t know how to use properly, it works great with groups. For example, if you have a group with ADetailer, you can turn it off as a whole by pressing on the group\u2019s label -> Set Group Nodes to Never. You can enable it back with Set Group Nodes to Always. Workflow; My Workflow If you want to download my workflow, you can get it here , but I highly suggest making one yourself (go to manager -> install missing custom nodes if you get an error). Figuring everything out yourself is vital for understanding how Comfy and Stable Diffusion work. ComfyUI; Conclusion This is basically it for the base txt2img. I\u2019m still figuring img2img and inpainting, so I won\u2019t be making these sections yet, but it\u2019s extremely similar in principle. I\u2019m not even close to being a pro in Comfy, but I hope it was even a bit useful for beginners.","title":"ComfyUI Guide"},{"location":"comfy_guide/#comfyui","text":"","title":"ComfyUI"},{"location":"comfy_guide/#comfyui-introduction","text":"So, the big boy. ComfyUI is arguably the most versatile and customisable frontend out there, but it\u2019s pretty hard to get into. It\u2019s pretty interesting to me, just because I can try various methods of doing various things, but it\u2019s really hard to just start using; you need to spend hours to get basic functionality. For new users, I highly suggest first picking a simpler frontend like Forge, just to learn basics and have all functions easily available. But for folks who\u2019re interested in what Comfy can do, here\u2019s a detailed section on it. Important. This section is intended for people who have at least some understanding of how image gen works, I won\u2019t be describing every single little detail . First, ComfyUI is a node-based frontend. All functionality is achieved by adding and connecting nodes. On the left of most nodes, there are Inputs , for example image , latent , clip , etc. In the middle, there are different parameters, like denoising , cfg , checkpoint_name , etc. On the right, there are outputs, like the same image , latent , etc. Keep in mind that most parameters can be also turned into inputs, for example you can connect a node called Primitive to cfg , steps , text , etc. Most nodes can take multiple nodes connected to a single input/output, for example, we connect both Load Checkpoint and Load LoRA to the same model input of KSampler. I'll talk about these nodes later.","title":"ComfyUI; Introduction"},{"location":"comfy_guide/#comfyui-installation","text":"Go to Comfy\u2019s Github . Here, go to Getting Started; you can pick either a Desktop Application that will install Comfy as an app (it should be the simplest option), or Windows Portable Package if you want a portable instance. They should be effectively the same, but I personally use the Windows Portable Package. To update Comfy, go to /update/ and start update_comfyui.bat . To provide models from another frontend (Forge, in this case) to Comfy, go to /comfy/ and find a file called extra_model_paths.yaml.example . Rename it to extra_model_paths.yaml and open it in Notepad. Find base_path: line under a111: and replace the path/to/stable-diffusion-webui/ string to your A1111/Forge/reForge path, for example base path: E:/Forge/webui .","title":"ComfyUI; Installation"},{"location":"comfy_guide/#comfyui-custom-nodes","text":"Sooner or later, you will want to use custom nodes. To get them, install ComfyUI Manager by following the instructions on Github. After that, you\u2019ll get a Manager button in your Comfy. Press it \u2014> Custom Nodes Manager, and you\u2019ll be able to download custom nodes. I\u2019ll mention a few of them later, but you\u2019d want to get ComfyUI-custom-scripts as it gives a ton of essentials nodes and it\u2019s a prerequisite for a lot of other custom nodes.","title":"ComfyUI; Custom Nodes"},{"location":"comfy_guide/#comfyui-building-your-workflow","text":"While there are example workflows available, I highly recommend building your own. This is an incredible learning experience, and you\u2019d know exactly what it does and how it works. For now, I\u2019ll be using default nodes exclusively. Let\u2019s begin","title":"ComfyUI; Building Your Workflow"},{"location":"comfy_guide/#ksampler-the-main-node","text":"KSampler is a node that transforms latent noise into your image. To find it, you can either RMB \u2014> Add Node \u2014> Sampling \u2014> KSampler, or search for it in Node Library. Let\u2019s take a look at it and see what inputs it takes, what parameters has and what it outputs.","title":"KSampler; The Main Node"},{"location":"comfy_guide/#ksampler-loading-model","text":"The first input KSampler takes is a model. To load a model, we will use a node called Load Checkpoint. There, you can select which model you want to use and output the model, CLIP and VAE. We\u2019ll figure out CLIP and VAE later, but now, let\u2019s connect MODEL from Load Checkpoint to KSampler.","title":"KSampler; Loading Model"},{"location":"comfy_guide/#ksampler-positive-negative-and-clip-in-general","text":"Positive and Negative are your Positive and Negative prompts. To load them, we will use a node called CLIP Text Encode (Prompt). It inputs CLIP, which we connect to Load Checkpoint, and outputs Conditioning, which we connect to KSampler. You should make two CLIP Text Encodes, one for Positive prompt and second for Negative Prompt. You can recolour and rename nodes, to have them distinct for convenience.","title":"KSampler; Positive, Negative and CLIP in General."},{"location":"comfy_guide/#ksampler-latent-image","text":"Latent Image is the noise that Stable Diffusion resolves into your image. To load it, we will use Empty Latent Image node. You just connect Latent output to Latent input of KSampler. There, make sure to select a correct resolution you want your image to be, 832 x 1216 for example.","title":"KSampler; Latent Image"},{"location":"comfy_guide/#ksampler-parameters","text":"Let\u2019s see what parameters you can configure in KSampler. - Seed. Selects a seed for your gen. - Control after Generate. Determines whether Seed changes or not after a single gen, and how it changes. Fixed to not have it changed, Increment for +1, Decrement for -1, Random for random. - Steps. Regular Steps counter, basically how much times it interacts with the image. - Sampler Name. The sampler you want to use. - Scheduler. What scheduler you want to use. - Denoise. The fraction of image which is turned into noise. Always keep at 1.0 for txt2img, values lower than 1 are needed for img2img.","title":"KSampler; Parameters"},{"location":"comfy_guide/#ksampler-output-and-vae-decode","text":"When KSampler is done, it\u2019ll output your image via Latent output. To turn this Latent into an image, add the VAE Decode node. Connect Latent output of KSampler to VAE Decode, then connect VAE output of Load Checkpoint to VAE Decode, and add Save Image node to connect Image output of VAE Decode to Image input of Save Image. Now, we\u2019re finally done with basic img2img. Configure everything, enter your prompt and press Run at the bottom.","title":"KSampler; Output and VAE Decode."},{"location":"comfy_guide/#ksampler-final-result","text":"This is the bare minimum you need for txt2img gens. From now on, I\u2019ll be less detailed about each node.","title":"KSampler; Final Result"},{"location":"comfy_guide/#workflow-separating-your-prompt-and-quality-presets","text":"First, I\u2019ll specify custom nodes with Node Name (custom-node-pack-title) . To download them, just search the name in ( ) in Custom Nodes Manager. It\u2019s extremely convenient to just enter your prompt in a separate box and have your quality tags applied automatically. To do this, we\u2019ll use String Function node (comfyui-custom-scripts). Add two of them. Then, we\u2019ll use a default node called Primitive. It\u2019s empty at first, but will double the input it\u2019s connected to. Connect Primitive to text_a. Then, you can either connect another Primitive to text_b or use Preset Text (comfyui-custom-scripts), connecting it to text_b. In String Function, choose Append in Action and Yes in tidy_tags. Connect String output to text of Clip Text Encode and voila, it\u2019s done. You use Primitive connected to text_a for your regular prompt, and just change Preset Text or another Primitive connected to text_b.","title":"Workflow; Separating your Prompt and Quality Presets"},{"location":"comfy_guide/#workflow-cleaning-your-workflow-with-pipes","text":"So, there\u2019s a very useful custom nodes pack called comfyui-impact-pack . It\u2019s mostly used for Face Fixing (that I\u2019ll mention later), but it also introduces incredible Pipe nodes. They\u2019re great for an easier way to connect multitude of nodes, especially if you also do Face Fixing and/or Hires. First, we have a ToBasicPipe. It takes Model, CLIP, VAE, Positive, and Negative, and outputs a basic_pipe. Using FromBasicPipe, you can output them. . You can also use KSampler (pipe). This is extremely useful for more complicated workflows.","title":"Workflow; Cleaning your Workflow with Pipes"},{"location":"comfy_guide/#workflow-face-fixing","text":"For Face Fixing, we\u2019ll use comfyui-impact-pack and ComfyUI Impact Subpack . First, let\u2019s create a FaceDetailer (pipe) node. Then, we make a BasicPipe -> DetailerPipe node. We connect Basic_pipe. Then, we add UltralyticsDetectorProvider, select face_yolo8m.pt and connect BBOX_DETECTOR to BasicPipe -> DetailerPipe . Now we create SamLoader (Impact), leave everything at default and connect SAM_MODEL. Any parameters in BasicPipe -> DetailerPipe don\u2019t need to be changed. We connect detailer_pipe to FaceDetailer. Image input of FaceDetailer needs to be connected to VAE Decode. As for the parameters of FaceDetailer, I suggest leaving everything at default, but changing bbox_crop_factor to 2, so that the area of FaceFixing will not be that big. After that, you connect Image output to Save Image, and we\u2019re done.","title":"Workflow; Face Fixing"},{"location":"comfy_guide/#workflow-hires","text":"Let\u2019s go to Hires. In my case, this Hires will be going after ADetailer, so the current gen is Image. First, we add Upscale Image (using Model) node. Then we add Load Upscale Model and connect it. In Load Upscale Model, I highly recommend 4x-AnimeSharp. After going through Upscale Image, the gen will be upscaled by 4x. To make it smaller, we\u2019ll use Upscale Image By node. Connect it to the previous node. I suggest using Lanczos as a method. Then, we create a Math Expression node (comfyui-custom-scripts). We create a Primitive for variable a; it\u2019ll be our Upscale By value. As an expression, we enter a/4 . This way, we downscale this 4x upscaled image to the result we want, relatively to the original gen. Connect Float of Math Expression to Upscale Image By node\u2019s upscale by variable. Now, we create VAE Encode node to turn our image into Latent, and make a KSampler. Using Latent from VAE Encode to connect to KSampler, we connect basic pipe and choose the parameters: I suggest about 20-25 steps, 3.0 - 4.0 CFG, Euler a Normal, 0.35 - 0.45 denoise. With KSampler, out upscaled image will get touched by Stable Diffusion to make it better. We connect KSampler to VAE Decode, make Save Image, and we\u2019re done. This is how it looks:","title":"Workflow; Hires"},{"location":"comfy_guide/#workflow-regional-prompting","text":"For regional prompting, we're going to need comfyui-impact-pack . First, we need the Sampler node that's going to generate our image; we need to use RegionalSampler node . You can connect Empty Latent Image to it's Samples input. Let's assume we're doing 2 character gens. We create two RegionalPrompt nodes. For each of them, we create Mask Area Rect and KSamplerAdvancedProvider . Connect Mask Area Rect to a corresponding RegionalPrompt . In Mask Area Rect , we choose a part of the image. With 2 character gens, I put w to 50 , h to 100 , blur to 10 for the first node, and the same for the second, but also doing x to 50 . These are percents, so they scale with your image. Create toBasicPipe for each KSamplerAdvancedProvider . Make separate LoadCheckpoint and two CLIP Text Encode for each . Connect corresponding nodes to their own toBasicPipe, then connect basic_pipe with it's corresponding KSamplerAdvancedProvider. Make sure that each KSamplerAdvancedProvider has the same settings. Do not use anything else but dpmpp_2m or euler or euler_ancestral . Now your RegionalPrompt and KSamplerAdvancedProvider should be fully populated. Create CombineRegionalPrompts node, and connect both RegionalPrompt to it. Then, REGIONAL_PROMPTS output to RegionalSampler 's regional_prompts . Now, make a single KSamplerAdvancedProvider , connect a checkpoint and two prompts to it, and connect this KSamplerAdvancedProvider to RegionalSampler 's base_sampler input. Make an output, and the workflow is ready. Now, let's talk parameters and what the hell we even did. First, make sure that each KSamplerAdvancedProvider has the exact same settings. Parameters from txt2img work here, but one important thing: Use no less than 30 steps, and put base_only_steps of RegionalSampler to 1/3 of your total steps; it'll be 10 steps with 30 total steps. Now, about prompts. CLIPs connected to base_sampeler of RegionalSampler are your base prompt . For the duration of base_only_steps , only this prompt is applied to the gen. Your base prompt should have a general outline of the image; for example, 2girls, standing, indoors, cowboy shot, pants, . It's there to make the overall composition, and stop RegionalSampler from messing up the gen. Two pairs of CLIPs connected to your two RegionalPrompt s are your prompts for these two parts of the image. Both should be prompted like it's the only character present; for example first prompt is 1girl, standing, looking at viewer, blue hair, smile, elf, maid, and the second is 1girl, standing, looking at viewer, smile, animal ears, red shirt, red hair, . We don't do composition or angles here, as it's already defined by your base prompt, but you may want to transfer styles and artists to each prompt, just in case. Here's the result: You can get the workflow here , but I highly recommend building your own workflow to understand how exactly everything here works.","title":"Workflow; Regional Prompting"},{"location":"comfy_guide/#workflow-lora","text":"To use LoRAs, we can\u2019t do it like in ForgeUI; Default nodes don\u2019t understand and such, so we\u2019ll use nodes. Add Load LoRA node. Now: Input Model and CLIP are connected to Load Checkpoint , and output Model and CLIP are connected to KSampler or ToBasicPipe. As for the parameters, Strength_LoRA is the same as editing weights in Forge/AUTO1111, and strength_clip is a different option I don\u2019t know much about, so play with it yourselves.","title":"Workflow; LoRA"},{"location":"comfy_guide/#workflow-groups-bypass-activation","text":"Groups. These are convenient things to group your nodes. Press Create Node, select nodes with held CTRL, RMB -> Add Selected Nodes to Group. Bypass. If you press Bypass in the context menu of a node, or press CTRL + B (CMD + B) on a node, it\u2019ll turn purple. This means that inputs and outputs still work, but the node itself it disabled. It\u2019s useful to disable parts of your workflow, like skipping LoRAs. Activation on. While there are a few things with triggers and such I don\u2019t know how to use properly, it works great with groups. For example, if you have a group with ADetailer, you can turn it off as a whole by pressing on the group\u2019s label -> Set Group Nodes to Never. You can enable it back with Set Group Nodes to Always.","title":"Workflow; Groups, Bypass, Activation."},{"location":"comfy_guide/#workflow-my-workflow","text":"If you want to download my workflow, you can get it here , but I highly suggest making one yourself (go to manager -> install missing custom nodes if you get an error). Figuring everything out yourself is vital for understanding how Comfy and Stable Diffusion work.","title":"Workflow; My Workflow"},{"location":"comfy_guide/#comfyui-conclusion","text":"This is basically it for the base txt2img. I\u2019m still figuring img2img and inpainting, so I won\u2019t be making these sections yet, but it\u2019s extremely similar in principle. I\u2019m not even close to being a pro in Comfy, but I hope it was even a bit useful for beginners.","title":"ComfyUI; Conclusion"},{"location":"conclusion/","text":"Conclusion I think that's all, besides img2img and ControlNet, but I have literally 0 strength left to investigate and document them as well. Maybe some other time. That's basically months of my experience with Stable Diffusion and Illustrious, dozens of hours spent clicking letters on the keyboard, smashing \"Generate\", sucking ass, gens failing, remaking prompts, finally coming to understand shit, experiment with basically everything there is, testing models, prompts, interactions between tags, shit like that. Besides my dramatic mood right now, of course I didn't talk about everything; it's impossible for one man to do, and I'm not aiming to. I really hope that this guide was at least a little bit useful, and that you learned something new. I wish you luck and all the best; be creative, make stuff, think for yourself, be happy and take care.","title":"Conclusion"},{"location":"conclusion/#conclusion","text":"I think that's all, besides img2img and ControlNet, but I have literally 0 strength left to investigate and document them as well. Maybe some other time. That's basically months of my experience with Stable Diffusion and Illustrious, dozens of hours spent clicking letters on the keyboard, smashing \"Generate\", sucking ass, gens failing, remaking prompts, finally coming to understand shit, experiment with basically everything there is, testing models, prompts, interactions between tags, shit like that. Besides my dramatic mood right now, of course I didn't talk about everything; it's impossible for one man to do, and I'm not aiming to. I really hope that this guide was at least a little bit useful, and that you learned something new. I wish you luck and all the best; be creative, make stuff, think for yourself, be happy and take care.","title":"Conclusion"},{"location":"forge_guide/","text":"Installing Stable Diffusion Locally: Of course, before generating anything, we have to have something to generate it with. While there are numerous sites and services that do it on their servers, they leave you with much less customisation, convenience, sometimes speed, and, of course, they are paid. I'm no expert on these services, I only ever used Civitai just to try Early Access checkpoints, there are a few of them that people on Aegiscord keep mentioning and using, here are two I remember: Pixai.art and Tensor.art There are also Novel.ai , Midjourney and others that are good, but they're not Stable Diffusion and I'm not knowledgable in them at all. These services have little to no customisation or setup required, so there's no point in talking about them specifically. The best option available is, of course, local Stable Diffusion. There is an option to run it from something like Google Colab, but these Colabs come and go as Google bans them, so I can't provide any links. There should be numerous guides about them anyway, just Google it. The purpose of this section of the guide is to explain how to get Stable Diffusion running on your hardware, so let's get to choosing an app for that. System Requirements First, let's talk about hardware you need to run SD locally. For PC users, you have to own an NVIDIA GPU with at least 8 GB of VRAM (for AMD GPU users, see SD.Next below). SDXL models use 6 gigabytes of VRAM themselves, and including fluctuations from the ongoing generation, moving models around, and external VRAM usage from Windows or, for example, your browser, total VRAM usage may easily reach somewhere from 7.5 GB to 8 GB max on my RTX 3070 8 GB. It can be possible to run SDXL with 6 GB of VRAM, a lot of frontends have optimisations for that, but be ready for slow generation speed, increased model moving times and CUDA Out of Memory errors. It is technically possible to run SD on just RAM, but it will be so extremely slow it's not worth it at all (but later in the guide I will talk about one method that can make it just barely viable). For two Mac users out there, SD runs on any Apple Silicon chip; the main issue will be the speed of Unified Memory. While it's usually much faster than conventional RAM, it's still far from VRAM speeds, so while it's more than possible and pretty simple to install and run, do not expect high generation speed. Choosing a Frontend Let's talk about frontends: they are essentially environments for Stable Diffusion. They offer various performance, compatibility, quality and customisation, which are extremely important. Let's talk about some of the most popular of them: (first, some explanations. IT/s, or Iterations per Second is a metric that shows the amount of generation steps completed in a second. It depends on the Resolution of the image, so all numbers I'll show are from a 832x1216 generation, which is equal to 1024x1024.) AUTO1111 . The oldest and most known frontend, the good old stable-diffusion-webui. I myself used it as my first frontend years ago during the great days of Stable Diffusion 1.0. Sadly, it is extremely outdated in terms of performance: it offers literally 10x slower speeds than some other options I'll mention. I mention it here mostly for people that remember how good it was, because it's no longer the best option available; only use it if you know exactly why you need it . As a somewhat subjective metric for performance, AUTO1111 gives me just 0.15-0.20 IT/s, which is about 5-6 s/IT. There are forks of it available, for example, ForgeUI . A huge fork for AUTO1111 that offers big improvements in terms of models it supports and, most importantly, the performance. It retains the incredible interface of AUTO1111 while providing more in-built features. From my testing, it's the second best performing frontend, giving me somewhere from 2 IT/s to 2.3 IT/s. Note that it has a slimmer extensions support compared to A1111. UPD: finally gave this frontend a proper shot after the death of reForge; it has the exact same performance as reForge and either the same or slightly higher quality of generations. It supports all extensions I use (them being ADetailer, tag-autocomplete and Infinite Image Browsing), so it is a solid choice. Besides that, I don't see any noticeable differences between Forge and reForge, so I'm pretty sure I don't have to rewrite anything in this guide (besides the UI section, but I'm lazy). For newbies and people experienced with A1111/reForge, I highly recommend ForgeUI. This guide will be about it specifically going forward, but 95% of stuff is applicable to anything Stable Diffusion. reForgeUI . It's a fork of ForgeUI. It's extremely similar to ForgeUI, but has some advantages over it. First, it fully supports most of AUTO1111's extensions. Second, it shows much less quality loss that I'd estimate at 0% to 5% compared to AUTO1111 and ComfyUI while retaining most of the performance; to me, it's somewhere between 1.9 IT/s and 2.1 IT/s. Third, it's updated constantly, ensuring the most stability and support for new models. ~~This is a frontend I personally use as my main, and I highly recommend it to you. I'll be talking specifically about it throughout the guide.~~ The development of reForge has ceased, so I cannot recommend it anymore. Important Update : On 13th of April 2025, reForge ceased development. This means that it will no longer receive any updates, so new foundational models may not work on it. The guide was updated to focus on Forge. ComfyUI . Arguably the best, most powerful, most customisable, supported and versatile frontend available. While it lets you do absolutely everything with your generations and receives support for new image and video generation models first, it's extremely complex, difficult and convoluted. Essentially, you're building every single function that you need yourself, from basic txt2img and img2img to Inpainting, Hires and others. There are community made workflows available, so you don't have to figure out and build them yourself, but you'd still have to understand them, switch around between workflows to get different functionality and figure out how to even use them. For all of that, it offers the best quality and performance out of all frontends, giving me about 2 IT/s at full, beautiful quality. If you're willing to spend your time understanding everything Comfy offers, tinkering with nodes, building features and such, it is the best option available, but for a regular user, it might just scare you out of doing anything image-gen or local related ever again. Just know that it exists, and if you feel ready, give it a shot. UPD: Now this Rentry also features sections on ComfyUI's installation and a basic usage guide. See below. SD.Next . More of a honorable mention, this frontend is the only one I know that works with AMD GPUs on Windows and supports AMD ROCm. Stuff like ComfyUI works with AMD GPUs too, but it requires Linux. Installing Stable Diffusion ForgeUI and Why So, reForge is no longer in development, meaning that there will be a time when you'd have to switch to something else. For newbies and people familiar with A1111/reForge, I highly suggest using ForgeUI, as it's basically the same. I used it quite a bit for the past week, and, in my experience, it has the same performance and same/slightly higher image quality. To install, go to the Github page and press the One-Click Install button. Unpack the archive, press update.bat , then run.bat ; that should be it. You can start SD by clicking run.bat . Keep in mind that you will need ~5 GB of free space for the Forge itself, 6 GB for every SDXL model and ~500 MB for each LoRA you download. Stable Diffusion XL Models Let's talk about the most important thing: models we're going to use. First, you should get your models from civitai.com , it has the biggest library of models to download. To search for them, go into \"Models\", then \"Filters\" and check \"Checkpoints\", then \"Illustrious\" below. Most of these models (especially more popular ones) will be Illustrious 0.1, but there are some new models based on Illustrious 1.0 and 1.1 that are slightly different. To install, just put a .safetensors model file into models/stable-diffusion . Here are some models I recommend: OUTDATED Amanatsu 1.1 . It's based on Hassaku and made by the same people, and it's incredible. It has a style I adore and it shows some of the best prompt adherence I've seen. Great character knowledge and detail + quite good backgrounds. Works well with Artists, LoRAs and styles. OUTDATED Hassaku XL . The model Amanatsu is based on. Has a more neutral style while keeping the greats of Amanatsu. A solid model. I suggest using version 2.1, but there can be arguments made that v2.1fix is better, or that v1.2 is better, and, well, they are in some aspects, but I consider them to be just slightly different from each other. OUTDATED WAI NSFW Illustrious XL . A model of the WAI family, great and versatile. I consider it to be about the same as Hassaku quality-wise, just a different style. I suggest using v12, I didn't like v13 that much. Nova Orange . A model I found somewhat recently. While it suffers from a not-so-great prompt adherence, it has maybe the best quality I've seen, in both detail and backgrounds. UPD: at version 9.0, it's one of the best models available, showing a pretty neutral style, great backgrounds, good prompt adherence and general detail. SLIGHTLY OUTDATED AnnnslMixIllustrious . The only good Illustrious 1.0/1.1 based model I know at the moment. It's very versatile, has advantages of Illustrious 1.0/1.1 like an option to use higher resolution and some amount of Natural Language prompting, and adheres to your prompts well. It's default style is not exactly to my liking, but all my older Illustrious 0.1-based LoRAs work with it well. Nova Anime . Found it somewhat recently, but now it's my main model. While it's pretty picky on your generation parameters, the quality, default style, prompt adherence and versatility are almost unmatched. Make sure to use a sampler that has SDE or ancestral / a in the name. The list is not exhaustive. If you want to see other models and what they have to offer, I did quite a few model comparisons on Aegiscord. File sizes of them are enormous, I can't just link them with Catbox, so you can just go to the #image-spam channel of Aegiscord to find my thread called Bex's SD Hub . Link I usually post all my findings and research there first. What to do once you got it installed Command Line Flags This was an important section for reForge, but it's extremely simple with Forge; you basically don't have to use it. One notable thing is: to get your Forge instance accessible from your local network, go to webui folder, and find webui-user.bat. . Open it in Notepad, find COMMANDLINE_ARGS , and add --listen . After that, Forge will be accessible on DESKTOP_LOCAL_IP:7860 . In my case, for example, it's 192.168.2.207:7860 . On the machine itself, you can always go to 127.0.0.1:7860 . User Interface; Basic Setup Important Note: This section is mostly the same as it was for reForge, but: I'll ~~cross out~~ sections that are not applicable to ForgeUI, and make bold sections new to ForgeUI. Screenshots are not updated yet. I really should overhaul this whole section, but not yet. Let's start from top to bottom, left to right. First, the top line of buttons: * UI: here, you select the architecture of a model you're gonna use. For Illustrious/NoobAI/Pony, it's always sdxl . Stable Diffusion Checkpoint: your current model + you can choose to load another model. Use the Refresh button if you add new models to models/stable-diffusion while SD is running. SD VAE: to choose which VAE you want to run. ~~In 99% of cases you want to leave it at Automatic and never touch again.~~ Leave empty in ForgeUI for Automatic . Diffusion in Low Bits: Right now, I really don't know what it is. I assume it's some optimisation thing. I just leave it at auto. ~~CLIP Skip: In my understanding, it's the amount of deepest CLIP layers SD skips during Prompt Processing. With SDXL, you almost always want to keep it at 2 and never change again.~~ txt2img: a tab for creating images from text. We'll be focusing on it throughout the guide. img2img: a tab that uses a reference picture for generation in addition to text. ~~Will explain separately later.~~ Maybe will add a section about it some other time. Spaces: some ForgeUI feature to install and run tools that are separate from Forge. I won't be talking about them. Extras: a tab where extra functions live. Most of the time there's nothing to do there. PNG Info: a tab where you can get metadata of your and other's generations and immediately move it to txt2img and other tabs. Checkpoint Merger: used for merging different models together. I won't be talking about it. ~~Train: A very scary section with questionable usefulness. I won't be talking about it at all.~~ Settings: Self-explanatory Extensions: a place where you can turn on/off and download extensions. Now, second row of top buttons specific to txt2img: Generation: our main tab for txt2img generation. Textural Inversion: a tab where all your downloaded embeddings live. I don't plan on touching them here as I don't use them, so, a few words about them: from my understanding, you use them to apply Quality tags and such to a generation without actually typing them in as tags. While the idea is cool, I prefer having full control over my generation, and these embeddings do God knows what. If they work for you - great, I prefer not using them at all. ~~Hypernetworks: a very scary tab I have 0 idea about. I think it's to train models or something? I won't be talking about it at all here.~~ Checkpoints: a tab that shows all of your models. You can assign images to them by clicking the Edit Metadata button --> Replace Preview. It'll be replaced with the current image selected in txt2img. To delete the image, go to models/stable-diffusion and remove a checkpoint_name.png . LoRAs: a tab that shows all your LoRAs. You can assign images to them the same way as with Checkpoints. User Interface; txt2img Finally, let's talk about txt2img buttons and how to setup them to get to genning quickly. I'll explain most of them later separately and in more detail, this is just to get started. Prompt: A place where you input your Positive Prompt. At the top right you can see a 0/75 indicator: this is the amount of tokens in your prompt out of CLIP's capacity. I'll talk about CLIPs in more detail later, but for most cases, you should aim to not exceed 75 tokens, unless you know exactly what you're doing and how. This is not a hard rule, and if you go above 75, another CLIP will be created, but your prompt has the most efficiency when you're keeping it within the single CLIP. There are a lot of exceptions and cases when you'd want to have multiple CLIPs, but I'll talk about it later. Negative Prompt: The field where you input your Negative Prompt. You should only have your negative Quality tags there for the most cases. A rule of thumb is, unless you have a specific reason to add them, don't. Negative prompting is very powerful, and being careless can degrade your gens a lot. For example, if you have monochrome images popping of frequently, you may add monochrome to the negatives; they should have a reason to be there. I'll touch on them in more detail later. The Arrow Button: inputs your previous generation parameters into the UI. It also works on SD restart. It's great if you refreshed the page. The Trash Can Button: Clears both Prompt fields. Can be buggy, I don't recommend using it. Notebook Button: Moves prompt from enabled Styles to Prompt fields and disables the Style. Blue Spiral Button (usually hidden): It appears if you refresh the page in the middle of the ongoing generation. Allows you to get the prompt, previous settings and images from this gen. Styles Field and the Pen Button: It's an option to have a set combination of tags be added to your prompt. You can set them up by pressing the Pen button, you can see an example below. By default, the tags are added like this: 1girl, standing, ...your prompt..., <style> . You can make Style prompt be added to any position of the prompt, by using {prompt} , for example, {prompt}, masterpiece, good quality, amazing quality, . Sampling Method: This is where you choose your Sampler. While I won't go into what they do exactly, I can recommend using DPM++ 2M , and, if for some reason it doesn't work on some specific model, Euler a . I'll touch on some other Samplers later. Scheduler Type: It's a secondary setting for Samplers. With DPM++ 2M you should use Karras , and with Euler a you should use Normal . There are other useful Schedulers I'll mention much later. Sampling Steps: how many generation steps are done for a single image. I recommend having it from 26 to 34 . I use 28 , 30 is also very good. There are methods to drastically reduce the amount of steps needed (thus decreasing the generation time) with Hyper LoRAs, LCM or AYS, see further ahead. Width and Height: options to set the resolution of your gen. SDXL is trained on 1024x1024 images and finetuned on a number of other resolutions. Here is a list of good SDXL resolutions. I personally use either 832x1216 or 1216x832 for either Landscape or Portrait, as it's very close to a regular 2:3 aspect ratio. Note that there is no way to natively generate 16:9 images, do not use resolutions like 1366x768 and such, they will result in artifacts. Refer to the link above to see what aspect ratios are supported . Batch Count: how many images are generated separately. I recommend using it exclusively unless you have a GPU with enough VRAM to support simultaneous generation of multiple images. Batch Size: how many images are generated at the same time. Extremely VRAM intensive and not much faster than generating images one by one, I do not recommend using it. Leave it at 1. CFG Scale: A very tricky setting. Basically, the higher CFG is, the more model will adhere to your prompt. With Illustrious models, you get cohere generations in the range from CFG 3 to 7. I recommend using 5 and keeping in mind the option to move it to 4 or 3.5 the more complex your prompt gets. Seed: a number that defines your generation. By reusing the exact same prompt and exact same seed from another generation, you will get the (almost) exact same image. -1 for Random (or by pressing the Dice button). Recycle button allows you to automatically enter the seed from previous generation. It's very useful if you want to test LoRAs or models, or Hires/Stylize an already genned image. Other menus below: These are all the different extensions you have installed. I will talk about some them separately. Gallery buttons: You have a set of useful different buttons under the image. They are explained if you hover over them, so I won't be explaining them. Just know that they're there, and that they're useful. Metadata: Under gallery buttons there is text. This text is the metadata from the current image; it contains your prompt, generation parameters, extension parameters, etc. This text is useful if you or someone else wants to replicate the exact image you got. UI Settings While there is a ton of settings, I want to tell you about some of the most important. I'll be referring to the by their name in the list on the left: - Defaults: Allows you to save all your current parameters and settings as a default, so that SD will be using them on launch. Press View Changes to see what exactly it'll save, and Apply to save them. Very useful to not have to setup basic things like CFG, Samplers and Resolution each time you launch SD. - Live Previews: This section is responsible for Live Previews of images that are still generating. While it is extremely useful, it takes a bit of performance away, so if that's an issue for you, turn it off. You can also set the period of Live Previews renders; I use 7, for example, because I always use 28 Generation steps, so I get a preview every quarter of the generation. You can also change the Preview Method; I use TAESD , as it's almost doesn't have quality degradation from it being a preview and I didn't notice much of a performance loss. - Infotext: allows you to disable writing generation metadata into the image. While I'm all for image gen being an open-source, completely collaborative hobby where we all learn from each other to improve, people should have an option to hide their generation parameters and prompts from others. Feel free to do so. - Paths for Saving: allows you to change the folder to where SD saves your generations. These are most notable settings. Feel free to explore them yourself, I could've easily missed something important. Extensions While base Forge is more than enough for most stuff, there are some very useful extensions you can download. First, how to install them. Go to the Extensions tab, then press the Available tab under it and press the big blue Load From button. It will give you a list of all the available extensions. Once you installed it, go to Installed tab and press Apply and Restart UI. Note that if you're using --listen , Forge will not allow you to install, enable and disable extensions. Remove the arg, do what you need with the extensions and put it back. Now, some cool extensions: Booru tag autocompletion prompting : it functions as an autocomplete for Booru tags, containing, I think, literally all of them. Very convenient and useful even if you already memorised them. You can press Tab to put in the top result and arrows to move between results. On pressing Tab, it also auto-formats (brackets) into \\(sd-recongisable brackets\\) and auto-removes _underscores_ . Very cool. ADetailer : an essential extension for almost everyone. What it does is: once the image finishes generation, it automatically detects any faces and re-generates them in full resolution, resulting in crisp and detailed faces in any situation. It doesn't require any setup; you just install it, enable and voila, now character faces look beautiful. Regional Prompter : I never figured it out, but a few people on Aegiscord mention it from time to time and show impressive results with it, so I'll mention this extension too. What it does is: it separates the image area in X areas and generates characters separately for each one. It's extremely useful for multiple character gens, but is completely unintuitive and I didn't have enough patience to learn it; Illustrious is capable of generating 2 or 3 characters by itself with some tricky prompting I'll talk about in detail later. If I ever learn this extension, maybe I'll expand this section or make a new one. Infinite Image Browser : An incredible tool to view, sort, tag, organize and compare all your generations. It indexes and views everything in your outputs (or any custom) folder. You can see a showcase of this extension on it's github page . I highly recommend giving it a shot; it's one of my essential extensions now. We're ready? I think that's all the technical stuff need to know to get to image generation with Forge. Let's get to the deeds.","title":"Installing Stable Diffusion; ForgeUI"},{"location":"forge_guide/#installing-stable-diffusion-locally","text":"Of course, before generating anything, we have to have something to generate it with. While there are numerous sites and services that do it on their servers, they leave you with much less customisation, convenience, sometimes speed, and, of course, they are paid. I'm no expert on these services, I only ever used Civitai just to try Early Access checkpoints, there are a few of them that people on Aegiscord keep mentioning and using, here are two I remember: Pixai.art and Tensor.art There are also Novel.ai , Midjourney and others that are good, but they're not Stable Diffusion and I'm not knowledgable in them at all. These services have little to no customisation or setup required, so there's no point in talking about them specifically. The best option available is, of course, local Stable Diffusion. There is an option to run it from something like Google Colab, but these Colabs come and go as Google bans them, so I can't provide any links. There should be numerous guides about them anyway, just Google it. The purpose of this section of the guide is to explain how to get Stable Diffusion running on your hardware, so let's get to choosing an app for that.","title":"Installing Stable Diffusion Locally:"},{"location":"forge_guide/#system-requirements","text":"First, let's talk about hardware you need to run SD locally. For PC users, you have to own an NVIDIA GPU with at least 8 GB of VRAM (for AMD GPU users, see SD.Next below). SDXL models use 6 gigabytes of VRAM themselves, and including fluctuations from the ongoing generation, moving models around, and external VRAM usage from Windows or, for example, your browser, total VRAM usage may easily reach somewhere from 7.5 GB to 8 GB max on my RTX 3070 8 GB. It can be possible to run SDXL with 6 GB of VRAM, a lot of frontends have optimisations for that, but be ready for slow generation speed, increased model moving times and CUDA Out of Memory errors. It is technically possible to run SD on just RAM, but it will be so extremely slow it's not worth it at all (but later in the guide I will talk about one method that can make it just barely viable). For two Mac users out there, SD runs on any Apple Silicon chip; the main issue will be the speed of Unified Memory. While it's usually much faster than conventional RAM, it's still far from VRAM speeds, so while it's more than possible and pretty simple to install and run, do not expect high generation speed.","title":"System Requirements"},{"location":"forge_guide/#choosing-a-frontend","text":"Let's talk about frontends: they are essentially environments for Stable Diffusion. They offer various performance, compatibility, quality and customisation, which are extremely important. Let's talk about some of the most popular of them: (first, some explanations. IT/s, or Iterations per Second is a metric that shows the amount of generation steps completed in a second. It depends on the Resolution of the image, so all numbers I'll show are from a 832x1216 generation, which is equal to 1024x1024.) AUTO1111 . The oldest and most known frontend, the good old stable-diffusion-webui. I myself used it as my first frontend years ago during the great days of Stable Diffusion 1.0. Sadly, it is extremely outdated in terms of performance: it offers literally 10x slower speeds than some other options I'll mention. I mention it here mostly for people that remember how good it was, because it's no longer the best option available; only use it if you know exactly why you need it . As a somewhat subjective metric for performance, AUTO1111 gives me just 0.15-0.20 IT/s, which is about 5-6 s/IT. There are forks of it available, for example, ForgeUI . A huge fork for AUTO1111 that offers big improvements in terms of models it supports and, most importantly, the performance. It retains the incredible interface of AUTO1111 while providing more in-built features. From my testing, it's the second best performing frontend, giving me somewhere from 2 IT/s to 2.3 IT/s. Note that it has a slimmer extensions support compared to A1111. UPD: finally gave this frontend a proper shot after the death of reForge; it has the exact same performance as reForge and either the same or slightly higher quality of generations. It supports all extensions I use (them being ADetailer, tag-autocomplete and Infinite Image Browsing), so it is a solid choice. Besides that, I don't see any noticeable differences between Forge and reForge, so I'm pretty sure I don't have to rewrite anything in this guide (besides the UI section, but I'm lazy). For newbies and people experienced with A1111/reForge, I highly recommend ForgeUI. This guide will be about it specifically going forward, but 95% of stuff is applicable to anything Stable Diffusion. reForgeUI . It's a fork of ForgeUI. It's extremely similar to ForgeUI, but has some advantages over it. First, it fully supports most of AUTO1111's extensions. Second, it shows much less quality loss that I'd estimate at 0% to 5% compared to AUTO1111 and ComfyUI while retaining most of the performance; to me, it's somewhere between 1.9 IT/s and 2.1 IT/s. Third, it's updated constantly, ensuring the most stability and support for new models. ~~This is a frontend I personally use as my main, and I highly recommend it to you. I'll be talking specifically about it throughout the guide.~~ The development of reForge has ceased, so I cannot recommend it anymore. Important Update : On 13th of April 2025, reForge ceased development. This means that it will no longer receive any updates, so new foundational models may not work on it. The guide was updated to focus on Forge. ComfyUI . Arguably the best, most powerful, most customisable, supported and versatile frontend available. While it lets you do absolutely everything with your generations and receives support for new image and video generation models first, it's extremely complex, difficult and convoluted. Essentially, you're building every single function that you need yourself, from basic txt2img and img2img to Inpainting, Hires and others. There are community made workflows available, so you don't have to figure out and build them yourself, but you'd still have to understand them, switch around between workflows to get different functionality and figure out how to even use them. For all of that, it offers the best quality and performance out of all frontends, giving me about 2 IT/s at full, beautiful quality. If you're willing to spend your time understanding everything Comfy offers, tinkering with nodes, building features and such, it is the best option available, but for a regular user, it might just scare you out of doing anything image-gen or local related ever again. Just know that it exists, and if you feel ready, give it a shot. UPD: Now this Rentry also features sections on ComfyUI's installation and a basic usage guide. See below. SD.Next . More of a honorable mention, this frontend is the only one I know that works with AMD GPUs on Windows and supports AMD ROCm. Stuff like ComfyUI works with AMD GPUs too, but it requires Linux.","title":"Choosing a Frontend"},{"location":"forge_guide/#installing-stable-diffusion-forgeui-and-why","text":"So, reForge is no longer in development, meaning that there will be a time when you'd have to switch to something else. For newbies and people familiar with A1111/reForge, I highly suggest using ForgeUI, as it's basically the same. I used it quite a bit for the past week, and, in my experience, it has the same performance and same/slightly higher image quality. To install, go to the Github page and press the One-Click Install button. Unpack the archive, press update.bat , then run.bat ; that should be it. You can start SD by clicking run.bat . Keep in mind that you will need ~5 GB of free space for the Forge itself, 6 GB for every SDXL model and ~500 MB for each LoRA you download.","title":"Installing Stable Diffusion ForgeUI and Why"},{"location":"forge_guide/#stable-diffusion-xl-models","text":"Let's talk about the most important thing: models we're going to use. First, you should get your models from civitai.com , it has the biggest library of models to download. To search for them, go into \"Models\", then \"Filters\" and check \"Checkpoints\", then \"Illustrious\" below. Most of these models (especially more popular ones) will be Illustrious 0.1, but there are some new models based on Illustrious 1.0 and 1.1 that are slightly different. To install, just put a .safetensors model file into models/stable-diffusion . Here are some models I recommend: OUTDATED Amanatsu 1.1 . It's based on Hassaku and made by the same people, and it's incredible. It has a style I adore and it shows some of the best prompt adherence I've seen. Great character knowledge and detail + quite good backgrounds. Works well with Artists, LoRAs and styles. OUTDATED Hassaku XL . The model Amanatsu is based on. Has a more neutral style while keeping the greats of Amanatsu. A solid model. I suggest using version 2.1, but there can be arguments made that v2.1fix is better, or that v1.2 is better, and, well, they are in some aspects, but I consider them to be just slightly different from each other. OUTDATED WAI NSFW Illustrious XL . A model of the WAI family, great and versatile. I consider it to be about the same as Hassaku quality-wise, just a different style. I suggest using v12, I didn't like v13 that much. Nova Orange . A model I found somewhat recently. While it suffers from a not-so-great prompt adherence, it has maybe the best quality I've seen, in both detail and backgrounds. UPD: at version 9.0, it's one of the best models available, showing a pretty neutral style, great backgrounds, good prompt adherence and general detail. SLIGHTLY OUTDATED AnnnslMixIllustrious . The only good Illustrious 1.0/1.1 based model I know at the moment. It's very versatile, has advantages of Illustrious 1.0/1.1 like an option to use higher resolution and some amount of Natural Language prompting, and adheres to your prompts well. It's default style is not exactly to my liking, but all my older Illustrious 0.1-based LoRAs work with it well. Nova Anime . Found it somewhat recently, but now it's my main model. While it's pretty picky on your generation parameters, the quality, default style, prompt adherence and versatility are almost unmatched. Make sure to use a sampler that has SDE or ancestral / a in the name. The list is not exhaustive. If you want to see other models and what they have to offer, I did quite a few model comparisons on Aegiscord. File sizes of them are enormous, I can't just link them with Catbox, so you can just go to the #image-spam channel of Aegiscord to find my thread called Bex's SD Hub . Link I usually post all my findings and research there first.","title":"Stable Diffusion XL Models"},{"location":"forge_guide/#what-to-do-once-you-got-it-installed","text":"","title":"What to do once you got it installed"},{"location":"forge_guide/#command-line-flags","text":"This was an important section for reForge, but it's extremely simple with Forge; you basically don't have to use it. One notable thing is: to get your Forge instance accessible from your local network, go to webui folder, and find webui-user.bat. . Open it in Notepad, find COMMANDLINE_ARGS , and add --listen . After that, Forge will be accessible on DESKTOP_LOCAL_IP:7860 . In my case, for example, it's 192.168.2.207:7860 . On the machine itself, you can always go to 127.0.0.1:7860 .","title":"Command Line Flags"},{"location":"forge_guide/#user-interface-basic-setup","text":"Important Note: This section is mostly the same as it was for reForge, but: I'll ~~cross out~~ sections that are not applicable to ForgeUI, and make bold sections new to ForgeUI. Screenshots are not updated yet. I really should overhaul this whole section, but not yet. Let's start from top to bottom, left to right. First, the top line of buttons: * UI: here, you select the architecture of a model you're gonna use. For Illustrious/NoobAI/Pony, it's always sdxl . Stable Diffusion Checkpoint: your current model + you can choose to load another model. Use the Refresh button if you add new models to models/stable-diffusion while SD is running. SD VAE: to choose which VAE you want to run. ~~In 99% of cases you want to leave it at Automatic and never touch again.~~ Leave empty in ForgeUI for Automatic . Diffusion in Low Bits: Right now, I really don't know what it is. I assume it's some optimisation thing. I just leave it at auto. ~~CLIP Skip: In my understanding, it's the amount of deepest CLIP layers SD skips during Prompt Processing. With SDXL, you almost always want to keep it at 2 and never change again.~~ txt2img: a tab for creating images from text. We'll be focusing on it throughout the guide. img2img: a tab that uses a reference picture for generation in addition to text. ~~Will explain separately later.~~ Maybe will add a section about it some other time. Spaces: some ForgeUI feature to install and run tools that are separate from Forge. I won't be talking about them. Extras: a tab where extra functions live. Most of the time there's nothing to do there. PNG Info: a tab where you can get metadata of your and other's generations and immediately move it to txt2img and other tabs. Checkpoint Merger: used for merging different models together. I won't be talking about it. ~~Train: A very scary section with questionable usefulness. I won't be talking about it at all.~~ Settings: Self-explanatory Extensions: a place where you can turn on/off and download extensions. Now, second row of top buttons specific to txt2img: Generation: our main tab for txt2img generation. Textural Inversion: a tab where all your downloaded embeddings live. I don't plan on touching them here as I don't use them, so, a few words about them: from my understanding, you use them to apply Quality tags and such to a generation without actually typing them in as tags. While the idea is cool, I prefer having full control over my generation, and these embeddings do God knows what. If they work for you - great, I prefer not using them at all. ~~Hypernetworks: a very scary tab I have 0 idea about. I think it's to train models or something? I won't be talking about it at all here.~~ Checkpoints: a tab that shows all of your models. You can assign images to them by clicking the Edit Metadata button --> Replace Preview. It'll be replaced with the current image selected in txt2img. To delete the image, go to models/stable-diffusion and remove a checkpoint_name.png . LoRAs: a tab that shows all your LoRAs. You can assign images to them the same way as with Checkpoints.","title":"User Interface; Basic Setup"},{"location":"forge_guide/#user-interface-txt2img","text":"Finally, let's talk about txt2img buttons and how to setup them to get to genning quickly. I'll explain most of them later separately and in more detail, this is just to get started. Prompt: A place where you input your Positive Prompt. At the top right you can see a 0/75 indicator: this is the amount of tokens in your prompt out of CLIP's capacity. I'll talk about CLIPs in more detail later, but for most cases, you should aim to not exceed 75 tokens, unless you know exactly what you're doing and how. This is not a hard rule, and if you go above 75, another CLIP will be created, but your prompt has the most efficiency when you're keeping it within the single CLIP. There are a lot of exceptions and cases when you'd want to have multiple CLIPs, but I'll talk about it later. Negative Prompt: The field where you input your Negative Prompt. You should only have your negative Quality tags there for the most cases. A rule of thumb is, unless you have a specific reason to add them, don't. Negative prompting is very powerful, and being careless can degrade your gens a lot. For example, if you have monochrome images popping of frequently, you may add monochrome to the negatives; they should have a reason to be there. I'll touch on them in more detail later. The Arrow Button: inputs your previous generation parameters into the UI. It also works on SD restart. It's great if you refreshed the page. The Trash Can Button: Clears both Prompt fields. Can be buggy, I don't recommend using it. Notebook Button: Moves prompt from enabled Styles to Prompt fields and disables the Style. Blue Spiral Button (usually hidden): It appears if you refresh the page in the middle of the ongoing generation. Allows you to get the prompt, previous settings and images from this gen. Styles Field and the Pen Button: It's an option to have a set combination of tags be added to your prompt. You can set them up by pressing the Pen button, you can see an example below. By default, the tags are added like this: 1girl, standing, ...your prompt..., <style> . You can make Style prompt be added to any position of the prompt, by using {prompt} , for example, {prompt}, masterpiece, good quality, amazing quality, . Sampling Method: This is where you choose your Sampler. While I won't go into what they do exactly, I can recommend using DPM++ 2M , and, if for some reason it doesn't work on some specific model, Euler a . I'll touch on some other Samplers later. Scheduler Type: It's a secondary setting for Samplers. With DPM++ 2M you should use Karras , and with Euler a you should use Normal . There are other useful Schedulers I'll mention much later. Sampling Steps: how many generation steps are done for a single image. I recommend having it from 26 to 34 . I use 28 , 30 is also very good. There are methods to drastically reduce the amount of steps needed (thus decreasing the generation time) with Hyper LoRAs, LCM or AYS, see further ahead. Width and Height: options to set the resolution of your gen. SDXL is trained on 1024x1024 images and finetuned on a number of other resolutions. Here is a list of good SDXL resolutions. I personally use either 832x1216 or 1216x832 for either Landscape or Portrait, as it's very close to a regular 2:3 aspect ratio. Note that there is no way to natively generate 16:9 images, do not use resolutions like 1366x768 and such, they will result in artifacts. Refer to the link above to see what aspect ratios are supported . Batch Count: how many images are generated separately. I recommend using it exclusively unless you have a GPU with enough VRAM to support simultaneous generation of multiple images. Batch Size: how many images are generated at the same time. Extremely VRAM intensive and not much faster than generating images one by one, I do not recommend using it. Leave it at 1. CFG Scale: A very tricky setting. Basically, the higher CFG is, the more model will adhere to your prompt. With Illustrious models, you get cohere generations in the range from CFG 3 to 7. I recommend using 5 and keeping in mind the option to move it to 4 or 3.5 the more complex your prompt gets. Seed: a number that defines your generation. By reusing the exact same prompt and exact same seed from another generation, you will get the (almost) exact same image. -1 for Random (or by pressing the Dice button). Recycle button allows you to automatically enter the seed from previous generation. It's very useful if you want to test LoRAs or models, or Hires/Stylize an already genned image. Other menus below: These are all the different extensions you have installed. I will talk about some them separately. Gallery buttons: You have a set of useful different buttons under the image. They are explained if you hover over them, so I won't be explaining them. Just know that they're there, and that they're useful. Metadata: Under gallery buttons there is text. This text is the metadata from the current image; it contains your prompt, generation parameters, extension parameters, etc. This text is useful if you or someone else wants to replicate the exact image you got.","title":"User Interface; txt2img"},{"location":"forge_guide/#ui-settings","text":"While there is a ton of settings, I want to tell you about some of the most important. I'll be referring to the by their name in the list on the left: - Defaults: Allows you to save all your current parameters and settings as a default, so that SD will be using them on launch. Press View Changes to see what exactly it'll save, and Apply to save them. Very useful to not have to setup basic things like CFG, Samplers and Resolution each time you launch SD. - Live Previews: This section is responsible for Live Previews of images that are still generating. While it is extremely useful, it takes a bit of performance away, so if that's an issue for you, turn it off. You can also set the period of Live Previews renders; I use 7, for example, because I always use 28 Generation steps, so I get a preview every quarter of the generation. You can also change the Preview Method; I use TAESD , as it's almost doesn't have quality degradation from it being a preview and I didn't notice much of a performance loss. - Infotext: allows you to disable writing generation metadata into the image. While I'm all for image gen being an open-source, completely collaborative hobby where we all learn from each other to improve, people should have an option to hide their generation parameters and prompts from others. Feel free to do so. - Paths for Saving: allows you to change the folder to where SD saves your generations. These are most notable settings. Feel free to explore them yourself, I could've easily missed something important.","title":"UI Settings"},{"location":"forge_guide/#extensions","text":"While base Forge is more than enough for most stuff, there are some very useful extensions you can download. First, how to install them. Go to the Extensions tab, then press the Available tab under it and press the big blue Load From button. It will give you a list of all the available extensions. Once you installed it, go to Installed tab and press Apply and Restart UI. Note that if you're using --listen , Forge will not allow you to install, enable and disable extensions. Remove the arg, do what you need with the extensions and put it back. Now, some cool extensions: Booru tag autocompletion prompting : it functions as an autocomplete for Booru tags, containing, I think, literally all of them. Very convenient and useful even if you already memorised them. You can press Tab to put in the top result and arrows to move between results. On pressing Tab, it also auto-formats (brackets) into \\(sd-recongisable brackets\\) and auto-removes _underscores_ . Very cool. ADetailer : an essential extension for almost everyone. What it does is: once the image finishes generation, it automatically detects any faces and re-generates them in full resolution, resulting in crisp and detailed faces in any situation. It doesn't require any setup; you just install it, enable and voila, now character faces look beautiful. Regional Prompter : I never figured it out, but a few people on Aegiscord mention it from time to time and show impressive results with it, so I'll mention this extension too. What it does is: it separates the image area in X areas and generates characters separately for each one. It's extremely useful for multiple character gens, but is completely unintuitive and I didn't have enough patience to learn it; Illustrious is capable of generating 2 or 3 characters by itself with some tricky prompting I'll talk about in detail later. If I ever learn this extension, maybe I'll expand this section or make a new one. Infinite Image Browser : An incredible tool to view, sort, tag, organize and compare all your generations. It indexes and views everything in your outputs (or any custom) folder. You can see a showcase of this extension on it's github page . I highly recommend giving it a shot; it's one of my essential extensions now.","title":"Extensions"},{"location":"forge_guide/#were-ready","text":"I think that's all the technical stuff need to know to get to image generation with Forge. Let's get to the deeds.","title":"We're ready?"},{"location":"illustrious_guide/","text":"Prompting 101: Basic Principles, Structure, and Why Illustrious is Amazing All gens start from a prompt. Positive Prompt field, arguably, is the place you're going to spend the most time at. It may look deceptively simple; you just write what you want to see in the image, right? Well, yes, but no. This is where we have to understand how Image Generation models read and process your prompts. I promise it's not as technical as it sounds. I like to think that there are two main ways models understand what you want from them (or, more correctly, two different types of captioning the Training Data models are built upon): Natural Language: The name pretty much explains it all; you just type what you want to see verbatim. For example, the prompt for a model that uses Natural Language may look like this: a girl stands in a flower field and looks at the mountains, the wind lifts her hair. She wears a backpack. her hair is blonde and long, etc... . It's good in a sense that you can just describe the image in detail; it's especially good when it comes to actions, but, in my personal opinion, the fatal flaw of this approach is that you cannot know the most optimal way to prompt. When there's literally an infinite amount of ways to describe something, how do I do it at least decently? The question may seem silly, but it's actually incredibly frustrating when you get dozens of shitty gens; you know the model is capable of doing great stuff, but when you look into the prompts of these great images, you see that it can easily be 100+ words long. Unless you spend countless hours trying and failing, you won't even get a somewhat good looking image. That's why I prefer the other way to prompt. For example, Midjourney and Pony-Based Stable Diffusion XL models use this method, while stuff like NovelAI's Image Gen supports it partially. Tag-Based Prompting: It means that there are specific tags the model understands and uses to generate the image. For example, the prompt above will turn into 1girl, looking ahead, blonde hair, long hair, backpack, floating hair, mountain, scenery, landscape, flower field, from behind . It may look less coherent, but unlike with Natural Language, this time I know for sure how to word it, and that the model will understand what I want from it. It opens other opportunities like Artist Tags, Character Tags and such, but we'll talk about that later. Tag-Based prompts are simple to build, easy to understand, and very effective. Not to say that it doesn't have its own drawbacks, but we'll talk about them later. Almost any Image Generation model supports it at some capacity (for the simple reason that these tags are, in fact, words), but Illustrious-XL v0.1 and NoobAI based models work exclusively with tags, while NovelAI prefers them but still works with Natural Language. For the sake of this guide (and things I actually know myself), we'll be talking only about Tag-Based Prompting . UPD: So, Illustrious 1.0+ now supports some amount of Natural Language. It is pretty cool, but, in my experience, you can't rely exclusively on natural language; I see it as: you prompt what you can in tags, and then try to get details with Natural Language if you can't get them with tags. I'll go in more detail in Illustrious 1.0 section below. Why Illustrious? First, let's understand what Illustrious is. Illustrious (or, more specifically, either Illustrious 0.1/1.0/1.1/2.0) is a base Stable Diffusion XL model. Base Model, in my understanding, is a model that's trained right upon the original (or foundational) model, Stable Diffusion XL in this case. Most of modern models you see on popular online image generation services or Civitai are some form (or merge; or finetune) of Illustrious nowadays. Their specific is in Tag-Based prompting, while foundational SDXL models and Pony models before it were prompted exclusively in Natural Language. There are quite a few Image Generation options out there. For the sake of this guide, I won't touch on NovelAI, Midjourney, Dalle, and Stable Diffusion architectures besides SDXL; I simply don't know much about them. But when it comes to SDXL, there are three main base models that other people build upon: Pony Diffusion XL v6, IllustriousXL v0.1 and NoobAI. The main reason why I love Illustrious is because it's incredibly easy to prompt; you just \"build\" the image with tags, and usually it just works. From my personal experience, Illustrious also has a much higher image quality than Pony-based models; sometimes, Illustrious-based models just excel in stuff like small details and backgrounds and more important stuff like prompt adherence, image composition, stylization, and character adherence. Like literally anything out there, it takes quite a bit of learning, trial and error, research, and even more learning, but we'll go through the most important aspects of generating images in this guide. Let's get to it, I guess. Small addendum about NoobAI. While it's technically different from Illustrious, it's prompted the same way. As long as the NoobAI model you're downloading is eps-pred (sometimes also called e-pred), you won't see any difference from Illustrious models, at least in prompting and most technicalities. You should generally avoid v-pred models, they suffer from contrast and color issues, and may work badly with LoRAs. \"Where do I get the tags???\" Okay, actually, before we start, this is maybe the most important thing I need to address - the place where you'll get the info about tags. Illustrious models use Training Data obtained from Danbooru, so they're captioned in Booru tags. Danbooru, bless them, has a page where they list and explain almost every tag on the site: Danbooru Tag Groups Wiki . It is big and a bit complex, and will take a while to get used to, but as I explain further down the process of making a prompt, I will be linking different Tag Group pages that list all the tags I'll be using. Brackets and Underscores While we're here, let me talk about underscores and brackets . On the Danbooru site, to search for tags that consist of multiple words, you have to use underscores, like looking_at_viewer . In Stable Diffusion, you do not use them , so the tag above will turn into a simple looking at viewer . Brackets are a bit trickier; let's look at the tag that Danbooru recognises as 2000s_(style) . In Stable Diffusion, brackets are used exclusively for Weight Manipulation (that we will talk about later), so it will not recognise the tag. To fix this, every bracket must be prefixed with a \\ , so the tag above will look like 2000s \\(style\\) in your Prompt field. Basic Prompting: Tag Types and your First Prompt Step 1: People Tags Let's start with the very beginning of a prompting process. For the sake of simplicity and, to be fair, the needs of the most people getting into image gen, we'll be doing character gens. The very first thing your prompt must have is something I call People Tags . These are essential and must be at the very beginning of your prompt. They are self-explanatory, and while there are quite a few of them, you won't ever need the most. Here are People Tags that will be enough for 100% of your gens: 1boy, 2boys, 3boys, 1girl, 2girls, 3girls, multiple boys, multiple girls, no people More Group Tags Let's start making our prompt. So far it's: Positive Prompt: 1girl, Step 2: Character Tags Now, let's think about how we want our 1girl to look like. Actually, it's not strictly necessary to define the character at all: the model will come up with something on it's own. It can be useful if the exact look of the character is not important to you, for example, when you're just testing styles or generating a scenery, cases where you want to keep your Prompt Complexity low (we'll talk more about it later). Most of the time, of course, you want to define your character. The simplest way possible is by just using a Character Tag \u2014 an existing character from some media. There're multiple lists of characters from different Media on Tag Groups Wiki under \" Copyrights, artists, projects and media \", but the easiest way to find them is by just searching the character or media you want. If you're making a completely original character, you should skip this step entirely. For example, I'll pick a character whose tag is called makima_(chainsaw_man) . In prompt, it will look like makima \\(chainsaw man\\) , so now we have: Positive Prompt: 1girl, makima \\(chainsaw man\\), Really, you should always check Booru for a tag you're going to use. Takes a few seconds, but spares trouble from using a wrong tag. Also make sure to check what a tag actually means. solo focus , for example, mandates the image to be a multiple character image; it will mess up solo gens. Step 3: Action Tags Now, let's make our prompt more alive. The next part of making a prompt is defining character's actions and position. I call it Action Tags . They are mostly up to your creativity: you think of what you want your character to do, look up the tags and enter them. You can find almost every Action Tag on Posture Tags Wiki , but they're not exhaustive; feel free to try different variations of words in Search to see if the tag not documented on Postures Wiki exists. For example, there are Holding Tags , Looking At tags and more on Eye Tags Wiki , a lot of actions on Verbs and Gerunds Wiki , also 'On' Tags and probably much more. Anyway, let's get back to our prompt. I'll be using looking at viewer, sitting, crossed legs, on chair, head tilt, . It makes our prompt: Positive Prompt: 1girl, makima \\(chainsaw man\\), looking at viewer, sitting, crossed legs, on chair, head tilt, Step 4: Appearance Tags This is where we define the way our character looks, like Eye Tags , Attire Tags , Body Tags , Face Tags , etc. Again, the easiest way to find them is to just use Search. Want Animal Ears? You just search it and find the correct name for the tag. I guess the most important thing here is, the less, the better , especially if you're using a Character Tag. If you want to get this non-original character 1:1 in their original attire, you can skip this step entirely 90% of the time, just like I'll do it here. I have some gens with defined appearances below, so don't worry. We still have: Positive Prompt: 1girl, makima \\(chainsaw man\\), looking at viewer, sitting, crossed legs, on chair, head tilt, Step 5: Background Tags Location Tags Wiki is your best friend here. Most of the time you'd want to specify separately, whether it's indoors or outdoors . For Simple Backgrounds, make sure that your negative prompt doesn't have simple background in it, here's the Backgrounds Wiki . For this prompt, I'll use indoors, office . Positive Prompt: 1girl, makima \\(chainsaw man\\), looking at viewer, sitting, crossed legs, on chair, head tilt, indoors, office Step 6: Image Composition and Style Tags It's one of the most important Tag Categories. Most Composition Tags are explained in Image Composition Wiki ; specifically in View Angle , Focus Tags and Framing of the Body . Style tags are used fairly rarely, but you can find them in Style Parodies Wiki . Please note that Composition tags are essential (in most cases. Note that you must not use View Angle tags in Multiple Character where both character are in focus) . Without them, your gen might suffer a huge loss in quality. In my prompt, I'll use cowboy shot . Positive Prompt: 1girl, makima \\(chainsaw man\\), looking at viewer, sitting, crossed legs, on chair, head tilt, indoors, office, cowboy shot, Step 7: Artist Tags This is maybe the hardest part of the prompt that I'll explain separately later. I won't be using them here. Okay, I never actually made this section, so a couple of words about Artist Tags. First, you can see all the different artists here . In general, an artist should have at least 100 images for their Artist Tag to work well; I recommend aiming at artists with 500+ images. Check LoRAs section; Artist are similar in how you should use them. Step X: Quality Tags This is something you'll most likely have to setup once and never change again. I recommend the following quality tags: Positive: {prompt},masterpiece,best quality,amazing quality Negative: {prompt},bad quality,worst quality,worst detail,sketch,censor, Depending on what you get, you may also want to negative signature, watermark, monochrome, l0l1, child, censor, multiple view, etc , but it's best to start expanding on negatives only when you get something you don't want in the gen. Result We finally finished going through the prompting process, and in the end we've got: Positive Prompt: 1girl, makima \\(chainsaw man\\), looking at viewer, sitting, crossed legs, on chair, head tilt, indoors, office, cowboy shot, masterpiece,best quality,amazing quality Negative Prompt: bad quality,worst quality,worst detail,sketch,censor -> <- -> Metadata: 1girl, makima (chainsaw man), looking at viewer, sitting, crossed legs, on chair, head tilt, indoors, office, cowboy shot, masterpiece,best quality,amazing quality, Negative prompt: bad quality,worst quality,worst detail,sketch,censor Steps: 28, Sampler: DPM++ 2M, Schedule type: Karras, CFG scale: 5, Seed: 3473795472, Size: 832x1216, Model hash: 0842c2a1d8, Model: Amanatsu_v11, Denoising strength: 0.39, Clip skip: 2, ADetailer model: face_yolov8n.pt, Hires CFG Scale: 5, Hires upscale: 1.5, Hires steps: 20, Hires upscaler: R-ESRGAN 4x+ Anime6B, <-","title":"Illustrious Guide; Basics"},{"location":"illustrious_guide/#prompting-101-basic-principles-structure-and-why-illustrious-is-amazing","text":"All gens start from a prompt. Positive Prompt field, arguably, is the place you're going to spend the most time at. It may look deceptively simple; you just write what you want to see in the image, right? Well, yes, but no. This is where we have to understand how Image Generation models read and process your prompts. I promise it's not as technical as it sounds. I like to think that there are two main ways models understand what you want from them (or, more correctly, two different types of captioning the Training Data models are built upon): Natural Language: The name pretty much explains it all; you just type what you want to see verbatim. For example, the prompt for a model that uses Natural Language may look like this: a girl stands in a flower field and looks at the mountains, the wind lifts her hair. She wears a backpack. her hair is blonde and long, etc... . It's good in a sense that you can just describe the image in detail; it's especially good when it comes to actions, but, in my personal opinion, the fatal flaw of this approach is that you cannot know the most optimal way to prompt. When there's literally an infinite amount of ways to describe something, how do I do it at least decently? The question may seem silly, but it's actually incredibly frustrating when you get dozens of shitty gens; you know the model is capable of doing great stuff, but when you look into the prompts of these great images, you see that it can easily be 100+ words long. Unless you spend countless hours trying and failing, you won't even get a somewhat good looking image. That's why I prefer the other way to prompt. For example, Midjourney and Pony-Based Stable Diffusion XL models use this method, while stuff like NovelAI's Image Gen supports it partially. Tag-Based Prompting: It means that there are specific tags the model understands and uses to generate the image. For example, the prompt above will turn into 1girl, looking ahead, blonde hair, long hair, backpack, floating hair, mountain, scenery, landscape, flower field, from behind . It may look less coherent, but unlike with Natural Language, this time I know for sure how to word it, and that the model will understand what I want from it. It opens other opportunities like Artist Tags, Character Tags and such, but we'll talk about that later. Tag-Based prompts are simple to build, easy to understand, and very effective. Not to say that it doesn't have its own drawbacks, but we'll talk about them later. Almost any Image Generation model supports it at some capacity (for the simple reason that these tags are, in fact, words), but Illustrious-XL v0.1 and NoobAI based models work exclusively with tags, while NovelAI prefers them but still works with Natural Language. For the sake of this guide (and things I actually know myself), we'll be talking only about Tag-Based Prompting . UPD: So, Illustrious 1.0+ now supports some amount of Natural Language. It is pretty cool, but, in my experience, you can't rely exclusively on natural language; I see it as: you prompt what you can in tags, and then try to get details with Natural Language if you can't get them with tags. I'll go in more detail in Illustrious 1.0 section below.","title":"Prompting 101: Basic Principles, Structure, and Why Illustrious is Amazing"},{"location":"illustrious_guide/#why-illustrious","text":"First, let's understand what Illustrious is. Illustrious (or, more specifically, either Illustrious 0.1/1.0/1.1/2.0) is a base Stable Diffusion XL model. Base Model, in my understanding, is a model that's trained right upon the original (or foundational) model, Stable Diffusion XL in this case. Most of modern models you see on popular online image generation services or Civitai are some form (or merge; or finetune) of Illustrious nowadays. Their specific is in Tag-Based prompting, while foundational SDXL models and Pony models before it were prompted exclusively in Natural Language. There are quite a few Image Generation options out there. For the sake of this guide, I won't touch on NovelAI, Midjourney, Dalle, and Stable Diffusion architectures besides SDXL; I simply don't know much about them. But when it comes to SDXL, there are three main base models that other people build upon: Pony Diffusion XL v6, IllustriousXL v0.1 and NoobAI. The main reason why I love Illustrious is because it's incredibly easy to prompt; you just \"build\" the image with tags, and usually it just works. From my personal experience, Illustrious also has a much higher image quality than Pony-based models; sometimes, Illustrious-based models just excel in stuff like small details and backgrounds and more important stuff like prompt adherence, image composition, stylization, and character adherence. Like literally anything out there, it takes quite a bit of learning, trial and error, research, and even more learning, but we'll go through the most important aspects of generating images in this guide. Let's get to it, I guess. Small addendum about NoobAI. While it's technically different from Illustrious, it's prompted the same way. As long as the NoobAI model you're downloading is eps-pred (sometimes also called e-pred), you won't see any difference from Illustrious models, at least in prompting and most technicalities. You should generally avoid v-pred models, they suffer from contrast and color issues, and may work badly with LoRAs.","title":"Why Illustrious?"},{"location":"illustrious_guide/#where-do-i-get-the-tags","text":"Okay, actually, before we start, this is maybe the most important thing I need to address - the place where you'll get the info about tags. Illustrious models use Training Data obtained from Danbooru, so they're captioned in Booru tags. Danbooru, bless them, has a page where they list and explain almost every tag on the site: Danbooru Tag Groups Wiki . It is big and a bit complex, and will take a while to get used to, but as I explain further down the process of making a prompt, I will be linking different Tag Group pages that list all the tags I'll be using.","title":"\"Where do I get the tags???\""},{"location":"illustrious_guide/#brackets-and-underscores","text":"While we're here, let me talk about underscores and brackets . On the Danbooru site, to search for tags that consist of multiple words, you have to use underscores, like looking_at_viewer . In Stable Diffusion, you do not use them , so the tag above will turn into a simple looking at viewer . Brackets are a bit trickier; let's look at the tag that Danbooru recognises as 2000s_(style) . In Stable Diffusion, brackets are used exclusively for Weight Manipulation (that we will talk about later), so it will not recognise the tag. To fix this, every bracket must be prefixed with a \\ , so the tag above will look like 2000s \\(style\\) in your Prompt field.","title":"Brackets and Underscores"},{"location":"illustrious_guide/#basic-prompting-tag-types-and-your-first-prompt","text":"","title":"Basic Prompting: Tag Types and your First Prompt"},{"location":"illustrious_guide/#step-1-people-tags","text":"Let's start with the very beginning of a prompting process. For the sake of simplicity and, to be fair, the needs of the most people getting into image gen, we'll be doing character gens. The very first thing your prompt must have is something I call People Tags . These are essential and must be at the very beginning of your prompt. They are self-explanatory, and while there are quite a few of them, you won't ever need the most. Here are People Tags that will be enough for 100% of your gens: 1boy, 2boys, 3boys, 1girl, 2girls, 3girls, multiple boys, multiple girls, no people More Group Tags Let's start making our prompt. So far it's: Positive Prompt: 1girl,","title":"Step 1: People Tags"},{"location":"illustrious_guide/#step-2-character-tags","text":"Now, let's think about how we want our 1girl to look like. Actually, it's not strictly necessary to define the character at all: the model will come up with something on it's own. It can be useful if the exact look of the character is not important to you, for example, when you're just testing styles or generating a scenery, cases where you want to keep your Prompt Complexity low (we'll talk more about it later). Most of the time, of course, you want to define your character. The simplest way possible is by just using a Character Tag \u2014 an existing character from some media. There're multiple lists of characters from different Media on Tag Groups Wiki under \" Copyrights, artists, projects and media \", but the easiest way to find them is by just searching the character or media you want. If you're making a completely original character, you should skip this step entirely. For example, I'll pick a character whose tag is called makima_(chainsaw_man) . In prompt, it will look like makima \\(chainsaw man\\) , so now we have: Positive Prompt: 1girl, makima \\(chainsaw man\\), Really, you should always check Booru for a tag you're going to use. Takes a few seconds, but spares trouble from using a wrong tag. Also make sure to check what a tag actually means. solo focus , for example, mandates the image to be a multiple character image; it will mess up solo gens.","title":"Step 2: Character Tags"},{"location":"illustrious_guide/#step-3-action-tags","text":"Now, let's make our prompt more alive. The next part of making a prompt is defining character's actions and position. I call it Action Tags . They are mostly up to your creativity: you think of what you want your character to do, look up the tags and enter them. You can find almost every Action Tag on Posture Tags Wiki , but they're not exhaustive; feel free to try different variations of words in Search to see if the tag not documented on Postures Wiki exists. For example, there are Holding Tags , Looking At tags and more on Eye Tags Wiki , a lot of actions on Verbs and Gerunds Wiki , also 'On' Tags and probably much more. Anyway, let's get back to our prompt. I'll be using looking at viewer, sitting, crossed legs, on chair, head tilt, . It makes our prompt: Positive Prompt: 1girl, makima \\(chainsaw man\\), looking at viewer, sitting, crossed legs, on chair, head tilt,","title":"Step 3: Action Tags"},{"location":"illustrious_guide/#step-4-appearance-tags","text":"This is where we define the way our character looks, like Eye Tags , Attire Tags , Body Tags , Face Tags , etc. Again, the easiest way to find them is to just use Search. Want Animal Ears? You just search it and find the correct name for the tag. I guess the most important thing here is, the less, the better , especially if you're using a Character Tag. If you want to get this non-original character 1:1 in their original attire, you can skip this step entirely 90% of the time, just like I'll do it here. I have some gens with defined appearances below, so don't worry. We still have: Positive Prompt: 1girl, makima \\(chainsaw man\\), looking at viewer, sitting, crossed legs, on chair, head tilt,","title":"Step 4: Appearance Tags"},{"location":"illustrious_guide/#step-5-background-tags","text":"Location Tags Wiki is your best friend here. Most of the time you'd want to specify separately, whether it's indoors or outdoors . For Simple Backgrounds, make sure that your negative prompt doesn't have simple background in it, here's the Backgrounds Wiki . For this prompt, I'll use indoors, office . Positive Prompt: 1girl, makima \\(chainsaw man\\), looking at viewer, sitting, crossed legs, on chair, head tilt, indoors, office","title":"Step 5: Background Tags"},{"location":"illustrious_guide/#step-6-image-composition-and-style-tags","text":"It's one of the most important Tag Categories. Most Composition Tags are explained in Image Composition Wiki ; specifically in View Angle , Focus Tags and Framing of the Body . Style tags are used fairly rarely, but you can find them in Style Parodies Wiki . Please note that Composition tags are essential (in most cases. Note that you must not use View Angle tags in Multiple Character where both character are in focus) . Without them, your gen might suffer a huge loss in quality. In my prompt, I'll use cowboy shot . Positive Prompt: 1girl, makima \\(chainsaw man\\), looking at viewer, sitting, crossed legs, on chair, head tilt, indoors, office, cowboy shot,","title":"Step 6: Image Composition and Style Tags"},{"location":"illustrious_guide/#step-7-artist-tags","text":"This is maybe the hardest part of the prompt that I'll explain separately later. I won't be using them here. Okay, I never actually made this section, so a couple of words about Artist Tags. First, you can see all the different artists here . In general, an artist should have at least 100 images for their Artist Tag to work well; I recommend aiming at artists with 500+ images. Check LoRAs section; Artist are similar in how you should use them.","title":"Step 7: Artist Tags"},{"location":"illustrious_guide/#step-x-quality-tags","text":"This is something you'll most likely have to setup once and never change again. I recommend the following quality tags: Positive: {prompt},masterpiece,best quality,amazing quality Negative: {prompt},bad quality,worst quality,worst detail,sketch,censor, Depending on what you get, you may also want to negative signature, watermark, monochrome, l0l1, child, censor, multiple view, etc , but it's best to start expanding on negatives only when you get something you don't want in the gen.","title":"Step X: Quality Tags"},{"location":"illustrious_guide/#result","text":"We finally finished going through the prompting process, and in the end we've got: Positive Prompt: 1girl, makima \\(chainsaw man\\), looking at viewer, sitting, crossed legs, on chair, head tilt, indoors, office, cowboy shot, masterpiece,best quality,amazing quality Negative Prompt: bad quality,worst quality,worst detail,sketch,censor -> <- -> Metadata: 1girl, makima (chainsaw man), looking at viewer, sitting, crossed legs, on chair, head tilt, indoors, office, cowboy shot, masterpiece,best quality,amazing quality, Negative prompt: bad quality,worst quality,worst detail,sketch,censor Steps: 28, Sampler: DPM++ 2M, Schedule type: Karras, CFG scale: 5, Seed: 3473795472, Size: 832x1216, Model hash: 0842c2a1d8, Model: Amanatsu_v11, Denoising strength: 0.39, Clip skip: 2, ADetailer model: face_yolov8n.pt, Hires CFG Scale: 5, Hires upscale: 1.5, Hires steps: 20, Hires upscaler: R-ESRGAN 4x+ Anime6B, <-","title":"Result"},{"location":"introduction/","text":"Introduction Hi there, it's Bex. You may know me from Statuo's Aegis City Discord or Chubcord. While both are botmaking/chat-botting oriented, image gen is a huge thing in the hobby and is fun in general. This Rentry is a place for me to organise my experience, some tricks, and technical stuff that will be useful to simply look up. I also intend this Rentry to be a useful resource for people inexperienced with Stable Diffusion and/or Illustrious-based models, as well as people who simply want to learn more. You may be surprised by how much stuff is unintuitive, almost not documented and hard to find and understand. While initially it started as a comprehensive guide for everything related to Stable Diffusion and Illustrious, I had a realization that if I actually stick to it and do all the research and investigation, all the time in the world won't be enough, and I want to eventually finish this thing, so let's get it straight: while I will be touching on a lot of stuff, this is not a comprehensive guide. I will not be explaining every single little thing. This is for people who are interested in Stable Diffusion and want to get more from their gens, so I expect you, dear reader, to have agency of your own. I'll be saying it many times throughout the guide, but: try doing things yourself, experiment, do the opposite of what I say if you want to, fail again and again to get the gen of your dreams; be creative . Anyway, let's get to it. If you have any questions, advices or corrections, feel free to contact me on Discord, @bextoper , either in the DMs or by pinging me in Aegis City or Chub Discord. I'm in no way an expert in Stable Diffusion or image gen, but there's a lot of stuff I've personally struggled with that took me months to learn and overcome, and I think it would be valuable to share some of it. UPD: Hell yeah, the migration. Rentry was lagging extremely hard, taking about a minute to process me adding 15 words, which is extremely miserable. Kinda understandable, honestly; My guide has ~20k words and ~120k characters, but still. Here, using mkdocs and Github Pages, I can just edit documents on my PC and push it to the site, without worrying about Rentry being a bitch, so yeah: welcome to the new (no) and improved (not really) SD/Illustrious guide.","title":"Introduction"},{"location":"introduction/#introduction","text":"Hi there, it's Bex. You may know me from Statuo's Aegis City Discord or Chubcord. While both are botmaking/chat-botting oriented, image gen is a huge thing in the hobby and is fun in general. This Rentry is a place for me to organise my experience, some tricks, and technical stuff that will be useful to simply look up. I also intend this Rentry to be a useful resource for people inexperienced with Stable Diffusion and/or Illustrious-based models, as well as people who simply want to learn more. You may be surprised by how much stuff is unintuitive, almost not documented and hard to find and understand. While initially it started as a comprehensive guide for everything related to Stable Diffusion and Illustrious, I had a realization that if I actually stick to it and do all the research and investigation, all the time in the world won't be enough, and I want to eventually finish this thing, so let's get it straight: while I will be touching on a lot of stuff, this is not a comprehensive guide. I will not be explaining every single little thing. This is for people who are interested in Stable Diffusion and want to get more from their gens, so I expect you, dear reader, to have agency of your own. I'll be saying it many times throughout the guide, but: try doing things yourself, experiment, do the opposite of what I say if you want to, fail again and again to get the gen of your dreams; be creative . Anyway, let's get to it. If you have any questions, advices or corrections, feel free to contact me on Discord, @bextoper , either in the DMs or by pinging me in Aegis City or Chub Discord. I'm in no way an expert in Stable Diffusion or image gen, but there's a lot of stuff I've personally struggled with that took me months to learn and overcome, and I think it would be valuable to share some of it. UPD: Hell yeah, the migration. Rentry was lagging extremely hard, taking about a minute to process me adding 15 words, which is extremely miserable. Kinda understandable, honestly; My guide has ~20k words and ~120k characters, but still. Here, using mkdocs and Github Pages, I can just edit documents on my PC and push it to the site, without worrying about Rentry being a bitch, so yeah: welcome to the new (no) and improved (not really) SD/Illustrious guide.","title":"Introduction"},{"location":"reusage/","text":"IMPORTANT. Re-usage of my prompts/generations. About using my prompts from this guide in your gens that you intend to share/publish. Let's break it into two cases: Partial re-usage, where you use a part of my prompt to generate something new: while I'd appreciate if you mention this guide, you don't have to credit me. This is your gen, made to look like you want, that I have no relation to. A complete re-usage of my prompt, where no major changes are made. This includes re-using a character with a distinct appearance or re-using characters with the style and composition of the image; in general, copying the complete prompt with no or minimal changes. In this case, please, do not claim ownership and give a credit to this guide or me. This does not apply to Makima gen, 2girls elf + kemonomimi gen, and 3girls Mihoyo MCs gen, as they're very generic and were done for testing. Much appreciated.","title":"IMPORTANT. Re-usage of My Prompt/Generations"},{"location":"reusage/#important-re-usage-of-my-promptsgenerations","text":"About using my prompts from this guide in your gens that you intend to share/publish. Let's break it into two cases: Partial re-usage, where you use a part of my prompt to generate something new: while I'd appreciate if you mention this guide, you don't have to credit me. This is your gen, made to look like you want, that I have no relation to. A complete re-usage of my prompt, where no major changes are made. This includes re-using a character with a distinct appearance or re-using characters with the style and composition of the image; in general, copying the complete prompt with no or minimal changes. In this case, please, do not claim ownership and give a credit to this guide or me. This does not apply to Makima gen, 2girls elf + kemonomimi gen, and 3girls Mihoyo MCs gen, as they're very generic and were done for testing. Much appreciated.","title":"IMPORTANT. Re-usage of my prompts/generations."},{"location":"thanks/","text":"Special Thanks My wholehearted gratitude goes to folks on Aegis City Discord; thanks for your support and encouragement, as well as showing me stuff and giving ideas. This guide wouldn't have existed without you. I'm also extremely grateful to everyone giving feedback or just saying thanks; because of you, I feel like all of this wasn't for nothing. Shoutouts to: @StatuoTW - for your gens, ideas, and creating this beautiful community. @Siberys - for ideas, discussions on different image gen topics, and support. @11yu - for huge help with Samplers, Schedulers and ComfyUI, as well as general knowledge. @Corgi - for help with Tag Bleeding and Overtraining. @MadDetective - for help with tags. @Skelly - for support. I really fucking hope I didn't forget anyone. Tell me if I did. Bex out.","title":"Special Thanks"},{"location":"thanks/#special-thanks","text":"My wholehearted gratitude goes to folks on Aegis City Discord; thanks for your support and encouragement, as well as showing me stuff and giving ideas. This guide wouldn't have existed without you. I'm also extremely grateful to everyone giving feedback or just saying thanks; because of you, I feel like all of this wasn't for nothing. Shoutouts to: @StatuoTW - for your gens, ideas, and creating this beautiful community. @Siberys - for ideas, discussions on different image gen topics, and support. @11yu - for huge help with Samplers, Schedulers and ComfyUI, as well as general knowledge. @Corgi - for help with Tag Bleeding and Overtraining. @MadDetective - for help with tags. @Skelly - for support. I really fucking hope I didn't forget anyone. Tell me if I did. Bex out.","title":"Special Thanks"},{"location":"todo/","text":"To-Do List (since mkdocs apparently doesn't understand crossed out text, having ~~ ~~ in between stuff means that it's done. I'll figure something out... someday...) img2img and Inpainting ControlNet, specifically Style Cloning Hyper-LoRA section. Credits to @Siberys. ~~Rewrite Installation section to feature ForgeUI instead of reForge.~~ Kinda did it. Still need to overhaul it properly. ~~Mention Infinite Image Browsing Extension. It's incredible.~~ Make a Hands Manifesto. Fuck you hands. LatentModifier. ~~Mixing Natural Language and Tags on Illustrious 1.0/1.1 models.~~ ~~IPNDM_V section; simply the best Sampler I ever tried.~~ Credits to @11yu ~~DEIS SGM Uniform section.~~ Credits to @11yu More Screenshots ~~Explain \"Illustrious\"~~ ~~CFG++ section.~~ Make the first part of the guide more beginner-friendly ~~ComfyUI section. I finally somewhat learned it.~~ Expand the section on multiple character gens. I'm absolutely sure there's a way to get 2/3 characters gens consistent, I just need to find the exact formula. \"Notable Tags\" section. Get a life (will never happen).","title":"To-Do List"},{"location":"todo/#to-do-list","text":"(since mkdocs apparently doesn't understand crossed out text, having ~~ ~~ in between stuff means that it's done. I'll figure something out... someday...) img2img and Inpainting ControlNet, specifically Style Cloning Hyper-LoRA section. Credits to @Siberys. ~~Rewrite Installation section to feature ForgeUI instead of reForge.~~ Kinda did it. Still need to overhaul it properly. ~~Mention Infinite Image Browsing Extension. It's incredible.~~ Make a Hands Manifesto. Fuck you hands. LatentModifier. ~~Mixing Natural Language and Tags on Illustrious 1.0/1.1 models.~~ ~~IPNDM_V section; simply the best Sampler I ever tried.~~ Credits to @11yu ~~DEIS SGM Uniform section.~~ Credits to @11yu More Screenshots ~~Explain \"Illustrious\"~~ ~~CFG++ section.~~ Make the first part of the guide more beginner-friendly ~~ComfyUI section. I finally somewhat learned it.~~ Expand the section on multiple character gens. I'm absolutely sure there's a way to get 2/3 characters gens consistent, I just need to find the exact formula. \"Notable Tags\" section. Get a life (will never happen).","title":"To-Do List"},{"location":"updates/","text":"Update History 09/04/25 - v0.1. First draft 10/04/25 - v0.2. Proof read + added images. Released for a preview. 12/04/25 - v1.0. Full release. 14/04/25 - v1.1. Added IPNDM_V section. Noted about reForge's cease of development. A few fixes. 19/04/25 - v1.2. Edits to the Installation and UI guides to make it fit Forge; I think I still need to redo them properly, but it'll do for now. Added Infinite Image Browsing extension to Extensions. Made a small section on Illustrious 1.0/1.1. Made a small section on DEIS. Started img2img. v1.21. Expanded the Multiple Characters section. Some fixes. 21/04/25 - v1.22. Some fixes and clarifications, expanded a few sections. 25/04/25 - v1.23. Added a section on prompt re-usage + a small section on V-pred. 27/04/25 - v1.3. Added ComfyUI guide. 28/04/25 - v1.35. Migrated to mkdocs. v1.36. Some migration-related fixes, expanded some Comfy sections. v1.37. CFG++ Section. 07/05/25 - v1.4. Expanded CFG++ and Recommended Checkpoints sections, added Illustrious 2.0 and Regional Prompter sections. A lot of fixes, thanks @kirhn from Aegiscord. Removed img2img section until I'm actually ready to do it. Removed reForge section.","title":"Update History"},{"location":"updates/#update-history","text":"09/04/25 - v0.1. First draft 10/04/25 - v0.2. Proof read + added images. Released for a preview. 12/04/25 - v1.0. Full release. 14/04/25 - v1.1. Added IPNDM_V section. Noted about reForge's cease of development. A few fixes. 19/04/25 - v1.2. Edits to the Installation and UI guides to make it fit Forge; I think I still need to redo them properly, but it'll do for now. Added Infinite Image Browsing extension to Extensions. Made a small section on Illustrious 1.0/1.1. Made a small section on DEIS. Started img2img. v1.21. Expanded the Multiple Characters section. Some fixes. 21/04/25 - v1.22. Some fixes and clarifications, expanded a few sections. 25/04/25 - v1.23. Added a section on prompt re-usage + a small section on V-pred. 27/04/25 - v1.3. Added ComfyUI guide. 28/04/25 - v1.35. Migrated to mkdocs. v1.36. Some migration-related fixes, expanded some Comfy sections. v1.37. CFG++ Section. 07/05/25 - v1.4. Expanded CFG++ and Recommended Checkpoints sections, added Illustrious 2.0 and Regional Prompter sections. A lot of fixes, thanks @kirhn from Aegiscord. Removed img2img section until I'm actually ready to do it. Removed reForge section.","title":"Update History"}]}